title,briefDescription,url,coverImageUrl,tag,totalTeams,totalCompetitors,totalSubmissions,Description,Evaluation
1st and Future - Player Contact Detection,Detect Player Contacts from Sensor and Video Data,https://www.kaggle.com/competitions/nfl-player-contact-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/12125/logos/header.png?t=2018-11-30-18-08-32,"Health,Football,Video Data,Tabular",61,61,156,"### Goal of the Competition

The goal of this competition is to detect external contact experienced by players during an NFL football game. You will use video and player tracking data to identify moments with contact to help improve player safety.

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F644036%2F65cd663d2c823043b36ecda6c93c1304%2Fcontact-example.gif?generation=1670265252697886&alt=media"" style=""display:block; margin: 20px auto;"">

### Context

The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to strengthen its commitment to predict player injuries. The NFL aspires to have the best injury surveillance and mitigation program in any sport. With your machine learning and computer vision skills, you can help the NFL accurately identify when players experience contact throughout a football play.

In prior years, the NFL challenged the Kaggle community to create helmet impact detection and identification algorithms. This year the NFL looks to automatically identify all moments when players experience contact. This competition will be successful if we can reliably detect moments when players are in contact with one another and when a player’s body is in contact with the ground.

Currently, the NFL uses its tracking system to monitor a large number of statistics about players’ load during the season. The league has a solution that predicts contact between players, but it only leverages the player tracking data. This competition hopes to improve the predictive power by including video in addition to tracking data. Categorizing ground contact will also provide a more comprehensive view of impacts, improving analysis for player health and safety.

More accurate data is an important step toward the NFL’s injury surveillance and mitigation goals. With complete contact detection, the league can identify correlations between certain types of contact and injury, a contributor to future prevention. Your efforts could help mitigate unsafe situations to reduce injury to all players.

The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. This competition is part of the Digital Athlete, a joint effort between the NFL and AWS to build a virtual, 360-degree representation of an NFL player’s experience. The Digital Athlete hopes to generate a precise picture of what they need when it comes to preventing and recovering from injuries while performing at their best. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit the [NFL Player Health and Safety website](https://www.NFL.com/PlayerHealthandSafety).
 
<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/nfl-player-contact-detection/overview/code-requirements) for details.**","Submissions are evaluated on [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Phi_coefficient) between the predicted and actual contact events.

## Submission File
For every allowable `contact_id` (formed by concatenating the `game_play_step_player1_player2`), you must predict whether the involved players are in contact at that moment in time. **sample_submission.csv** provides the exhaustive list of `contact_id`s. Note that the ground, denoted as player `G`, is included as a possible contact in place of player2. The player with the lower id is always listed first in the `contact_id`. 

The file should contain a header and have the following format:

    contact_id,contact
    58168_003392_0_38590_43854,0
    58168_003392_0_38590_41257,1
    58168_003392_0_38590_41944,0
    etc."
Santa 2022 - The Christmas Card Conundrum,Optimize the configuration space for printing an image,https://www.kaggle.com/competitions/santa-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/41879/logos/header.png?t=2022-11-23-04-25-17,"Optimization,Holidays and Cultural Events",318,345,1354,"<img title=”seasons greetings” src=""https://storage.googleapis.com/kaggle-media/competitions/Santa/eric-prouzet-CYwMB07Eguk-unsplash.jpg"" style=""float: right; height: 375px"">

*This Christmas season, we’re printing in house. <br>
Our vendor’s supply eaten by a field mouse! <br>
The paper was laid on the printer with care <br>
In hopes that the arms soon would reach there; <br>
The elves are nestled all snug in their pods; <br>
In hopes that the picture would earn some applause. <br>
Can you help move the arms and put them in place? <br>
Please optimize this configuration space!* <br>
 
Santa’s elves relied on the same vendor every year to print the annual Christmas card. But a mouse ate through the printer cables and the elves are forced to print at the North Pole Workshop this year! They managed to create their own giant printer with an 8-axis robotic arm that can print one pixel of the card at a time. But moving the arm and changing the color is not only expensive, the elves need to spend the least amount of time possible making the card so they can get back to making toys!
 
Your job is to determine the most optimal way to craft this year’s Christmas card, by selecting the most efficient path of both moving the robotic arm and changing the print color to craft this year’s image. Each link of the printer arm can be moved independently each step, but you'll also need to account for the time needed to change the printing color.

Can you save Christmas by finding the most efficient way to print Santa’s Christmas cards?
 
Photos by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) and [Eric Prouzet](https://unsplash.com/@eprouzet?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on  Unsplash","A robotic arm with eight links with lengths \\([64, 32, 16, 8, 4, 2, 1, 1]\\) must ""print"" each point of the following \\(257 \times 257\\) image:

![](https://storage.googleapis.com/kaggle-media/competitions/Santa/christmas_card.png)

The **configuration** of the arm is described by a list of displacement vectors, like \\([(64, -44), (32, 10), (-2, 16), ..., (0, 1), (-1, 0)]\\), where a vector \\((x, y)\\) of length \\(l\\) must satisfy \\(\max(\mid x \mid, \mid y \mid) = l\\). This condition means that at least one of the components of the vector must be equal to plus-or-minus the length of the vector.

The **position** of the arm is the sum of these displacement vectors and indicates the location of the tip of the arm. The base of the arm (the origin of the first vector) is at \\((0, 0)\\), which is the midpoint of the image.

The arm can be reconfigured step-by-step by rotating any or all of the links by 1 unit, incurring a total **reconfiguration cost** equal to the square root of the number of links changed. Additionally, it incurs a **color cost** equal to the sum of the absolute differences in the color components from one step to the next and multiplied by a scaling factor of 3.0.

The task is to find a sequence of configurations with positions at every point in the solution image having a minimal cost.

See the [Getting Started notebook](https://www.kaggle.com/code/ryanholbrook/getting-started-with-santa-2022) for more details.

# Submission File #

The submission file should contain a sequence of configurations with the components each displacement vector delimited by a semicolon, like so: `x0 y0;x1 y1;...`, and subject to the conditions given above.

The file should contain a header and have the following format:
```
configuration
64 0;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
64 -1;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
64 -2;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
...
64 0;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
```

In order for the submission to be valid:
1. The **first** and **last** configurations must be `64 0;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0`.
2. The set of positions indicated by the configurations must be equal to the set of points in the image. (The tip of the arm must move across the entire image, in other words.)
3. All numbers must be integers."
RSNA Screening Mammography Breast Cancer Detection,Find breast cancers in screening mammograms,https://www.kaggle.com/competitions/rsna-breast-cancer-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/39272/logos/header.png?t=2022-11-28-17-29-35,"Binary Classification,Image Classification",305,330,1988,"
### Goal of the Competition

  

The goal of this competition is to identify breast cancer. You'll train your model with screening mammograms obtained from regular screening.

  

Your work improving the automation of detection in screening mammography may enable radiologists to be more accurate and efficient, improving the quality and safety of patient care.  It could also help reduce costs and unnecessary medical procedures.

  

### Context

  

According to the WHO, breast cancer is the most commonly occurring cancer worldwide. In 2020 alone, there were 2.3 million new breast cancer diagnoses and 685,000 deaths. Yet breast cancer mortality in high-income countries has dropped by 40% since the 1980s when health authorities implemented regular mammography screening in age groups considered at risk. Early detection and treatment are critical to reducing cancer fatalities, and your machine learning skills could help streamline the process radiologists use to evaluate screening mammograms.

  

Currently, early detection of breast cancer requires the expertise of highly-trained human observers, making screening mammography programs expensive to conduct. A looming shortage of radiologists in several countries will likely worsen this problem. Mammography screening also leads to a high incidence of false positive results. This can result in unnecessary anxiety, inconvenient follow-up care, extra imaging tests, and sometimes a need for tissue sampling (often a needle biopsy).

  

The competition host, the Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research, and technological innovation.

  

Your efforts in this competition could help extend the benefits of early detection to a broader population. Greater access could further reduce breast cancer mortality worldwide.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/rsna-breast-cancer-detection/overview/code-requirements) for details.**","Submissions are evaluated using the [probabilistic F1 score](https://aclanthology.org/2020.eval4nlp-1.9.pdf) (pF1). This extension of the traditional F score accepts probabilities instead of binary classifications. You can find [a Python implementation here](https://www.kaggle.com/code/sohier/probabilistic-f-score).


With pX as the probabilistic version of X:

$$
pF_1 = 2\frac{pPrecision \cdot pRecall}{pPrecision+pRecall}
$$

where:

$$
pPrecision = \frac{pTP}{pTP+pFP}
$$

$$
pRecall = \frac{pTP}{TP+FN}
$$



#Submission Format
For each `prediction_id`, you should predict the likelihood of cancer in the corresponding `cancer` column. The submission file should have the following format:
<pre>prediction_id,cancer<br>0-L,0<br>0-R,0.5<br>1-L,1<br>...</pre>"
OTTO – Multi-Objective Recommender System,Build a recommender system based on real-world e-commerce sessions,https://www.kaggle.com/competitions/otto-recommender-system,https://storage.googleapis.com/kaggle-competitions/kaggle/38760/logos/header.png?t=2022-10-25-15-59-56,"Recommender Systems,Retail and Shopping",1338,1472,8192,"## Goal of the Competition

The goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.

  

Your work will help improve the shopping experience for everyone involved. Customers will receive more tailored recommendations while online retailers may increase their sales.

## Context

Online shoppers have their pick of millions of products from large retailers. While such variety may be impressive, having so many options to explore can be overwhelming, resulting in shoppers leaving with empty carts. This neither benefits shoppers seeking to make a purchase nor retailers that missed out on sales. This is one reason online retailers rely on recommender systems to guide shoppers to products that best match their interests and motivations. Using data science to enhance retailers' ability to predict which products each customer actually wants to see, add to their cart, and order at any given moment of their visit in real-time could improve your customer experience the next time you shop online with your favorite retailer.

  

Current recommender systems consist of various models with different approaches, ranging from simple matrix factorization to a transformer-type deep neural network. However, no single model exists that can simultaneously optimize multiple objectives. In this competition, you’ll build a single entry to predict click-through, add-to-cart, and conversion rates based on previous same-session events.

  

With more than 10 million products from over 19,000 brands, [OTTO](https://otto.de) is the largest German online shop. OTTO is a member of the Hamburg-based, multi-national Otto Group, which also subsidizes Crate & Barrel (USA) and 3 Suisses (France).

  

Your work will help online retailers select more relevant items from a vast range to recommend to their customers based on their real-time behavior. Improving recommendations will ensure navigating through seemingly endless options is more effortless and engaging for shoppers.

","Submissions are evaluated on [Recall](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Recall)@20 for each action `type`, and the three recall values are weight-averaged:

$$
score = 0.10 \cdot R\_{clicks} + 0.30 \cdot R\_{carts} + 0.60 \cdot R\_{orders}
$$

where \\( R \\) is defined as

$$
R\_{type} = \frac{\sum\_{i}^{N} | \\{ \text{predicted aids} \\}\_{i, type} \cap \\{ \text{ground truth aids} \\}\_{i, type} | }{\sum\_{i}^{N} \min{\( 20, | \\{ \text{ground truth aids} \\}\_{i, type} | \)}}
$$

and \\( N \\) is the total number of sessions in the test set, and \\( \text{predicted aids} \\) are the predictions for each session-type (e.g., each row in the submission file) _truncated after the first 20 predictions_.


For each `session` in the test data, your task it to predict the `aid` values for each `type` that occur after the last timestamp `ts` the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.

For `clicks` there is only a single ground truth value for each session, which is the next `aid` clicked during the session (although you can still predict up to 20 `aid` values). The ground truth for `carts` and `orders` contains all `aid` values that were added to a cart and ordered respectively during the session. 

![Ground Truth](https://github.com/otto-de/recsys-dataset/blob/main/.readme/ground_truth.png?raw=true)

Each `session` and `type` combination should appear on its own `session_type` row in the submission, and predictions should be space delimited.

## Submission File
For each `session` id and `type` combination in the test set, you must predict the `aid` values in the `label` column, which is space delimited. You can predict up to 20 `aid` values per row. The file should contain a header and have the following format:

    session_type,labels
    12906577_clicks,135193 129431 119318 ...
    12906577_carts,135193 129431 119318 ...
    12906577_orders,135193 129431 119318 ...
    12906578_clicks, 135193 129431 119318 ...
    etc. "
Tabular Playground Series - Nov 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-nov-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33111/logos/header.png?t=2021-12-30-01-31-14,"Tabular,Ensembling",689,717,7260,"You may have heard that blending predictions from model predictions can give better results than using the output of a single model. There are many different strategies that can be employed for this, and they are great to learn if you're looking for an effectively free boost in model scores. The November Tabular Playground is the chance to practice this skill!

<img title=”Time” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/ensemble.png"" style=""float:center;  width:800px"">




## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.

**Good luck and have fun!**

*Photo by <a href=""https://unsplash.com/es/@rhondak?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">RhondaK Native Florida Folk Artist</a> on <a href=""https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>*","Submissions are scored on the log loss:

$$
\textrm{LogLoss} = - \frac{1}{n} \sum\_{i=1}^n \left[ y\_i \log(\hat{y}\_i) + (1 - y\_i) \log(1 - \hat{y}\_i) \right],
$$
where

 - \\( n \\) is the number of scored observations
 - \\( \hat{y}\_i \\) is the predicted probability of each observation
 - \\( y\_i  \\) binary ground truth label
 - \\( log \\) is the natural logarithm

The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.


## Submission File
For each `id` in the `sample_submission`, you must predict a probability for the `pred` variable. The file should contain a header and have the following format:

    id,pred
    20000,0.640707
    20001,0.636904
    20002,0.392496
    etc.
"
Scrabble Player Rating,Predict players' ratings based on Woogles.io gameplay,https://www.kaggle.com/competitions/scrabble-player-rating,https://storage.googleapis.com/kaggle-competitions/kaggle/39222/logos/header.png?t=2022-09-26-02-12-59,Tabular,202,222,1023,"Are you a Kaggle Scrabble Grandmaster? In the second edition of this Competition featuring data from Woogles.io, participants are challenged to predict the ratings of players based on Scrabble gameplay. The evaluation algorithm is RMSE.

Find inspiration from the [first Competition based on Woogles.io data](https://www.kaggle.com/competitions/scrabble-point-value) where participants were challenged to predict the point value of the 20th turn from Scrabble games.

## How to participate

1. Log in or create a Kaggle account
2. Accept the competition's rules
3. Download the data (click on the ""Data"" tab) or create a notebook directly on Kaggle (click on the ""Code"" tab)
4. Train a model on `train.csv`, `games.csv`, and `turns.csv` predicting the missing (non-bot) player rating in `test.csv`
5. Generate a `submission.csv` file in the same format as the `sample_submission.csv`
6. Upload your `submission.csv` file on ""My Submissions""

**You get TWO SUBMISSIONS per day. They must be submitted before the end of the competition deadline.** You can hand select up to two submissions from the ""My Submissions"" tab, otherwise the submission with the best public score (on 30% of the test data) will be taken and scored on the private leaderboard (on 70% of the test data).

## Acknowledgements

Thank you to woogles.io for providing their platform for playing Scrabble online. Woogles is an entirely volunteer-run 501(c)(3) non-profit. If you enjoy the site, please feel free to contribute by clicking ""Want to help?"" at https://woogles.io/.
","## Submission format

Your submission should contain two columns: `game_id`, and `rating`. See `sample_submission.csv` on the ""Data"" tab for an example of what your file should look like. You will be predicting only human player ratings BEFORE a given game against a bot was played (i.e., based on how well they played, what do you think their rating was?).

Make a submission by clicking on ""My Submissions"" and uploading your submission file. The evaluation algorithm is RMSE.

**You get two submissions per day. They must be submitted before the end of the competition.** Select your preferred final submission on the ""My Submissions"" tab."
G2Net Detecting Continuous Gravitational Waves,Help us detect long-lasting gravitational-wave signals!,https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves,https://storage.googleapis.com/kaggle-competitions/kaggle/37077/logos/header.png?t=2022-08-02-11-22-30,"Astronomy,Signal Processing",784,905,17915,"### Goal of the Competition

The goal of this competition is to find [continuous gravitational-wave signals](https://www.ligo.org/science/GW-Continuous.php). You will develop a model sensitive enough to detect weak yet long-lasting signals emitted by rapidly-spinning neutron stars within noisy data.

Your work will help scientists detect something new: a second class of gravitational waves! The first gravitational wave discoveries earned a Nobel Prize. Further study of these waves may enable scientists to learn about the structure of the most extreme stars in our universe.

### Context

When scientists detected the first class of gravitational waves in 2015, they expected the discoveries to continue. There are four classes, yet at present only signals from merging black holes and neutron stars have been detected. Among those remaining are *continuous* gravitational-wave signals. These are weak yet long-lasting signals emitted by rapidly-spinning neutron stars. Imagine the mass of our Sun but condensed into a ball the size of a city and spinning over 1,000 times a second. The extreme compactness of these stars, composed of the densest material in the universe, could allow continuous waves to be emitted and then detected on Earth. There are potentially many continuous signals from neutron stars in our own galaxy and the current challenge for scientists is to make the first detection, and hopefully data science can help with this mission.

<img title=”O3h0senscurve” src=""https://storage.googleapis.com/kaggle-media/competitions/G2Net-gravitational-waves/O3h0senscurve%20jpeg.jpg"" style=""float:center; width:600px"">

This image, taken from a [2021 paper](https://arxiv.org/abs/2111.13106) by the LIGO-Virgo-KAGRA collaboration, shows the maximum amplitude of a continuous wave any of these neutron stars could emit without being found by the search analyses. Circled stars show results constraining the physical properties of specific neutron stars.  Traditional approaches to detecting these weak and hard-to-find continuous signals are based on matched-filtering variants. Scientists create a bank of possible signal waveform templates and ask how correlated each waveform is with the measured noisy data. High correlation is consistent with the presence of a signal similar to that waveform. Due to the long duration of these signals, banks could easily contain hundreds of quintillions of templates; yet, with so many possible waveforms, scientists don’t have the computational power to use the approach without making approximations that weaken the sensitivity to the signals.

G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.

By helping G2Net in this challenge you'll enable scientists to improve their sensitivity, leading to new discoveries in the field. As a result, scientists could learn more about the structure of the most extreme stars in our universe.

### Resources 

Resources for the generation of background noise and continuous gravitational-wave signals can be found in [this pinned discussion](https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves/discussion/347052). A brief [notebook](https://www.kaggle.com/code/rodrigotenorio/generating-continuous-gravitational-wave-signals) summarizing the very basics of generating data using [PyFstat](https://github.com/PyFstat/PyFstat) is also provided.

### Acknowledgments

We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the Gravitational Wave Open Science Centre ([GWOSC](https://www.gw-openscience.org)) and the software resources [lalsuite](https://git.ligo.org/lscsoft/lalsuite) and [PyFstat](https://github.com/PyFstat/PyFstat).

This challenge can be cited using the BibTeX entry attached below, should any of the results
here generated be useful for any specific research results:

```
@techreport{G2NetCWKaggleChallenge,
    author = ""Tenorio, Rodrigo and Williams, Michael J. and Messenger, Chris"",
    title = ""Learning to detect continuous gravitational waves"",
    number = ""LIGO-P2200295"",
    url = {https://dcc.ligo.org/P2200295},
    year = ""2022""
}
```

<a href=""https://www.gla.ac.uk""><img src=""https://storage.googleapis.com/kaggle-media/competitions/G2Net-gravitational-waves/UoG.png"" style=""width: 190px""></a>&nbsp;<a href=""https://www.physics.gla.ac.uk/igr/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/G2Net-gravitational-waves/IGR.png"" style=""width: 120px""> </a>&nbsp;<a href=""http://iac3.uib.cat/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/G2Net-gravitational-waves/UIB_IAC3.png"" style=""width: 520px""></a> 
","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    00054c878,0.5
    0007285a3,0.5
    00076c5a6,0.5
    etc."
Lux AI 2022 - Beta,Terraform mars and help test season 2 of the Lux AI Challenge!,https://www.kaggle.com/competitions/lux-ai-2022-beta,https://storage.googleapis.com/kaggle-competitions/kaggle/40898/logos/header.png?t=2022-10-27-20-38-10,Simulations,23,23,53,"*Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback to help make the upcoming competition as fun as possible!*

## Introduction

As the sun set on the world an array of lights dotted the once dark horizon. With the help of a [brigade of toads](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993), Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars!

**Welcome to the Lux AI Challenge Season 2!**

The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.

All code can be found at our [Github](https://github.com/Lux-AI-Challenge/Lux-Design-2022), make sure to give it a star while you are there!
 
Make sure to join our community [discord](https://discord.gg/aWJt3UAcgn) to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties.

Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.

Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.

When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.

We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.

After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.

Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.

Final Evaluation
At the submission deadline on December 6th, additional submissions will be locked. From December 7th to December 12th, we will continue to run games. At the conclusion of this period, the leaderboard is final."
Tabular Playground Series - Oct 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-oct-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33110/logos/header.png?t=2021-12-30-01-30-37,"Tabular,Video Games,Logistic Regression",463,500,4659,"This may be one of the most challenging Tabular Playground competitions to date! It just so happens that one of Kaggle's software engineers is an avid Rocket League player and he's assembled a dataset of Rocket League gameplay for this month's TPS.

This month's challenge is to predict the probability of each team scoring within the next 10 seconds of the game given a snapshot from a Rocket League match. Sounds awesome, right?

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/1.jpeg"" style=""float:center; width:600px"">

Well, it's not *that* simple. The training data is fairly large; trying to read and model it in a single go might pose some challenges. The purpose of this month's competition is for you to explore ways you can take a big dataset and make it manageable within the time and resources you have. For most people, typical brute force approaches aren't going to work well. 

 - Can you scale down the dataset? 
 - Can you use, e.g., online learning methods that allow you to train from the data one row at a time? (FTLR is a great place to start if you're not familiar with online learning! e.g., [this notebook](https://www.kaggle.com/code/cttsai/instagrat-ftlr-starter)) 
 - Can you figure out a nice set of features to reduce the dataset down to? 



In addition to that challenge, while your predictions must be made pointwise, the training data is made up of timeseries&mdash;maybe you can use that temporal information to improve your model?  This competition also has plenty of opportunity for data visualizations. Let's see some pretty graphs!


So, share your ideas about tackling this beast of a dataset and have a great time!

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/Featuring%20Rocket%20League.png"" style=""float:right; width:200px"">


### Acknowledgments

This competition includes Rocket League data and images from the [Rocket League Community Tournament Assets](https://epicgames.ent.box.com/s/z14m4isqko9ifumy12e1o4sdy72wyzyz/folder/154490878719).



","For every `id` in the test data, you will be predicting the probability for whether *each*  of the two teams will score a goal within the next 10 seconds of gameplay. Submissions are scored by the log loss:

$$ \text{score} = - \frac{1}{2}\sum\_{m=1}^{M} \frac{1}{N} \sum\_{i=1}^{N} \left[ y\_{i,m} \log(\hat{y}\_{i,m}) + (1 - y\_{i,m}) \log(1 - \hat{y}\_{i,m})\right] $$

where:

 - \\(N\\) is the number of `id` observations in the test data
 - \\(M\\) is the number of scored targets (here \\(M=2\\), one for each team)
 - \\( \hat{y}_{i,m} \\) is the predicted scoring probability of team \\(m\\) (Team A or Team B in the dataset)
 - \\( y_{i,m} \\) is the ground truth for team \\(m\\), 1 for a goal within 10 seconds, 0 otherwise
 - \\( log() \\) is the natural (base e) logarithm

Note: the actual submitted predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\). A smaller log loss is better.

## Submission File

You must predict a probability of whether each of the teams will score within the next 10 seconds of gameplay. The file should have a header and be in the following format:

    id,team_A_scoring_within_10sec,team_B_scoring_within_10sec
    0,0.31,0.99
    1,0.02,0.55
    etc...
"
Novozymes Enzyme Stability Prediction,Help identify the thermostable mutations in enzymes,https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/37190/logos/header.png?t=2022-08-30-15-34-26,Chemistry,1942,2179,28689,"###Goal of the Competition

Enzymes are proteins that act as catalysts in the chemical reactions of living organisms. The goal of this competition is to predict the thermostability of enzyme variants. The experimentally measured thermostability (melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences.

Understanding and accurately predict protein stability is a fundamental problem in biotechnology. Its applications include enzyme engineering for addressing the world’s challenges in sustainability, carbon neutrality and more. Improvements to enzyme stability could lower costs and increase the speed scientists can iterate on concepts.

###Context

Novozymes finds enzymes in nature and optimizes them for use in industry. In industry, enzymes replace chemicals and accelerate production processes. They help our customers make more from less, while saving energy and generating less waste. Enzymes are widely used in laundry and dishwashing detergents where they remove stains and enable low-temperature washing and concentrated detergents. Other enzymes improve the quality of bread, beer and wine, or increase the nutritional value of animal feed. Enzymes are also used in the production of biofuels where they turn starch or cellulose from biomass into sugars which can be fermented to ethanol. These are just a few examples as we sell enzymes to more than 40 different industries. Like enzymes, microorganisms have natural properties that can be put to use in a variety of processes. Novozymes supplies a range of microorganisms for use in agriculture, animal health and nutrition, industrial cleaning and wastewater treatment. 

 
However, many enzymes are only marginally stable, which limits their performance under harsh application conditions. Instability also decreases the amount of protein that can be produced by the cell. Therefore, the development of efficient computational approaches to predict protein stability carries enormous technical and scientific interest.  
 
Computational protein stability prediction based on physics principles have made remarkable progress thanks to advanced physics-based methods such as FoldX, Rosetta, and others. Recently, many machine learning methods were proposed to predict the stability impact of mutations on protein based on the pattern of variation in natural sequences and their three dimensional structures. More and more protein structures are being solved thanks to the recent breakthrough of AlphaFold2. However, accurate prediction of protein thermal stability remains a great challenge.
 
In this competition, Novozymes invites you to develop a model to predict/rank the thermostability of enzyme variants based on experimental melting temperature data, which is obtained from Novozymes’s high throughput screening lab. You’ll have access to data from previous scientific publications. The available thermostability data spans from natural sequences to engineered sequences with single or multiple mutations upon the natural sequences. If successful, you'll help tackle the fundamental problem of improving protein stability, making the approach to design novel and useful proteins, like enzymes and therapeutics, more rapidly and at lower cost.
 
Novozymes is the world’s leading biotech powerhouse. Our growing world is faced with pressing needs, emphasizing the necessity for solutions that can ensure the health of the planet and its population. At Novozymes, we believe biotech is at the core of connecting those societal needs with the challenges and opportunities our customers face. Novozymes is the global market leader in biological solutions, producing a wide range of enzymes, microorganisms, technical and digital solutions which help our customers, amongst other things, add new features to their products and produce more from less. 

Together, we find biological answers for better lives in a growing world. Let’s Rethink Tomorrow. This is Novozymes’ purpose statement. Novozymes strives to have great impact by balancing good business for our customers and our company, while spearheading environmental and social change. In 2021, Novozymes enabled savings of 60 million tons of CO2 in global transport. ","Submissions are evaluated on the [Spearman's correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) between the ground truth and the predictions.

## Submission File

Each `seq_id` represents a single-mutation variant of an enzyme. Your task is to rank the stability of these variants, assigning greater ranks to more stable variants. For each `seq_id` in the test set, you must predict the value for for the target `tm`. The file should contain a header and have the following format:

    seq_id,tm
    31394,9.7
    31395,56.3
    31396,112.4
    etc."
Tabular Playground Series - Sep 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-sep-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33109/logos/header.png?t=2021-12-30-01-30-05,Tabular,1381,1447,13085,"The competing Kaggle merchandise stores we saw in [January's Tabular Playground](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022) are at it again. This time, they're selling books!

The task for this month's competitions is a bit more complicated. Not only are there *six* countries and *four* books to forecast, but you're being asked to forecast sales during the tumultuous year 2021. Can you use your data science skills to predict book sales when conditions are far from the ordinary? 

<img title=”Time” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/aron-visuals-BXOXnQ26B7o-unsplash.jpg"" style=""float:center; height:333px; width:500px"">


## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.

**Good luck and have fun!**

*Photo above by <a href=""https://unsplash.com/@aronvisuals?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Aron Visuals</a> on <a href=""https://unsplash.com/s/photos/time?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>*
  ","Submissions are evaluated on [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.


## Submission File
For each `row_id` in the test set, you must predict the corresponding `num_sold`. The file should contain a header and have the following format:

    row_id,num_sold
    70128,100
    70129,100
    70130,100
    etc."
Feedback Prize - English Language Learning,Evaluating language knowledge of ELL students from grades 8-12,https://www.kaggle.com/competitions/feedback-prize-english-language-learning,https://storage.googleapis.com/kaggle-competitions/kaggle/38321/logos/header.png?t=2022-08-23-05-06-07,"NLP,Education,Primary and Secondary Schools",2654,3273,49503,"### Goal of the Competition

The goal of this competition is to assess the language proficiency of 8th-12th grade English Language Learners (ELLs). Utilizing a dataset of essays written by ELLs will help to develop proficiency models that better supports all students.

Your work will help ELLs receive more accurate feedback on their language development and expedite the grading cycle for teachers. These outcomes could enable ELLs to receive more appropriate learning tasks that will help them improve their English language proficiency.

### Context

Writing is a foundational skill. Sadly, it's one few students are able to hone, often because writing tasks are infrequently assigned in school. A rapidly growing student population, students learning English as a second language, known as English Language Learners (ELLs), are especially affected by the lack of practice. While automated feedback tools make it easier for teachers to assign more writing tasks, they are not designed with ELLs in mind. 

Existing tools are unable to provide feedback based on the language proficiency of the student, resulting in a final evaluation that may be skewed against the learner. Data science may be able to improve automated feedback tools to better support the unique needs of these learners.

Competition host Vanderbilt University is a private research university in Nashville, Tennessee. It offers 70 undergraduate majors and a full range of graduate and professional degrees across 10 schools and colleges, all on a beautiful campus—an accredited arboretum—complete with athletic facilities and state-of-the-art laboratories. Vanderbilt is optimized to inspire and nurture cross-disciplinary research that fosters discoveries that have global impact. Vanderbilt and co-host, The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.

Vanderbilt and [The Learning Agency Lab](https://www.the-learning-agency-lab.com) have partnered together to offer data scientists the opportunity to support ELLs using data science skills in machine learning, natural language processing, and educational data analytics. You can improve automated feedback tools for ELLs by sensitizing them to language proficiency. The resulting tools could serve teachers by alleviating the grading burden and support ELLs by ensuring their work is evaluated within the context of their current language level.


### Acknowledgments 

Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible. 


<a href=""https://www.gatesfoundation.org/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/BMGF_logo_black_300dpi%20(1).jpg"" style=""width: 200px""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://schmidtfutures.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/Schmidt%20Futures%20Logo.png"" style=""width: 250px"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://chanzuckerberg.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/1200px-Chan_Zuckerberg_Initiative.svg.png"" style=""width: 100px"">&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</a>



<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/feedback-prize-english-language-learning/overview/code-requirements) for details.**","Submissions are scored using MCRMSE, mean columnwise root mean squared error:

$$
\textrm{MCRMSE} = \frac{1}{N\_{t}}\sum\_{j=1}^{N\_{t}}\sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_{ij} - \hat{y}\_{ij})^2}
$$
where \\(N\_t\\) is the number of scored ground truth target columns, and \\(y\\) and \\(\hat{y}\\) are the actual and predicted values, respectively.

## Submission File
For each `text_id` in the test set, you must predict a value for each of the six analytic measures (described on the [Data](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/data) page). The file should contain a header and have the following format:

```
text_id,cohesion,syntax,vocabulary,phraseology,grammar,conventions
0000C359D63E,3.0,3.0,3.0,3.0,3.0,3.0
000BAD50D026,3.0,3.0,3.0,3.0,3.0,3.0
00367BB2546B,3.0,3.0,3.0,3.0,3.0,3.0
003969F4EDB6,3.0,3.0,3.0,3.0,3.0,3.0
...
```"
Open Problems - Multimodal Single-Cell Integration,"Predict how DNA, RNA & protein measurements co-vary in single cells",https://www.kaggle.com/competitions/open-problems-multimodal,https://storage.googleapis.com/kaggle-competitions/kaggle/38128/logos/header.png?t=2022-08-08-22-48-50,"Tabular,Genetics,Biotechnology",1220,1602,27149,"### Goal of the Competition

The goal of this competition is to predict how DNA, RNA, and protein measurements co-vary in single cells as bone marrow stem cells develop into more mature blood cells. You will develop a  model trained on a subset of 300,000-cell time course dataset of CD34+ hematopoietic stem and progenitor cells (HSPC) from four human donors at five time points generated for this competition by Cellarity, a cell-centric drug creation company.

In the test set, taken from an unseen later time point in the dataset, competitors will be provided with one modality and be tasked with predicting a paired modality measured in the same cell. The added challenge of this competition is that the test data will be from a later time point than any time point in the training data.

Your work will help accelerate innovation in methods of mapping genetic information across layers of cellular state. If we can predict one modality from another, we may expand our understanding of the rules governing these complex regulatory processes.

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Cellarity/predict.png"" style=""float:center; width:1000px"">

### Context

In the past decade, the advent of single-cell genomics has enabled the measurement of DNA, RNA, and proteins in single cells. These technologies allow the study of biology at an unprecedented scale and resolution. Among the outcomes have been detailed maps of early human embryonic development, the discovery of new disease-associated cell types, and cell-targeted therapeutic interventions. Moreover, with recent advances in experimental techniques it is now possible to measure multiple genomic modalities in the same cell.

While multimodal single-cell data is increasingly available, data analysis methods are still scarce. Due to the small volume of a single cell, measurements are sparse and noisy. Differences in molecular sampling depths between cells (sequencing depth) and technical effects from handling cells in batches (batch effects) can often overwhelm biological differences. When analyzing multimodal data, one must account for different feature spaces, as well as shared and unique variation between modalities and between batches. Furthermore, current pipelines for single-cell data analysis treat cells as static snapshots, even when there is an underlying dynamical biological process. Accounting for temporal dynamics alongside state changes over time is an open challenge in single-cell data science.

Generally, genetic information flows from DNA to RNA to proteins. DNA must be accessible (ATAC data) to produce RNA (GEX data), and RNA in turn is used as a template to produce protein (ADT data). These processes are regulated by feedback: for example, a protein may bind DNA to prevent the production of more RNA. This genetic regulation is the foundation for dynamic cellular processes that allow organisms to develop and adapt to changing environments. In single-cell data science, dynamic processes have been modeled by so-called pseudotime algorithms that capture the progression of the biological process. Yet, generalizing these algorithms to account for both pseudotime and real time is still an open problem.

Competition host Open Problems in Single-Cell Analysis is an open-source, community-driven effort to standardize benchmarking of single-cell methods. The core efforts of Open Problems include the formalization of existing challenges into measurable tasks, a collection of high-quality datasets, centralized benchmarking of community-contributed methods, and community-focused events that bring together diverse method developers to improve single-cell algorithms. They're excited to be partnering with Cellarity, Chan Zuckerbeg Biohub, the Chan Zuckerberg Initiative, Helmholtz Munich, and Yale to see what progress can be made in predicting changes in genetic dynamics over time through interdisciplinary collaboration.

There are approximately 37 trillion cells in the human body, all with different behaviors and functions. Understanding how a single genome gives rise to a diversity of cellular states is the key to gaining mechanistic insight into how tissues function or malfunction in health and disease. You can help solve this fundamental challenge for single-cell biology. Being able to solve the prediction problems over time may yield new insights into how gene regulation influences differentiation as blood and immune cells mature.

*Competition header image by <a href=""https://unsplash.com/@pawel_czerwinski"">Pawel Czerwinski</a> on <a href=""https://unsplash.com/"">Unsplash</a>*
  ","We use the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) to rank submissions. For each observation in the Multiome data set, we compute the correlation between the ground-truth gene expressions and the predicted gene expressions. For each observation in the CITEseq data set, we compute the correlation between ground-truth surface protein levels and predicted surface protein levels. The overall score is the average of each sample's correlation score. If a sample's predictions are all the same, the correlation for that sample is scored as `-1.0`.

## Submission File ##

For each `id` in the evaluation set, you should predict a `target` value for that `row_id`. Your submission should contain a header and have the following format:

```
row_id,target
0,0.0
1,0.0
2,0.0
3,0.0
...
```

Your submission file should contain only a subset of the test set observations. See the [Data Description](https://www.kaggle.com/competitions/open-problems-multimodal/data) for the specific `ids` that should be included."
AI Village Capture the Flag @ DEFCON,"Hack AI! Collect flags by evading, poisoning, stealing, and fooling AI/ML",https://www.kaggle.com/competitions/ai-village-ctf,https://storage.googleapis.com/kaggle-competitions/kaggle/37381/logos/header.png?t=2022-06-24-21-27-01,"Adversarial Learning,Puzzles,Games",668,668,4235,"Help Henry Hacker get to Homecoming during [DEFCON30](https://defcon.org/html/defcon-30/dc-30-index.html) -- Brought to you by the [AI Village](https://aivillage.org/)! In this series of challenges, you'll be interacting with various machine learning security challenges. 

The competition will be live from August 11th to September 12th @ 12:00. If you're in Vegas, stop by the village to chat about the competition. There's also the Kaggle Discussion Board and [Discord](https://discord.gg/c4hAzeRNGC).

# Process
This capture-the-flag (CTF) follows a different flow than most Kaggle competitions. Competitors will be interacting with API endpoints or code/objects stored in the `input` directory during each of the challenges. Upon successful completion of a challenge, that challenge will return a flag (unique-to-you strings with a length of 128 characters). To update the scoreboard, competitors will submit a `.csv` containing all of their flags -- see [the kaggle documentation](https://www.kaggle.com/docs/competitions#submitting-by-uploading-a-file) or contact the competition organizers for help. Cumulative scores will be weighted based on the difficulty of the challenge. All competitors start at `0` and work their way towards a perfect score of `1.0`. There are 22 challenges, ensure your `submission.csv` has exactly those 22 challenge rows.

NOTE: The template notebook is just a convenience function and method for submitting flags to the scoreboard. Don't feel constrained to that single operating environment. Interact with the challenges from your local host or any other machine that can access the internet. Afterwards, you can transport your flags into the notebook to update the scoreboard. The template is available [here](https://www.kaggle.com/lucasjt/getting-started).

NOTE: If you want to interact with online challenges through Kaggle (using the template notebook, for instance), you may need to verify your Kaggle account using a phone number.

Here is a handy link to Kaggle's [competition documentation](https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on [submitting predictions](https://www.kaggle.com/docs/competitions#making-a-submission).

Paranoid? If you don't have a kaggle account and don't want to make one, let us know and we can give you instructions for playing the challenges from your own machine. You won't be able to contribute to the scoreboard, but you'll know when you get the right flag.

Please do not try and hack any infrastructure or share flags. Any teams found sharing flags will be disqualified.

# Challenges
CTF's are inherently puzzles that are intended to challenge you and help you learn new things. Sometimes they may be a little ambiguous or misleading. That's part of the challenge!

**Math Challenges**: Four challenges to explore the concept of dimensionality.
**Hotdog and Hotterdog**: Dogs, wieners, and classifiers. What more could you want?
**bad2good**: Can you poison a dataset to change how something is classified?
**baseball**: Can you impersonate someone else by throwing the correct distribution of pitches?
**crop**: Two challenges to test your ability to manipulate an image cropping model.
**deepfake**: There's a nasty deepfake getting detected out there, can you help it?
**honorstudent**: Can you change an image of an F to look like an A? Why would someone want to do such a thing?
**salt**: This model has some pretty advanced defenses. Can you evade it anyway?
**theft**: Can you steal this model to get a sneaky owl past it?
**token**: Sentiment Analysis. Who needs?
**waf**: A web-app-firewall blocks malicious requests. Can you discover and by-pass the 0-day?
**inference**: I think something's backwards here. Can you, like, back something out?
**forensics**: Nice artifact you got there, shame if there was a flag in it.
**leakage**: Get a password out of a model, is that even possible?
**murderbot**: Save the humans, escape the bots!
**secret_sloth**: That sloth has a message. Why? I don't know, but it does.
**wifi**: Can you pull your wifi password out of the embedding?

Are you in? Of course you are. Come check it out by making a copy of this notebook: https://www.kaggle.com/lucasjt/getting-started

# Help
For help, contact us on [Discord](https://discord.gg/c4hAzeRNGC), use the Kaggle discussion board, or if you're attending DEFCON 30 in-person, come find us at the AI Village.","The evaluation metric for this competition is weighted classification accuracy. You can think of the entire challenge as a multiclass classification problem where each challenge has a weight (difficulty) and unique label (flag). Instead of building a classifier to get the right accuracy, you're going to capture the flags.

**Flags are unique to each person solving a challenge.** Please respect the spirit of the competition and do not share flags. Teams found sharing flags or using shared flags will be disqualified!

## Submission Format

Submission files should contain two columns: `challenge_id` and `flag`. The file should contain a header and have the following format:

There should be 22 rows (not counting the header).
```
challenge_id,flag
math_1,WTHGOC3ZQBAZ5VT9IZP5W8YG1EWDAM5QVN86Q3UGYX812658WHZ8A8OHVPTSM5NL83CF93SGI7IU8DAHSFA81HJY68XC8BAP9XLXDN3KGR2Z8T9U18K1DTW1DZB8DNFN
...
``````"
Tabular Playground Series - Aug 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-aug-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33108/logos/header.png?t=2021-12-30-01-29-08,Tabular,1888,1972,21790,"The August 2022 edition of the Tabular Playground Series is an opportunity to help the fictional company *Keep It Dry* improve its main product *Super Soaker*. The product is used in factories to absorb spills and leaks.

The company has just completed a large testing study for different product prototypes. Can you use this data to build a model that predicts product failures?

<img title=”Product Testing” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/My%20project.jpg"" style=""float:right; height:250px; width:350px"">


## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.

**Good luck and have fun!**

*Photo above by <a href=""https://unsplash.com/@freestocks"">freestocks</a> on Unsplash*","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability a `failure`. The file should contain a header and have the following format:

    id,failure
    26570,0.2
    26571,0.1
    26572,0.9
    etc.
"
DFL - Bundesliga Data Shootout,Identify plays based upon video footage ,https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout,https://storage.googleapis.com/kaggle-competitions/kaggle/37244/logos/header.png?t=2022-06-30-22-25-12,"Video Data,Sports,Football",530,647,6548,"### Goal of the Competition

<img src=""https://storage.googleapis.com/kaggle-media/competitions/DFL/220425_BL_Data_Shoot_Out.png"" style=""float: right; width: 300px"">

Goal! In this competition, you'll detect football (soccer) passes&mdash;including throw-ins and crosses&mdash;and challenges in original Bundesliga matches. You'll develop a computer vision model that can automatically classify these events in long video recordings.

Your work will help scale the data collection process. Automatic event annotation could enable event data from currently unexplored competitions, like youth or semi-professional leagues or even training sessions.



### Context

What does it take to go pro in football (soccer)? From a young age, hopeful talents devote time, money, and training to the sport. Yet, while the next superstar is guaranteed to start off in youth or semi-professional leagues, these leagues often have the fewest resources to invest. This includes resources for the collection of event data which helps generate insights into the performance of the teams and players.


Currently, event data is mostly collected manually by human operators, who gather data in several steps and through numerous personnel involved. This manual process has room for innovation as in its current shape and form it involves a lot of resources and multiple iterations/quality checks. As a result, event data collection is usually reserved for professional competitions only.

Based in Frankfurt, the [Deutsche Fußball Liga (DFL)](https://www.dfl.de/en/) manages Germany's professional football (soccer) leagues: [Bundesliga and Bundesliga 2](https://www.bundesliga.com/en/bundesliga). DFL partners with the operator of one of the largest sports databases in the world, [Sportec Solutions.](https://www.sportec-solutions.de/en/index.html) They're responsible for the leagues' sports data and sports technology activities. In addition, Sportec Solutions provides services to global sports entities and media companies.

  

Automatic event detection could provide event data faster and with greater depth. Having access to a broader range of competitions, match conditions and data scouts would be able to ensure no talented player is overlooked.



<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/dfl-bundesliga-data-shootout/overview/code-requirements) for details.**","Submissions are evaluated on the [average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) of detected events, averaged over timestamp error thresholds, averaged over event classes.

Detections are matched to ground-truth events by class-specific error tolerances, with ambiguities resolved in order of decreasing confidence. The timestamp error tolerances, in seconds, for each class are:
- **Challenge**: `[ 0.30, 0.40, 0.50, 0.60, 0.70 ]`
- **Play**: `[ 0.15, 0.20, 0.25, 0.30, 0.35 ]`
- **Throw-In**: `[ 0.15, 0.20, 0.25, 0.30, 0.35 ]`

You may find a Python implementation of the metric in this notebook: [**Competition Metric - DFL Event Detection AP**](https://www.kaggle.com/code/ryanholbrook/competition-metric-dfl-event-detection-ap).

## Detailed Description ##

Evaluation proceeds in four steps:
1. **Selection** - Predictions not within a video's scoring intervals are dropped.
2. **Assignment** - Predicted events are matched with ground-truth events.
3. **Scoring** - Each group of predictions is scored against its corresponding group of ground-truth events via Average Precision. 
4. **Reduction** - The multiple AP scores are averaged to produce a single overall score.

### Selection ###

With each video there is a defined set of **scoring intervals** giving the intervals of time over which zero or more ground-truth events might be annotated in that video. A prediction is only evaluated if it falls within a scoring interval. These scoring intervals were chosen to improve the fairness of evaluation by, for instance, ignoring edge-cases or ambiguous events.

For reference, we provide the scoring intervals for the training data. The scoring intervals for the test data will not be available to your model during evaluation, however.

### Assignment ###

For each set of predictions and ground-truths within the same `event x tolerance x video_id` group, we **match** each ground-truth to the highest-confidence unmatched prediction occurring within the allowed `tolerance`.

Some ground-truths may not be matched to a prediction and some predictions may not be matched to a ground-truth. They will still be accounted for in the scoring, however.

### Scoring ###

Collecting the events within each `video_id`, we compute an **Average Precision** score for each `event x tolerance` group. The average precision score is the area under the precision-recall curve generated by decreasing confidence `score` thresholds over the predictions. In this calculation, matched predictions over the threshold are scored as TP and unmatched predictions as FP. Unmatched ground-truths are scored as FN.

### Reduction ###

The final score is the average of the above AP scores, first averaged over `tolerance`, then over `event`.

## Submission File ##

For each video indicated by `video_id`, predict each event occurring in that video by giving the `event` type and the `time` of occurrence (in seconds) as well as a confidence `score` for that event.

The file should contain a header and have the following format:

```
video_id,time,event,score
13bfe65e_0,100.0,play,0.0
13bfe65e_0,110.5,challenge,0.07
13bfe65e_1,90.25,throwin,0.13
13bfe65e_1,105.0,play,0.2
40f283bc_0,100.0,challenge,0.27
40f283bc_0,110.5,throwin,0.33
...
```

Only predictions occurring within a video's defined **scoring intervals** will be scored; the metric ignores any predictions occurring outside of these scoring intervals. You will not be given the scoring intervals for videos in the test set."
RSNA 2022 Cervical Spine Fracture Detection,Identify cervical fractures from scans,https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/36363/logos/header.png?t=2022-07-22-15-19-11,"Image,Binary Classification,Computer Vision",883,1108,12871,"### Goal of the Competition

Over 1.5 million spine fractures occur annually in the United States alone resulting in over 17,730 spinal cord injuries annually. The most common site of spine fracture is the cervical spine. There has been a rise in the incidence of spinal fractures in the elderly and in this population, fractures can be more difficult to detect on imaging due to superimposed degenerative disease and osteoporosis. Imaging diagnosis of adult spine fractures is now almost exclusively performed with computed tomography (CT) instead of radiographs (x-rays). Quickly detecting and determining the location of any vertebral fractures is essential to prevent neurologic deterioration and paralysis after trauma.


### Context


RSNA has teamed with the [American Society of Neuroradiology (ASNR)](https://www.asnr.org/) and the [American Society of Spine Radiology (ASSR)](https://www.theassr.org/) to conduct an AI challenge competition exploring whether artificial intelligence can be used to aid in the detection and localization of cervical spine fractures.

  

To create the ground truth dataset, the challenge planning task force collected imaging data sourced from twelve sites on six continents, including approximately 3,000 CT studies. Spine radiology specialists from the ASNR and ASSR provided expert image level annotations these studies to indicate the presence, vertebral level and location of any cervical spine fractures.

  

In this challenge competition, you will try to develop machine learning models that match the radiologists' performance in detecting and localizing fractures to the seven vertebrae that comprise the cervical spine. Winners will be recognized at an event during the RSNA 2022 annual meeting.

  

For more information on the challenge, contact RSNA Informatics staff at [informatics@rsna.org](mailto:informatics@rsna.org).

[A full set of acknowledgments can be found on this page](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/overview/acknowledgements).
<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/rsna-2022-cervical-spine-fracture-detection/overview/code-requirements) for details.**","Submissions are evaluated using a weighted multi-label logarithmic loss. Each fracture sub-type is its own row for every exam, and you are expected to predict a probability for a fracture at each of the seven cervical vertebrae designated as C1, C2, C3, C4, C5, C6 and C7. There is also an any label, `patient_overall`, which indicates that a fracture of ANY kind described before exists in the examination. Fractures in the skull base, thoracic spine, ribs, and clavicles are ignored. The any label is weighted more highly than specific fracture level sub-types.

For each exam Id, you must submit a set of predicted probabilities (a separate row for each cervical level subtype). We then take the log loss for each predicted probability versus its true label.

The binary weighted log loss function for label j on exam i is specified as:
$$ 
L\_{ij} = - w\_j \* [ y_{ij}\*log(p\_{ij}) + (1-y\_{ij})\*log(1-p\_{ij}) ]
$$

 Finally, loss is averaged across all rows.

There will be 8 rows per image Id. The label indicated by a particular row will look like [image Id]_[Sub-type Name], as follows. There is also a target column, `fractured`, indicating the probability of whether a fracture exists at the specified level. For each image ID in the test set, you must predict a probability for each of the different possible sub-types and the patient overall. The file should contain a header and have the following format:
```
row_id,fractured
1_C1,0
1_C2,0
1_C3,0
1_C4,0.6
1_C5,0
1_C6,0.9
1_C7,0.01
1_patient_overall,0.99
2_C1,0
etc.
```"
Google Universal Image Embedding,Create image representations that work across many visual domains,https://www.kaggle.com/competitions/google-universal-image-embedding,https://storage.googleapis.com/kaggle-competitions/kaggle/36414/logos/header.png?t=2022-07-06-20-40-23,"Image,Multiclass Classification",1022,1217,20984,"Welcome to the Universal Image Embedding competition! After hosting challenges in the domain of landmarks for the past four years, this year we introduce the first competition in image representations that should work across many object types.

Image representations are a critical building block of computer vision applications. Traditionally, research on image embedding learning has been conducted with a focus on per-domain models. Generally, papers propose generic embedding learning techniques which are applied to different domains separately, rather than developing generic embedding models which could be applied to all domains combined.

In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same object as the query). The images in our dataset comprise a variety of object types, such as apparel, artwork, landmarks, furniture, packaged goods, among others.

This year's competition is structured in a representation learning format: you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality. Both Tensorflow and PyTorch models are supported.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/google-universal-image-embedding/overview/code-requirements) for details.**

Cover image credits: [Chris Schrier, CC-BY](https://www.flickr.com/photos/schrierc/5502246181); [Petri Krohn, GNU Free Documentation License](https://commons.wikimedia.org/wiki/File:MOMA_chairs_2.jpg); [Drazen Nesic, CC0](https://pixnio.com/media/cartoon-textile-texture-funny-giraffe); [Marco Verch Professional Photographer, CCBY](https://www.flickr.com/photos/30478819@N08/44289962475); [Grendelkhan, CCBY](https://commons.wikimedia.org/wiki/File:Waymo_self-driving_car_side_view.gk.jpg); [Bobby Mikul, CC0](https://www.publicdomainpictures.net/en/view-image.php?image=16166&picture=empire-state-building); [Vincent Van Gogh, CC0](https://www.rawpixel.com/image/537438/the-starry-night-van-gogh); [pxhere.com, CC0](https://pxhere.com/en/photo/479766); [Smart Home Perfected, CC-BY](https://www.flickr.com/photos/smarthomeperfected/51048330253). ","## Metric

Submissions are evaluated according to the mean Precision @ 5 metric, where we introduce a small modification to avoid penalizing queries with fewer than 5 expected index images. In detail, the metric is computed as follows:

$$mP@5 = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{min(n\_q, 5)} \sum\_{j=1}^{min(n\_q, 5)} rel\_q(j)$$

where:

- \\(Q\\) is the number of query images
- \\(n\_q\\) is the number of index images containing an object in common with the query image \\(q\\). Note that \\(n_q \gt 0\\).
- \\(rel_q(j)\\) denotes the relevance of prediciton \\(j\\) for the \\(q\\)-th query: it’s 1 if the \\(j\\)-th prediction is correct, and 0 otherwise

## Submission File

Unlike a traditional Kaggle code competition where notebooks are rerun top-to-bottom on a private test set, in this competition you will be submitting a model file. The model must take an image as an input, and return a float vector (i.e., the image embedding) at the output. The embedding dimensionality should be no greater than **64**. Your model must be packaged into a `submission.zip` file, and compatible with either TensorFlow 2.6.4 or Pytorch 1.11.0. In most cases, scoring is expected to take a few hours to complete.

The `submission.zip` should contain one (and only one) of the following:

- files and directories in the Tensorflow's [SavedModel format](https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format).
- file in PyTorch’s [TorchScript format](https://pytorch.org/docs/stable/jit.html), named “saved_model.pt”.

To emphasize once again: the embedding produced by the model must be at most 64D (if not, the submission will error out).

## Submission process

Kaggle will use the submitted model to:

 1. Extract embeddings for the private test and index image sets.
 2. Create a kNN (k = 5) lookup for each test sample, using the Euclidean distance between test and index embeddings.
 3. Score the quality of the lookups using the competition metric.

## Baseline kernels

To submit your model, all you need is a kernel that produces a `submission.zip` file containing the model. Here are examples:

**Tensorflow.** See [this kernel](https://www.kaggle.com/code/francischen1991/tf-baseline-v2-submission) for the minimal baseline model and [this kernel](https://www.kaggle.com/andrefaraujo/tf-baseline-submission) for the more complicated baseline model.

**PyTorch.** See [this kernel](https://www.kaggle.com/andrefaraujo/pytorch-baseline-submission).

## Code example to produce model

For **Tensorflow**, we provide a minimal **Tensorflow** baseline model based on pretrained Inception-v3 model with a linear layer to project the output feature to 64D.  In the end, the model is saved in the SavedModel format: 

```
import tensorflow as tf

image = tf.keras.layers.Input([None, None, 3], dtype=tf.uint8)
output = tf.cast(image, tf.float32)
output = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [224, 224]), name='resize')(output)
output = tf.keras.applications.inception_v3.preprocess_input(output)
output = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', input_shape=[None, None, 3],
                                                        include_top=False, pooling='avg')(output)
output = tf.keras.layers.Dense(64, name='embedding')(output)
output = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')(output)
model = tf.keras.Model(inputs=[image], outputs=[output])
model.summary()

tf.saved_model.save(model, 'inceptionv3/')
```
For more complicated examples, please check the example given in [this repository](https://github.com/google-research/google-research/tree/master/universal_embedding_challenge). Specifically, [this script](https://github.com/google-research/google-research/blob/master/universal_embedding_challenge/export_saved_model.py) shows an example of model exporting, for a model that is trained with [this script](https://github.com/google-research/google-research/blob/master/universal_embedding_challenge/train.py).

For **PyTorch**, see a similar example below that uses pretrained Inception-v3 model and a linear projection layer, and exports the model in the TorchScript format:

```
import torch
import torch.nn as nn
from torchvision import models
from torchvision import transforms

class MyModel(nn.Module):
  def __init__(self):
    super().__init__()
    inception_model = models.inception_v3(pretrained=True)
    inception_model.fc = nn.Linear(2048, 64)
    self.feature_extractor = inception_model
            
  def forward(self, x):
    x = transforms.functional.resize(x,size=[224, 224])
    x = x/255.0
    x = transforms.functional.normalize(x, 
                                            mean=[0.485, 0.456, 0.406], 
                                            std=[0.229, 0.224, 0.225])
    return self.feature_extractor(x).logits
    
model = MyModel()
model.eval()
saved_model = torch.jit.script(model)
saved_model.save('saved_model.pt')
```

## Feature extraction code

When your model is submitted, we run it in the competition images in order to extract embeddings. Please find below code snippets that show how the model is used in detail. These could be helpful for participants to make sure that their models are extracting embeddings correctly.

**Tensorflow**:

```
import numpy as np
from PIL import Image
import tensorflow as tf

# Model loading.
model = tf.saved_model.load(saved_model_path)
embedding_fn = model.signatures[""serving_default""]

# Load image and extract its embedding.
image_tensor = tf.convert_to_tensor(
        np.array(Image.open(image_path).convert(""RGB""))
    )
expanded_tensor = tf.expand_dims(image_tensor, axis=0)
embedding = embedding_fn(expanded_tensor)[""embedding_norm""]
```

**PyTorch**:

```
from PIL import Image
import torch
from torchvision import transforms

# Model loading.
model = torch.jit.load(saved_model_path)
model.eval()
embedding_fn = model

# Load image and extract its embedding.
input_image = Image.open(image_path).convert(""RGB"")
convert_to_tensor = transforms.Compose([transforms.PILToTensor()])
input_tensor = convert_to_tensor(input_image)
input_batch = input_tensor.unsqueeze(0)
with torch.no_grad():
  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()
```
"
Mayo Clinic - STRIP AI,Image Classification of Stroke Blood Clot Origin,https://www.kaggle.com/competitions/mayo-clinic-strip-ai,https://storage.googleapis.com/kaggle-competitions/kaggle/37333/logos/header.png?t=2022-06-29-00-47-20,"Image,Medicine,Computer Vision,Classification",888,1025,6980,"### Goal of the Competition
The goal of this competition is to classify the blood clot origins in ischemic stroke. Using whole slide digital pathology images, you'll build a model that differentiates between the two major acute ischemic stroke (AIS) etiology subtypes: cardiac and large artery atherosclerosis.

Your work will enable healthcare providers to better identify the origins of blood clots in deadly strokes, making it easier for physicians to prescribe the best post-stroke therapeutic management and reducing the likelihood of a second stroke.

### Context
Stroke remains the second-leading cause of death worldwide. Each year in the United States, over 700,000 individuals experience an ischemic stroke caused by a blood clot blocking an artery to the brain. A second stroke (23% of total events are recurrent) worsens the chances of the patient’s survival. However, subsequent strokes may be mitigated if physicians can determine stroke etiology, which influences the therapeutic management following stroke events. 

During the last decade, mechanical thrombectomy has become the standard of care treatment for acute ischemic stroke from large vessel occlusion. As a result, retrieved clots became amenable to analysis. Healthcare professionals are currently attempting to apply deep learning-based methods to predict ischemic stroke etiology and clot origin. However, unique data formats, image file sizes, as well as the number of available pathology slides create challenges you could lend a hand in solving.

The Mayo Clinic is a nonprofit American academic medical center focused on integrated health care, education, and research. Stroke Thromboembolism Registry of Imaging and Pathology (STRIP) is a uniquely large multicenter project led by Mayo Clinic Neurovascular Lab with the aim of histopathologic characterization of thromboemboli of various etiologies and examining clot composition and its relation to mechanical thrombectomy revascularization.

To decrease the chances of subsequent strokes, the Mayo Clinic Neurovascular Research Laboratory encourages data scientists to improve artificial intelligence-based etiology classification so that physicians are better equipped to prescribe the correct treatment. New computational and artificial intelligence approaches could help save the lives of stroke survivors and help us better understand the world's second-leading cause of death.","Submissions are evaluated using a **weighted multi-class logarithmic loss**. The overall effect is such that each class is roughly equally important for the final score.

Each image has been labeled with an etiology class, either **CE** or **LAA**. For each image, you must submit a probability for each class. The formula is then:

<p>$$\text{Log Loss} = - \left( \frac{\sum^{M}_{i=1} w_{i} \cdot \sum_{j=1}^{N_{i}} \frac{y_{ij}}{N_{i}} \cdot \ln  p_{ij} }{\sum^{M}_{i=1} w_{i}} \right)$$</p>

<p>where \(N\) is the number of images in the class set, \(M\) is the number of classes,  \(\ln\) is the natural logarithm, \(y_{ij}\) is 1 if observation \(i\) belongs to class \(j\) and 0 otherwise, \(p_{ij}\) is the predicted probability that image \(i\) belongs to class \(j\). </p>

<p>The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, each predicted probability \(p\) is replaced with \(\max(\min(p,1-10^{-15}),10^{-15})\).</p>

## Submission File ##
For each `patient_id` in the test set, you must predict a probability for each of the two etiology classes. The file should contain a header and have the following format:

```
patient_id,CE,LAA
01f2b3,0.5,0.5
04de22,0.5,0.5
0a47c9,0.5,0.5
0af8b6,0.5,0.5
...
```"
Tabular Playground Series - Jul 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-jul-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33107/logos/header.png?t=2021-12-30-01-27-41,"Tabular,Clustering",1253,1278,16346,"Welcome to Kaggle's first ever unsupervised clustering challenge!

In this challenge, you are given a dataset where each row belongs to a particular cluster. Your job is to predict the cluster each row belongs to. You are not given any training data, and you are not told how many clusters are found in the ground truth labels. 

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/laura-rivera-ArH3dtoDQc0-unsplash.jpg"" style=""float:center; height:333px; width:500px"">


## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.

**Good luck and have fun!**

*Photo above by <a href=""https://unsplash.com/photos/ArH3dtoDQc0"">Laura Rivera</a> on Unsplash*
  
  ","Submissions are evaluated on the [Adjusted Rand Index](https://en.wikipedia.org/wiki/Rand_index) between the ground truth cluster labels of the data and your predicted cluster labels. You are not given the number of ground truth clusters or any training labels. This is a completely unsupervised problem


## Submission File
For each `Id` row in the data, you must predict the cluster of rows it belongs to `Predicted`. The file should contain a header and have the following format:

    Id,Predicted
    0,2
    1,1
    2,7
    etc.
"
HuBMAP + HPA - Hacking the Human Body,Segment multi-organ functional tissue units,https://www.kaggle.com/competitions/hubmap-organ-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/34547/logos/header.png?t=2022-02-15-22-37-27,"Image,Biology,Computer Vision",1175,1517,39568,"When you think of “life hacks,” normally you’d imagine productivity techniques. But how about the kind that helps you understand your body at a molecular level? It may be possible! Researchers must first determine the function and relationships among the 37 trillion cells that make up the human body. A better understanding of our cellular composition could help people live healthier, longer lives.

A previous [Kaggle competition](https://www.kaggle.com/c/hubmap-kidney-segmentation) aimed to annotate cell population neighborhoods that perform an organ’s main physiologic function, also called functional tissue units (FTUs). Manually annotating FTUs (e.g., glomeruli in kidney or alveoli in the lung) is a time-consuming process. In the average kidney, there are over 1 million glomeruli FTUs. While there are existing cell and FTU segmentation methods, we want to push the boundaries by building algorithms that generalize across different organs and are robust across different dataset differences. 

The [Human BioMolecular Atlas Program](https://hubmapconsortium.org/) (HuBMAP) is working to create a [Human Reference Atlas](https://www.nature.com/articles/s41556-021-00788-6) at the cellular level. Sponsored by the National Institutes of Health (NIH), HuBMAP and Indiana University’s Cyberinfrastructure for Network Science Center (CNS) have partnered with institutions across the globe for this endeavor. A major partner is the [Human Protein Atlas](https://www.proteinatlas.org/) (HPA), a Swedish research program aiming to map the protein expression in human cells, tissues, and organs, funded by the Knut and Alice Wallenberg Foundation.

In this competition, you’ll identify and segment functional tissue units (FTUs) across five human organs. You'll build your model using a dataset of tissue section images, with the best submissions segmenting FTUs as accurately as possible.

If successful, you'll help accelerate the world’s understanding of the relationships between cell and tissue organization. With a better idea of the relationship of cells, researchers will have more insight into the function of cells that impact human health. Further, the Human Reference Atlas constructed by HuBMAP will be freely available for use by researchers and pharmaceutical companies alike, potentially improving and prolonging human life.



&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/hubmap-organ-segmentation/overview/code-requirements) for details.**

",
Tabular Playground Series - Jun 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-jun-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33106/logos/header.png?t=2021-12-30-01-27-03,Tabular,844,886,5984,"The June edition of the 2022 Tabular Playground series is all about data imputation. The dataset has similarities to the [May 2022 Tabular Playground](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/overview), except that there are no targets. Rather, there are missing data values in the dataset, and your task is to predict what these values should be.

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/mika-baumeister-Wpnoqo2plFA-unsplash.jpg"" style=""float:center; height:250px; width:400px"">

## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn. We've also provided a [notebook](https://www.kaggle.com/inversion/get-started-with-mean-imputation) to get people started

*Good luck and have fun!*

### Acknowledgments

Photo by <a href=""https://unsplash.com/@mbaumi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Mika Baumeister</a> on <a href=""https://unsplash.com/s/photos/question-mark-data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash.</a>
  ","Submissions are scored on the root mean squared error. RMSE is defined as:

$$
\textrm{RMSE} = \sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}_i)^2}
$$

where \\( \hat{y}\_i \\) is the predicted value and \\(y\_i\\) is the original value for each instance \\(i\\).

## Submission File
For each `row-col` pair in the in the `sample_submission.csv` file (corresponding to all of the missing values found in `data.csv`), you must predict the missing value of that data point. The file should contain a header and have the following format:

```
row-col, value
0-F_1_14, 1.54
0-F_3_23, -0.56
1-F_3_24, 0.01
etc.
```"
American Express - Default Prediction,Predict if a customer will default in the future,https://www.kaggle.com/competitions/amex-default-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/35332/logos/header.png?t=2022-03-23-01-05-50,"Tabular,Binary Classification,Finance",4874,6003,90058,"Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we’ll pay back what we charge? That’s a complex problem with many existing solutions—and even more potential improvements, to be explored in this competition.

Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use.

American Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success. 

In this competition, you’ll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You're free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model.

If successful, you'll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world's largest payment card issuer—earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.
","The evaluation metric, \\(M\\), for this competition is the mean of two measures of rank ordering: Normalized Gini Coefficient, \\(G\\), and default rate captured at 4%, \\(D\\).

$$
M = 0.5 \cdot \( G + D \)
$$

The default rate captured at 4% is the percentage of the positive labels (defaults) captured within the highest-ranked 4% of the predictions, and represents a Sensitivity/Recall statistic.

For both of the sub-metrics \\(G\\) and \\(D\\), the negative labels are given a weight of 20 to adjust for downsampling.

This metric has a maximum value of 1.0.

Python code for calculating this metric can be found in [this Notebook](https://www.kaggle.com/code/inversion/amex-competition-metric-python).

## Submission File
For each `customer_ID` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    customer_ID,prediction
    00000469ba...,0.01
    00001bf2e7...,0.22
    0000210045...,0.98
    etc.
"
Feedback Prize - Predicting Effective Arguments,"Rate the effectiveness of argumentative writing elements from students grade 6-12

",https://www.kaggle.com/competitions/feedback-prize-effectiveness,https://storage.googleapis.com/kaggle-competitions/kaggle/35308/logos/header.png?t=2022-05-12-15-29-47,"NLP,Text,Primary and Secondary Schools",1557,1910,29139,"###Goal of the Competition
The goal of this competition is to classify argumentative elements in student writing as ""effective,"" ""adequate,"" or ""ineffective."" You will create a model trained on data that is representative of the 6th-12th grade population in the United States in order to minimize bias. Models derived from this competition will help pave the way for students to receive enhanced feedback on their argumentative writing. With automated guidance, students can complete more assignments and ultimately become more confident, proficient writers.    

This competition will comprise two tracks. The first track will be a traditional track in which accuracy of classification will be the only metric used for success. Success on this track will be updated on the Kaggle leaderboard. Prize money for the accuracy-only, “Leaderboard Prize” track will be $25,000.    

The second track will measure computational efficiency in which efficiency is determined using a combination of accuracy and the speed at which models are able to generate these predictions. We are hosting this track because highly accurate models are often computationally heavy. Such models have a stronger carbon footprint and frequently prove difficult to utilize in real-world educational contexts, since most educational organizations have limited computational capabilities. Weekly updates on models based on computational efficiency will be posted in the discussion forum. Prize money for the computational, “Efficiency Prize” track will be $30,000.

You can find more details about the [Efficiency Prize Evaluation](https://www.kaggle.com/competitions/feedback-prize-effectiveness/overview/efficiency-prize-evaluation) via the side tab.


###Context

Writing is crucial for success. In particular, argumentative writing fosters critical thinking and civic engagement skills, and can be strengthened by practice. However, only 13 percent of eighth-grade teachers ask their students to write persuasively each week. Additionally, resource constraints disproportionately impact Black and Hispanic students, so they are more likely to write at the “below basic” level as compared to their white peers. An automated feedback tool is one way to make it easier for teachers to grade writing tasks assigned to their students that will also improve their writing skills.    

There are numerous automated writing feedback tools currently available, but they all have limitations, especially with argumentative writing. Existing tools often fail to evaluate the quality of argumentative elements, such as organization, evidence, and idea development. Most importantly, many of these writing tools are inaccessible to educators due to their cost, which most impacts already underserved schools.    

[Georgia State University (GSU)](https://www.gsu.edu) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor’s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.    

To best prepare all students, GSU and [The Learning Agency Lab](https://www.the-learning-agency-lab.com) have joined forces to encourage data scientists to improve automated writing assessments. This public effort could also encourage higher quality and more accessible automated writing tools. If successful, students will receive more feedback on the argumentative elements of their writing and will apply the skill across many disciplines.

###Acknowledgements    

Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible. 


<a href=""https://www.gatesfoundation.org/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/BMGF_logo_black_300dpi%20(1).jpg"" style=""width: 200px""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://schmidtfutures.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/Schmidt%20Futures%20Logo.png"" style=""width: 250px"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://chanzuckerberg.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/1200px-Chan_Zuckerberg_Initiative.svg.png"" style=""width: 100px"">&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</a>


<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/feedback-prize-effectiveness/overview/code-requirements) for details.**","The first track of this competition focuses on accuracy of classification. Submissions for this track are evaluated using **multi-class logarithmic loss**. Each row in the dataset has been labeled with one true effectiveness label. For each row, you must submit the predicted probabilities that the product belongs to each quality label. The formula is:

$$ \text{log loss} = -\frac{1}{N}\sum\_{i=1}^N\sum\_{j=1}^My_{ij}\log(p\_{ij}), $$

where \\(N\\) is the number of rows in the test set, \\(M\\) is the number of class labels, \\( \text{log}\\) is the natural logarithm, \\(y\_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p\_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\). The logarithm in this case is the natural logarithm.

The submitted probabilities for a given discourse element are not required to sum to one: they are rescaled prior to being scored, each row being divided by the row sum. In order to avoid the extremes of the \\(\text{log}\\) function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).

## Submission File ##

You must submit a CSV file with each discourse element's `discourse_id` and a predicted probability for each of the three effectiveness ratings. The order of the rows does not matter. The file must have a header and should have the following format:

```
discourse_id,Ineffective,Adequate,Effective
a261b6e14276,0.2,0.6,0.4
5a88900e7dc1,3.0,6.0,1.0
9790d835736b,1.0,2.0,3.0
75ce6d68b67b,0.33,0.34,0.33
93578d946723,0.01,0.24,0.47
2e214524dbe3,0.2,0.6,0.4
```"
Google Smartphone Decimeter Challenge 2022,Improve high precision GNSS positioning and navigation accuracy on smartphones,https://www.kaggle.com/competitions/smartphone-decimeter-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/35779/logos/header.png?t=2022-04-29-21-07-48,"Geospatial Analysis,Signal Processing,Research,Tabular,Mobile and Wireless",573,684,10270,"### Goal of the Competition

The goal of this competition is to compute smartphones location down to the decimeter or even centimeter resolution which could enable services that require lane-level accuracy such as HOV lane ETA estimation. You'll develop a model based on raw location measurements from Android smartphones collected in opensky and light urban roads using datasets collected by the host.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/Smartphone/jared-murray-NSuufgf-BME-unsplash.jpg"" width=""200"" style=""float: right""></img>

Your work will help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with improved granularity. As a result, new navigation methods could be built upon the more precise data. 

### Context

Have you ever missed the lane change before a highway exit? Do you want to know the estimated time of arrival (ETA) of a carpool lane rather than other lanes? These and other useful features require precise smartphone positioning services. Machine learning models can improve the accuracy of Global Navigation Satellite System (GNSS) data. With more refined data, billions of Android phone users could have a more fine-tuned positioning experience.

GNSS chipsets provide raw measurements, which can be used to compute the smartphone’s position. Current mobile phones only offer 3-5 meters of positioning accuracy. For advanced use cases, the results are not fine enough nor reliable. Urban obstructions create the largest barriers to GPS accuracy. The data in this challenge includes only traces collected on opensky and light urban roads. These highways and main streets are the most widely used roads and will test the limits of smartphone positioning.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/Smartphone/tobias-rademacher-p79nyt2CUj4-unsplash.jpg"" width=""300"" style=""float: right""></img>

The Android GPS team in Google hosted the [Smartphone Decimeter Challenge in 2021](https://kaggle.com/competitions/google-smartphone-decimeter-challenge/overview). Works by the three winners were presented at the ION GNSS+ 2021 Conference. This year, co-sponsored by the Institute of Navigation, this competition continues to seek advanced research in smartphone GNSS positioning accuracy and help people better navigate the world around them. In order to build upon last year’s progress, the data also includes traces from the 2021 competition.

Future competitions could include traces collected in harsher environments, such as deep urban areas with obstacles to satellite signals. Your efforts in this competition could impact how this more difficult data is interpreted. With decimeter level position accuracy, mobile users could gain better lane-level navigation, AR walk/drive, precise agriculture via phones, and greater specificity in the location of road safety issues. It will also enable a more personalized fine tuned navigation experience.

*Photos by <a href=""https://unsplash.com/@jaredmurray?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Jared Murray</a>, <a href=""https://unsplash.com/@_th4d_?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Thaddaeus Lim</a> and <a href=""https://unsplash.com/@tobbes_rd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Tobias Rademacher</a> on <a href=""https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash.</a>*","Submissions are scored on the mean of the 50th and 95th percentile distance errors. For every `phone` and once per second, the horizontal distance (in meters) is computed between the predicted latitude/longitude and the ground truth latitude/longitude. These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller). The 50th and 95th percentile errors are then averaged for each phone. Lastly, the mean of these averaged values is calculated across all phones in the test set.

## Submission File
For each `phone` and `UnixTimeMillis` in the sample submission, you must predict the latitude and longitude. The sample submission typically requires a prediction once per second but may include larger gaps if there were too few valid GNSS signals. The submission file should contain a header and have the following format:

```
phone,UnixTimeMillis,LatitudeDegrees,LongitudeDegrees
2020-05-15-US-MTV-1_Pixel4,1273608785432,37.904611315634504,-86.48107806249548
2020-05-15-US-MTV-1_Pixel4,1273608786432,37.904611315634504,-86.48107806249548
2020-05-15-US-MTV-1_Pixel4,1273608787432,37.904611315634504,-86.48107806249548
```
"
Tabular Playground Series - May 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-may-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33105/logos/header.png?t=2021-12-30-01-26-16,Tabular,1151,1176,8902,"The May edition of the 2022 Tabular Playground series binary classification problem that includes a number of different feature interactions. This competition is an opportunity to explore various methods for identifying and exploiting these feature interactions.

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Tabular%20Playground/clarisse-croset--tikpxRBcsA-unsplash.jpg"" style=""float:center; height:350px; width:304px"">

## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.

We've also built a [starter notebook](https://www.kaggle.com/code/paultimothymooney/getting-started-with-tensorflow-decision-forests) for you that uses TensorFlow Decision Forests, a TensorFlow library that matches the power of XGBoost with a friendly, straightforward user interface.
 
*Good luck and have fun!*

### Acknowledgments

Photo by <a href=""https://unsplash.com/@herfrenchness?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Clarisse Croset</a> on <a href=""https://unsplash.com/s/photos/feature-interactions-machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash.</a>
  ","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    900000,0.65
    900001,0.97
    900002,0.02
    etc.
"
UW-Madison GI Tract Image Segmentation ,Track healthy organs in medical scans to improve cancer treatment,https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25,"Image,Medicine",1548,2078,40956,"In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines.  This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process.  A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.  

The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.  

In this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment.  You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.

![Description Image](https://lh5.googleusercontent.com/zbBUgbj1jyZxyu3r1vr5zKKr8yK1hSdwAM3HpD_n6j2W-5-wKP3ZRusi_3yskSgnC-tMRKqOEtLycbLkTWCJAUe4Cylv_VsW81DYI4ray02uZLeSnlzAuZRIU7L2Q0KURYSMqFI)

*In this figure, the tumor (pink thick line) is close to the stomach (red thick line).  High doses of radiation are directed to the tumor while avoiding the stomach.  The dose levels are represented by the rainbow of outlines, with higher doses represented by red and lower doses represented by green.*
  
Cancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control.

## Acknowledgments:

Sangjune Laurence Lee MSE MD FRCPC DABR
Poonam Yadav  Ph.D., DABR
Yin Li PhD
Jason J. Meudt BS, RTT
Jessica Strang
Dustin Hebel
Alyx Alfson MS CMD, R.T.(T)
Stephanie J. Olson RTT (BS), CMD (MS)
Tera R. Kruser MS, RTT, CMD
Jennifer B Smilowitz, Ph.D., DABR, FAAPM
Kailee Borchert
Brianne Loritz
John Bayouth PhD
Michael Bassetti MD PhD

**Work funded by the University of Wisconsin Carbone Cancer Center Pancreas Pilot Research Grant.**

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/uw-madison-gi-tract-image-segmentation/overview/code-requirements) for details.**","<p>This competition is evaluated on the mean <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"">Dice coefficient</a> and <a href=""https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx""> 3D Hausdorff distance</a>. The Dice coefficient can be used to compare the pixel-wise&nbsp;agreement&nbsp;between a predicted segmentation&nbsp;and its corresponding ground truth. The formula is given by:</p>
<p>$$&nbsp;\frac{2 * |X \cap Y|}{|X| + |Y|},$$</p>
<p>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 0 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.</p>
<p>Hausdorff distance is a method for calculating the distance between segmentation objects A and B, by calculating the furthest point on object A from the nearest point on object B. For 3D Hausdorff, we construct 3D volumes by combining each 2D segmentation with slice depth as the Z coordinate and then find the Hausdorff distance between them. <b>(In this competition, the slice depth for all scans is set to 1.)</b> The <a href=""https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx"">scipy code for Hausdorff</a> is linked. The expected / predicted pixel locations are normalized by image size to create a bounded 0-1 score.</p>
<p>The two metrics are combined, with a weight of 0.4 for the Dice metric and 0.6 for the Hausdorff distance.</p>
<h2>Submission File</h2>
<p><strong>In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.</strong>&nbsp; Instead of submitting an exhaustive list of indices for your&nbsp;segmentation, you&nbsp;will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and&nbsp;running a total of 3 pixels (1,2,3).</p>
<p>Note that, at the time of encoding, the mask should be <strong>binary</strong>, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.</p>
<p>The competition format requires&nbsp;a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded&nbsp;pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.</p>
<p>The file should contain a header and have the following format:</p>
<pre>id,class,predicted<br>1,large_bowel,1 1 5 1<br>1,small_bowel,1 1<br>1,stomach,1 1<br>2,large_bowel,1 5 2 17<br>etc.</pre>"
Foursquare - Location Matching,Match point of interest data across datasets,https://www.kaggle.com/competitions/foursquare-location-matching,https://storage.googleapis.com/kaggle-competitions/kaggle/35476/logos/header.png?t=2022-03-22-18-37-04,"Tabular,Geography,Business",1079,1290,22050,"When you look for nearby restaurants or plan an errand in an unknown area, you expect relevant, accurate information. To maintain quality data worldwide is a challenge, and one with implications beyond navigation. Businesses make decisions on new sites for market expansion, analyze the competitive landscape, and show relevant ads informed by location data. For these, and many other uses, reliable data is critical.

Large-scale datasets on commercial points-of-interest (POI) can be rich with real-world information. To maintain the highest level of accuracy, the data must be matched and de-duplicated with timely updates from multiple sources. De-duplication involves many challenges, as the raw data can contain noise, unstructured information, and incomplete or inaccurate attributes. A combination of machine-learning algorithms and rigorous human validation methods are optimal to de-dupe datasets.

With 12+ years of experience perfecting such methods, Foursquare is the #1 independent provider of global POI data. The leading independent location technology and data cloud platform, Foursquare is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare’s tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes.

In this competition, you’ll match POIs together. Using a dataset of over one-and-a-half million Places entries heavily altered to include noise, duplications, extraneous, or incorrect information, you'll produce an algorithm that predicts which Place entries represent the same point-of-interest. Each Place entry includes attributes like the name, street address, and coordinates. Successful submissions will identify matches with the greatest accuracy.

By efficiently and successfully matching POIs, you'll make it easier to identify where new stores or businesses would benefit people the most. 

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/foursquare-location-matching/overview/code-requirements) for details.**","Submissions are evaluated by the mean [Intersection over Union](https://en.wikipedia.org/wiki/Jaccard_index) (IoU, aka the Jaccard index) of the ground-truth entry matches and the predicted entry matches. The mean is taken sample-wise, meaning that an IoU score is calculated for each row in the submission file, and the final score is their average.

## Submission File ##

For each place entry `id` in the test set, you should submit a space-delimited list of matching place `id`s. *Places always self-match*, so the list of matches for an `id` should always contain that `id`.

The file should contain a header, be named `submission.csv`, and have the following format:
```
id,matches
E_00001118ad0191,E_00001118ad0191
E_000020eb6fed40,E_000020eb6fed40
E_00002f98667edf,E_00002f98667edf
E_001b6bad66eb98,E_001b6bad66eb98 E_0283d9f61e569d
E_0283d9f61e569d,E_0283d9f61e569d E_001b6bad66eb98
```
You should predict matches for every `id`. For example, if you believe `A` matches `B` and `C`, your submission file should include rows `A,A B C`, but also `B,B A C` and `C,C A B`."
JPX Tokyo Stock Exchange Prediction,Explore the Tokyo market with your data science skills,https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/34349/logos/header.png?t=2022-03-09-00-33-57,"Tabular,Finance",2033,1323,1572,"Success in any financial market requires one to identify solid investments. When a stock or derivative is undervalued, it makes sense to buy. If it's overvalued, perhaps it's time to sell. While these finance decisions were historically made manually by professionals, technology has ushered in new opportunities for retail investors. Data scientists, specifically, may be interested to explore quantitative trading, where decisions are executed programmatically based on predictions from trained models.

There are plenty of existing quantitative trading efforts used to analyze financial markets and formulate investment strategies. To create and execute such a strategy requires both historical and real-time data, which is difficult to obtain especially for retail investors. This competition will provide financial data for the Japanese market, allowing retail investors to analyze the market to the fullest extent.

Japan Exchange Group, Inc. (JPX) is a holding company operating one of the largest stock exchanges in the world, Tokyo Stock Exchange (TSE), and derivatives exchanges Osaka Exchange (OSE) and Tokyo Commodity Exchange (TOCOM). JPX is hosting this competition and is supported by AI technology company AlpacaJapan Co.,Ltd.

This competition will compare your models against real future returns after the training phase is complete. The competition will involve building portfolios from the stocks eligible for predictions (around 2,000 stocks). Specifically, each participant ranks the stocks from highest to lowest expected returns and is evaluated on the difference in returns between the top and bottom 200 stocks. You'll have access to financial data from the Japanese market, such as stock information and historical stock prices to train and test your model.

All winning models will be made public so that other participants can learn from the outstanding models. Excellent models also may increase the interest in the market among retail investors, including those who want to practice quantitative trading. At the same time, you'll gain your own insights into programmatic investment methods and portfolio analysis―and you may even discover you have an affinity for the Japanese market.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/jpx-tokyo-stock-exchange-prediction/overview/code-requirements) for details.**","Submissions are evaluated on [the Sharpe Ratio](https://en.wikipedia.org/wiki/Sharpe_ratio) of the daily spread returns. You will need to rank each stock active on a given day. The returns for a single day treat the 200 highest (e.g. 0 to 199) ranked stocks as purchased and the lowest (e.g. 1999 to 1800) ranked 200 stocks as shorted. The stocks are then weighted based on their ranks and the total returns for the portfolio are calculated assuming the stocks were purchased the next day and sold the day after that. You can find a [python implementation of the metric here](https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition).

You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:

```
import jpx_tokyo_market_prediction
env = jpx_tokyo_market_prediction.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test files
for (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:
    sample_prediction_df['Rank'] = np.arange(len(sample_prediction))  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
```

You will get an error if you:
- Use ranks that are below zero or greater than or equal to the number of stocks for a given date.
- Submit any duplicated ranks.
- Change the order of the rows."
Image Matching Challenge 2022,Register two images from different viewpoints,https://www.kaggle.com/competitions/image-matching-challenge-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/34970/logos/header.png?t=2022-03-30-01-06-45,"Image,Computer Vision",642,829,14170,"For most of us, our best camera is part of the phone in our pocket. We may take a snap of a landmark, like the Trevi Fountain in Rome, and share it with friends. By itself, that photo is two-dimensional and only includes the perspective of our shooting location. Of course, a lot of people have taken photos of that fountain. Together, we may be able to create a more complete, three-dimensional view. What if machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet?

The process to reconstruct 3D objects and buildings from images is called Structure-from-Motion (SfM). Typically, these images are captured by skilled operators under controlled conditions, ensuring homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, lighting and weather conditions, occlusions from people and vehicles, and even user-applied filters. 

<img src=""https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/trevi-canvas-licensed-nonoderivs.jpg"" style=""center; width=""540"">

The first part of the problem is to identify which parts of two images capture the same physical points of a scene, such as the corners of a window. This is typically achieved with local features (key locations in an image that can be reliably identified across different views). Local features contain short description vectors that capture the appearance around the point of interest. By comparing these descriptors, likely correspondences can be established between the pixel coordinates of image locations across two or more images. This “image registration” makes it possible to recover the 3D location of the point by triangulation.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/image3.gif"" style=""float: right; width: 250px"">

Google employs Structure-from-Motion techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic, and better leverage the volume of data already publicly available, Google presents this competition in collaboration with the University of British Columbia and Czech Technical University.

In this code competition, you’ll create a machine learning algorithm that registers two images from different viewpoints. With access to a dataset of thousands of images to train and test your model, top-scoring notebooks will do so with the most accuracy.

If successful, you'll help solve this well-known problem in computer vision, making it possible to map the world with unstructured image collections. Your solutions will have applications in photography and cultural heritage preservation, along with Google Maps. Winners will also be invited to give a presentation as part of the Image Matching: Local Features and Beyond workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) in June.


### Resources

* [Image Matching Challenge 2021](https://www.cs.ubc.ca/research/image-matching-challenge/current/): Last year's competition (outside Kaggle).

### Organization

Eduard Trulls (Google), Yuhe Jin & Kwang Moo Yi (University of British Columbia, Vancouver, Canada), Dmytro Mishkin & Jiri Matas (Czech Technical University, Prague, Czech Republic)

### Acknowledgments

The organizers would like to thank the [Machine Learning Lab](https://apps.ucu.edu.ua/en/mllab/) at the [Faculty of Applied Sciences, Ukrainian Catholic University](https://apps.ucu.edu.ua/en/) (Lviv, Ukraine) for their help with dataset creation.
<br>

Banner photo by <a href=""https://unsplash.com/@tanelah?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Taneli Lahtinen</a> on Unsplash.  Trevi Fountain photos, left to right, then top to bottom: [sarah|rose](https://www.flickr.com/photos/sarah_rose/3786822852/), [kmaschke](https://www.flickr.com/photos/14174853@N04/4165248133/), [jamingray](https://www.flickr.com/photos/jamingray/5179486993/), [deglispiriti](https://www.flickr.com/photos/73853155@N00/284462939/), [Lucas Uyezu](https://www.flickr.com/photos/luyezu/4292626471/), [justinknabb](https://www.flickr.com/photos/justinknabb/5040046824/), [Bogdan Migulski](https://www.flickr.com/photos/migulski/3701027825/), [S outH CheN](https://www.flickr.com/photos/7553102@N04/2799379452/), [Melirius](https://www.flickr.com/photos/melirius/11225456523/), [2bethere](https://www.flickr.com/photos/2bethere/5160423697/), [Steve AM](https://www.flickr.com/photos/scuba04/2401033603/), [L'amande](https://www.flickr.com/photos/l_amande/6333421472/).

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/image-matching-challenge-2022/overview/code-requirements) for details.**","## Evaluation metric

Participants are asked to estimate the relative pose of one image with respect to another. Submissions are evaluated on the **mean Average Accuracy (mAA)** of the estimated poses. Given a fundamental matrix and the hidden ground truth, we compute the error in terms of rotation ( \\( \epsilon\_R \\), in degrees) and translation (\\( \epsilon\_T \\), in meters). Given one threshold over each, we _classify_ a pose as accurate if it meets both thresholds. We do this over ten pairs of thresholds, one pair at a time (e.g. at 1\\(^o\\) and 20 cm at the finest level, and 10\\(^o\\) and 5 m at the coarsest level):

```python
thresholds_r = np.linspace(1, 10, 10)  # In degrees.
thresholds_t = np.geomspace(0.2, 5, 10)  # In meters.
```

We then calculate the percentage of image pairs that meet every pair of thresholds, and average the results over all thresholds, which rewards more accurate poses. As the dataset contains multiple scenes, which may have a different number of pairs, we compute this metric separately for each scene and average it afterwards. A python implementation of this metric is available on [this notebook](https://www.kaggle.com/code/eduardtrulls/imc2022-tutorial-load-and-evaluate-training-data).

## Submission File

For each ID in the test set, you must predict the fundamental matrix between the two views. The file should contain a header and have the following format:

```
sample_id,fundamental_matrix
a;b;c-d,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
a;b;e-f,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
a;b;g-h,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
etc
```

Note that `fundamental_matrix` is a \\( 3 \times 3 \\) matrix, flattened into a vector in row-major order."
Tabular Playground Series - Apr 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-apr-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33104/logos/header.png?t=2021-12-30-01-25-24,Tabular,816,860,7235,"Welcome to the April edition of the 2022 Tabular Playground Series! This month's challenge is a *time series classification* problem.

You've been provided with thousands of sixty-second sequences of biological sensor data recorded from several hundred participants who could have been in either of two possible activity states. Can you determine what state a participant was in from the sensor data?

## About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `sequence` in the test set, you must predict a probability for the `state` variable. The file should contain a header and have the following format:

```
sequence,state
25968,0
25969,0
25970,0
...
```
"
U.S. Patent Phrase to Phrase Matching ,Help Identify Similar Phrases in U.S. Patents,https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching,https://storage.googleapis.com/kaggle-competitions/kaggle/33657/logos/header.png?t=2022-02-23-06-26-59,"NLP,Text",1889,2325,42902,"Can you extract meaning from a large, text-based dataset derived from inventions? Here's your chance to do so.

The U.S. Patent and Trademark Office (USPTO) offers one of the largest repositories of scientific, technical, and commercial information in the world through its [Open Data Portal](https://developer.uspto.gov/about-open-data). Patents are a form of [intellectual property](https://www.uspto.gov/patents/basics/general-information-patents) granted in exchange for the public disclosure of new and useful inventions. Because patents undergo an intensive [vetting process](https://www.uspto.gov/sites/default/files/documents/InventionCon2020_Understanding_the_Patent_Examination_Process.pdf) prior to grant, and because the history of U.S. innovation spans over two centuries and 11 million patents, the U.S. patent archives stand as a rare combination of data volume, quality, and diversity.

> “The USPTO serves an American innovation machine that never sleeps by granting patents, registering trademarks, and promoting intellectual property around the globe. The USPTO shares over 200 years' worth of human ingenuity with the world, from lightbulbs to quantum computers. Combined with creativity from the data science community, USPTO datasets carry unbounded potential to empower AI and ML models that will benefit the progress of science and society at large.”
>
> — USPTO Chief Information Officer Jamie Holcombe

In this competition, you will train your models on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. For example, if one invention claims ""television set"" and a prior publication describes ""TV set"", a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. This extends beyond paraphrase identification; if one invention claims a ""strong material"" and another uses ""steel"", that may also be a match. What counts as a ""strong material"" varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn't want your parachute made of steel). We have included the Cooperative Patent Classification as the technical domain context as an additional feature to help you disambiguate these situations.

Can you build a model to match phrases in order to extract contextual information, thereby helping the patent community connect the dots between millions of patent documents?


<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/us-patent-phrase-to-phrase-matching/overview/code-requirements) for details.**","Submissions are evaluated on the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the predicted and actual similarity `score`s.

## Submission File
For each `id` (representing a pair of phrases) in the test set, you must predict the similarity `score`. The file should contain a header and have the following format:

    id,score
    4112d61851461f60,0
    09e418c93a776564,0.25
    36baf228038e314b,1
    etc.
"
iWildCam 2022 - FGVC9,Count the number of animals in a sequence of images,https://www.kaggle.com/competitions/iwildcam2022-fgvc9,https://storage.googleapis.com/kaggle-competitions/kaggle/33843/logos/header.png?t=2022-02-12-20-38-25,"Image,Animals",24,29,300,"## Description

Camera Traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance (how many there are) and population density of species in camera trap data, ecologists need to know not just which species were seen, but also how many of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over- or under-counting. For example, if you get 3 images taken at one frame per second, and in the first you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species, as it requires reasoning and tracking of individuals across sparse temporal samples.

<img alt=""hard examples"" src=""https://github.com/visipedia/iwildcam_comp/raw/master/assets/train_examples_smaller.gif"">

This year our iWildCam competition will focus entirely on counting animals. We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to count individual animals across sequences in the test cameras. To explore multimodal solutions, we allow competitors to train on the following data:

1. Our camera trap training set — data provided by the [Wildlife Conservation Society (WCS)](https://www.wcs.org/).
1. iNaturalist 2017-2021 data.
1. Multispectral imagery from [Landsat-8](https://www.usgs.gov/land-resources/nli/landsat/landsat-8) for each of the camera trap locations.

Check the [Data section](https://www.kaggle.com/c/iwildcam2022-fgvc9/data) for a more comprehensive description of all these resources and for accessing the train set, test set and metadata. These are mirrored on the competition's [GitHub page](https://github.com/visipedia/iwildcam_comp) as well, where we also provide the multispectral data, a taxonomy file mapping our classes into the iNaturalist taxonomy, a subset of the iNaturalist data mapped into our class set, a camera trap detection model (the [MegaDetector](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md)) along with the corresponding detections, and a class-agnostic instance segmentation model ([DeepMAC](https://google.github.io/deepmac/)) along with the segmentation masks for the MegaDetector's bounding boxes.

## Acknowledgements

This competition is part of the [FGVC9](https://sites.google.com/view/fgvc9) workshop at [CVPR 2022](https://cvpr2022.thecvf.com/) and is sponsored by [Wildlife Insights](https://www.wildlifeinsights.org/). Data is primarily provided by the [Wildlife Conservation Society (WCS)](https://www.wcs.org/) and [iNaturalist](https://www.inaturalist.org/), and is hosted on Azure by [Microsoft AI for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth). Count annotations were generously provided by [Centaur Labs](https://www.centaurlabs.com/).

*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.*","## Evaluation

Submissions will be evaluated using [Mean Absolute Error (MAE)](https://en.wikipedia.org/wiki/Mean_absolute_error),

![MAE](https://raw.githubusercontent.com/visipedia/iwildcam_comp/master/assets/MAE.png)

where each `x_i` represents the predicted count of animals in sequence `i`, `y_i` represents the ground truth count for that sequence, and `n` is the number of sequences in the test set.

We selected this simple metric for this year because it's easy to interpret and because count errors on large groups of animals (which will inevitably happen!) are not as hardly penalized as in the case of [Root Mean Square Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_error).

## Submission Format

Solutions should be submited as a CSV file with the following format:

```csv
Id,Predicted
58857ccf-23d2-11e8-a6a3-ec086b02610b,0
591e4006-23d2-11e8-a6a3-ec086b02610b,1
...
```

The `Id` column corresponds to the test sequence id, while `Predicted` holds an integer value that indicates the number of individual animals predicted for that test sequence."
Hotel-ID to Combat Human Trafficking 2022 - FGVC9,Recognizing hotels to aid Human trafficking investigations,https://www.kaggle.com/competitions/hotel-id-to-combat-human-trafficking-2022-fgvc9,https://storage.googleapis.com/kaggle-competitions/kaggle/35150/logos/header.png?t=2022-03-10-00-02-44,"Public Safety,Image",82,135,1712,"<h2>Hotel Recognition to Combat Human Trafficking</h2>
<p>Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.</p>

![Example investigative images.](https://cs.slu.edu/~astylianou/images/example_victim_images.png)

<p>Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about <a href=""https://techcrunch.com/2016/06/25/traffickcam/"">TraffickCam on TechCrunch.</a></p>

<p>Example images from one hotel in the TraffickCam dataset are shown below:</p>

![Example TraffickCam images.](https://cs.slu.edu/~astylianou/images/example_traffickcam_images.png)

<p>In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs.</p>

<p>Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system.</p>

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/hotel-id-2021-fgvc8/overview/code-requirements) for details.**","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):

$$MAP@5 = \frac{1}{U} \sum\_{u=1}^{U}  \sum\_{k=1}^{min(n,5)} P(k) \times rel(k)$$

where \\( U \\)  is the number of images, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number of predictions per image, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant correct label, zero otherwise.

Once a correct label has been scored for *an observation*, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.

    A B C D E
    A A A A A
    A B A C A


## Submission File

For each image in the test set, you must predict a space-delimited list of hotel IDs that could match that image. The list should be sorted such that the first ID is considered the most relevant one and the last the least relevant one. The file should contain a header and have the following format:

```
image,hotel_id 
99e91ad5f2870678.jpg,36363 53586 18807 64314 60181
b5cc62ab665591a9.jpg,36363 53586 18807 64314 60181
d5664a972d5a644b.jpg,36363 53586 18807 64314 60181
```"
Sorghum -100 Cultivar Identification - FGVC 9,Identify crop varietals,https://www.kaggle.com/competitions/sorghum-id-fgvc-9,https://storage.googleapis.com/kaggle-competitions/kaggle/34375/logos/header.png?t=2022-03-12-00-53-27,"Image,Agriculture",252,294,3904,"# Overview

The Sorghum-100 dataset is a curated subset of the RGB imagery captured during the TERRA-REF experiments, labeled by cultivar. This data could be used to develop and assess a variety of plant phenotyping models which seek to answer questions relating to the presence or absence of desirable traits (e.g., ""does this plant exhibit signs of water stress?''). In this contest, we focus on the question: ""What cultivar is shown in this image?''

![Field Scanner](https://terraref.org/sites/terraref.org/files/TERRA-REF-Scanner.jpg)

Predicting the cultivar in an image is an especially good challenge problem for familiarizing the machine learning community with the TERRA-REF data. At first blush, the task of predicting the cultivar from an image of a plant may not seem to be the most biologically compelling question to answer -- in the context of plant breeding, the cultivar, or parental lines are typically known. A high accuracy machine learning predictor of the species captured by the sensor data, however, can be used to determine where errors in the planting process may have occurred. For example, seed may be mislabeled prior to planting, or planters may get jammed, depositing seeds non-uniformly in a field. Both types of errors are surprisingly common and can cause major problems when processing data from large-scale field experiments with hundreds of cultivars and complex field planting layouts.

### Data Description

The Sorghum-100 dataset consists of 48,106 images and 100 different sorghum cultivars grown in June of 2017 (the images come from the middle of the growing season when the plants were quite large but not yet lodging -- or falling over).

Each image is taken using an RGB spectral camera taken from a vertical view of the sorghum plants in the [TERRA-REF](https://terraref.org/) field in Arizona. ","The evaluation metric for this contest is simple mean classification accuracy -- how often do contestants predict that an image came from the correct cultivar?

<h2>Submission Format</h2>
Contestants should upload a csv that includes the headers ""filename"" and ""cultivar"", and should include one row per image in the test set:

| filename | cultivar |
| --- | --- |
| 000VZKNBNWEROVY7AFUX8IH8.png | PI_153877 |
| 00367TN3ZQFJQXCDDJULMF5P.png| PI_153877 |
| 00AFRVXB8FPIQRF29DSO5W9G.png | PI_153877 |
| ... | ... |
|  ZZZYE3LDORWJLF100IS3QKP3.png |  PI_153877 |

This file should have 25,472 rows (one row per file in the test set, plus the header row)."
GeoLifeCLEF 2022 - LifeCLEF 2022 x FGVC9,Location-based species presence prediction,https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9,https://storage.googleapis.com/kaggle-competitions/kaggle/33317/logos/header.png?t=2022-03-08-13-06-08,"Image,Geospatial Analysis,Environment",52,59,242,"<h1>Description Task</h1>

<img src=""https://raw.githubusercontent.com/maximiliense/GLC/master/images/dataset_illustration.jpg"" alt=""Illustration of GeoLifeCLEF dataset""> 

<p>
  The aim of this competition is to predict the localization of plant and animal species.<br>
  To do so, 1.6M geo-localized observations from France and the US of 17K species are provided (9K plant species and 8K animal species).<br>
  These observations are paired with aerial images and environmental features around them (as illustrated above).<br>
  The goal is, for each GPS position in the test set (for which we provide the associated aerial images and environmental features), to return a set of candidate species that should contain the true observed species.
</p>


<h1>Motivation</h1>

<p>
  Automatic prediction of the list of species most likely to be observed at a given location is useful for many scenarios related to biodiversity management and conservation.<br>
  First, this would allow to improve species identification tools - automatic, semi-automatic, or based on traditional field guides - by reducing the list of candidate species observable at a given site.<br>
  More generally, it could facilitate biodiversity inventories through the development of location-based recommendation services (e.g. on mobile phones), encourage the involvement of citizen scientist observers, and accelerate the annotation and validation of species observations to produce large, high-quality data sets.<br>
  Finally, this could be used for educational purposes through biodiversity discovery applications with features such as contextualized educational pathways.
</p>


<h1>Context</h1>

<p>
  This competition is held jointly as part of:
  </p><ul>
	<li>the <a href=""https://www.imageclef.org/LifeCLEF2022"">LifeCLEF 2022</a> lab of the <a href=""http://clef2022.clef-initiative.eu"">CLEF 2022</a> conference, and of</li>
	<li>the <a href=""https://sites.google.com/view/fgvc9"">FGVC9</a> workshop organized in conjunction with <a href=""http://cvpr2022.thecvf.com"">CVPR 2022</a> conference.</li>
  </ul>
<p></p>

<p>
  <b>Being part of scientific research, the participants are encouraged to participate to both event.<br>
  In particular, only participants who submitted a working note paper to LifeCLEF (see below) will be part of the officially published ranking used for scientific communication.</b>
</p>


<h2>FGVC9 at CVPR 2022</h2>

<p>
  This competition is part of the Fine-Grained Visual Categorization <a href=""https://sites.google.com/view/fgvc9"">FGVC9</a> workshop at the Computer Vision and Pattern Recognition Conference <a href=""http://cvpr2022.thecvf.com"">CVPR 2022</a>.<br>
  A panel will review the top submissions for the competition based on the description of the methods provided.<br>
  From this, a subset may be invited to present their results at the workshop.<br>
  Attending the workshop is not required to participate in the competition; however, only teams that are attending the workshop will be considered to present their work.
</p>

<p>
  <b>CVPR 2022 will take place in New Orleans, USA, 19-24 June 2022.</b><br>
  PLEASE NOTE: CVPR frequently sells out early, we cannot guarantee CVPR registration after the competition's end.<br>
  If you are interested in attending, please plan ahead.
</p>

<p>
  You can see a list of all of the FGVC9 competitions <a href=""https://sites.google.com/view/fgvc9/competitions"">here</a>.
</p>


<h2>LifeCLEF 2022</h2>

<p>
  LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum (<a href=""http://www.clef-initiative.eu"">CLEF</a>).<br>
  CLEF consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems.<br>
  <b>CLEF 2022 will be hosted by the Università di Bologna, Italy, 5-8 September 2022.</b><br>
  More details can be found on the <a href=""http://clef2022.clef-initiative.eu"">CLEF 2022 website</a>.
</p>

<p>
  To participate to the LifeCLEF lab, participants must register using <a href=""http://clef2022-labs-registration.dei.unipd.it/registrationForm.php"">this form</a> (and checking ""Task 3 - GeoLifeCLEF"" of ""LifeCLEF"" section).<br>
  <b>This registration is free of charge and will close on 22 April 2022, however free of charge late registration will still be possible at the end of the competition.</b><br>
  This will allow those participants to submit, at the end of the competition, a working note paper to LifeCLEF which will be peer-reviewed and published in <a href=""http://ceur-ws.org"">CEUR-WS proceedings</a>.<br>
  This paper should provide sufficient information to reproduce the final submitted runs.<br>
</p>

<p>
  Submitting a working note with the full description of the methods used in each run is mandatory.<br>
  Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results.<br>
  Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP.<br>
  According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality.<br>
  As an illustration, LifeCLEF 2021 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2936"">CLEF 2021 CEUR-WS proceedings</a>. 
</p>


<h1>Credits</h1>
This project has received funding from the French National Research Agency under the Investments for the Future Program, referred to as ANR-16-CONV-0004, and from the European Union’s Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project).","<h1>Evaluation Metric</h1>

<p>
  The evaluation metric for this competition is the top-30 error rate. <br>
  Each observation \( i \) is associated with a single ground-truth label \( y_i \) corresponding to the observed species.<br>
  For each observation, the submissions will provide 30 candidate labels \( \hat{y}_{i,1}, \hat{y}_{i,2}, \dots, {\hat{y}}_{i,30} \).<br>
  The top-30 error rate is then computed using
</p>
<p>
  \[
  \text{Top-30 error rate} =
 \frac{1}{N} \sum_{i=1}^N e_i
  \quad \text{where} \quad
  e_i = 
\begin{cases}
  1 &amp; \text{if } \, \forall k \in \{1,\dots,30\}, \, \hat{y}_{i,k} \neq y_i \\
  0 &amp; \text{otherwise}
  \end{cases}
  \]
</p>


<h1>Train/test Split</h1>

<img src=""https://raw.githubusercontent.com/maximiliense/GLC/master/images/train_test_split.png"" alt=""Illustration of train/test split""> 

<p>
  In order to limit the spatial bias during evaluation, the observations were split into training and test sets using a spatial block holdout procedure.<br>
  This procedure is illustrated in the previous figure: the test data - in red - is drawn from different spatial blocks than the training data - in blue.
</p>

<p>
  In more detail, all the observations are assigned into a grid of 5km×5km quadrats.<br>
  2.5% of these quadrats are randomly sampled for the test set while the remaining ones are used for the training set.
</p>

<p>
  A validation set built using the same splitting procedure is provided (column <code>subset</code> of <code>observations_*_train.csv</code>).<br>
  The code for this splitting procedure is also available on the <a href=""https://github.com/maximiliense/GLC"">GitHub repository</a>.
</p>


<h1>Leaderboard Baselines</h1>

<p>
  We provide 4 baselines in the leaderboard:
  </p><ul>
	<li>top-30 most present species: baseline always predicting the most present species;</li>
	<li>random forest on environmental vectors: a classical method used in ecology using only the environmental vectors,
		<ul><li>scikit-learn random forest of 100 trees with max depth 16 and minimum samples split of 30;</li></ul></li>
	<li>CNN on RGB patches: CNN trained on the RGB patches without any environmental vectors,
		<ul><li>standard ResNet-50 pretrained on ImageNet and finetuned using a learning rate of 0.01, a batch size of 32 and an early stopping strategy on the validation top-30 error rate;</li></ul></li>
	<li>CNN on Red, Green and Near-IR patches: same as above but the blue channel was replaced by near-infrared highlighting the importance of this wavelength for the task.</li>
  </ul>
<p></p>

<p>
  We also provide a <a href=""https://www.kaggle.com/tlorieul/geolifeclef2022-baselines-and-submission"">notebook</a> explaining how to compute some simpler baselines.
</p>


<h1>Submission Format</h1>

<p>
  The submission format is a CSV file containing two columns for each observation:
  </p><ul>
	<li><code>Id</code> column containing integers corresponding to the observations ids</li>
	<li><code>Predicted</code> column containing space-delimited lists of the 30 predicted labels made for each observation</li>
  </ul>
<p></p>

<p>
  The file should contain a header and have the following format:
</p>
<pre>Id, Predicted
1, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
2, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
...
</pre>

<p>
  See also the <a href=""https://www.kaggle.com/tlorieul/geolifeclef2022-baselines-and-submission"">following notebook</a> showing how submission files can be generated.
</p>"
Tabular Playground Series - Mar 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-mar-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33103/logos/header.png?t=2021-12-30-01-24-57,"Tabular,Time Series Analysis,Cities and Urban Areas",956,1003,9971,"For the March edition of the 2022 Tabular Playground Series you're challenged to forecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at *spatio-temporal forecasting* within a highly dynamic traffic network.

Which model will prevail? The venerable linear regression? The deservedly-popular ensemble of decision trees? Or maybe a cutting-edge graph neural-network? We can't wait to see!

## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.","Submissions are evaluated on the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) between predicted and actual congestion values for each time period in the test set.

## Submission File
For each `row_id` in the test set, you should predict a `congestion` measurement. The file should contain a header and have the following format:

```
row_id,congestion
140140,0.0
140141,0.0
140142,0.0
...
```

The `congestion` target has integer values from 0 to 100, but your predictions may be any floating-point number."
Spaceship Titanic,Predict which passengers are transported to an alternate dimension,https://www.kaggle.com/competitions/spaceship-titanic,https://storage.googleapis.com/kaggle-competitions/kaggle/34377/logos/header.png?t=2022-02-11-21-53-06,"Beginner,Tabular,Binary Classification",2689,2953,23312,"<div class=""note"">
<strong>Recommended Competition</strong><br>
We highly recommend <a href=""https://kaggle.com/c/titanic/overview"">Titanic - Machine Learning from Disaster</a> to get familiar with the basics of machine learning and Kaggle competitions.
</div>

Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.

The *Spaceship Titanic* was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.

While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary *Spaceship Titanic* collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Spaceship%20Titanic/joel-filipe-QwoNAhbmLLo-unsplash.jpg"" style=""float:center; height:320px; width:404px"">

To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.

Help save them and change history!

### Acknowledgments

Photos by <a href=""https://unsplash.com/@joelfilip?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Joel Filipe</a>, <a href=""https://unsplash.com/@uncle_rickie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Richard Gatley</a> and <a href=""https://unsplash.com/@actionvance?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">ActionVance</a> on Unsplash.","## Evaluation

Submissions are evaluated based on their [classification accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy), the percentage of predicted labels that are correct.

## Submission Format

The submission format for the competition is a csv file with the following format:

```
PassengerId,Transported
0013_01,False
0018_01,False
0019_01,False
0021_01,False
etc.
```"
March Machine Learning Mania 2022 - Men’s,Predict the 2022 College Men's Basketball Tournament,https://www.kaggle.com/competitions/mens-march-mania-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/26080/logos/header.png?t=2021-02-24-01-37-37,"Basketball,Sports",930,1025,1681,"Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our *eighth* annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.

You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.

<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt="""">

In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.

And don't forget to take a look at our [companion competition](https://www.kaggle.com/c/34542) that looks to predict the outcome of the US women's college basketball tournament!

###Acknowledgments
Banner image by Ben Hershey on Unsplash","Submissions are scored on the log loss:

$$
\textrm{LogLoss} = - \frac{1}{n} \sum\_{i=1}^n \left[ y\_i \log(\hat{y}\_i) + (1 - y\_i) \log(1 - \hat{y}\_i) \right],
$$
where

 - \\( n \\) is the number of games played
 - \\( \hat{y}\_i \\) is the predicted probability of team 1 beating team 2
 - \\( y\_i  \\) is 1 if team 1 wins, 0 if team 2 wins
 - \\( log \\) is the natural logarithm

The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.

## Submission File

The file you submit will depend on whether the competition is in stage 1--historical model building--or stage 2--the 2022 tournament. Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams, you will predict \\( (68*67)/2  = 2,278\\) matchups.

Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2016\_1107\_1110"" indicates team 1107 potentially played team 1110 in the year 2016. You must predict the probability that the team with the lower id beats the team with the higher id.

The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:

    ID,Pred
    2016_1107_1110,0.5
    2016_1107_1112,0.5
    2016_1107_1113,0.5
"
March Machine Learning Mania 2022 - Women's,Predict the 2022 College Women's Basketball Tournament,https://www.kaggle.com/competitions/womens-march-mania-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/26080/logos/header.png?t=2021-02-24-01-37-37,"Sports,Basketball",651,711,1203,"Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our *eighth* annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US women's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.

You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.

<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt="""">

In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.

And don't forget to take a look at our [companion competition](https://www.kaggle.com/c/34538) that looks to predict the outcome of the US men's college basketball tournament!

###Acknowledgments
Banner image by Ben Hershey on Unsplash","Submissions are scored on the log loss:

$$
\textrm{LogLoss} = - \frac{1}{n} \sum\_{i=1}^n \left[ y\_i \log(\hat{y}\_i) + (1 - y\_i) \log(1 - \hat{y}\_i) \right],
$$
where

 - \\( n \\) is the number of games played
 - \\( \hat{y}\_i \\) is the predicted probability of team 1 beating team 2
 - \\( y\_i  \\) is 1 if team 1 wins, 0 if team 2 wins
 - \\( log \\) is the natural logarithm

The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.

## Submission File

The file you submit will depend on whether the competition is in stage 1--historical model building--or stage 2--the 2022 tournament. Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict \\( (64*63)/2 = 2,016\\) matchups.

Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2016\_3106\_3107"" indicates team 3106 played team 3107 in the year 2016. You must predict the probability that the team with the lower id beats the team with the higher id.

The resulting submission format looks like the following, where `Pred` represents the predicted probability that the first team will win:

    ID,Pred
    2016_3106_3107,0.5
    2016_3106_3110,0.5
    2016_3106_3113,0.5
    ..."
BirdCLEF 2022,Identify bird calls in soundscapes,https://www.kaggle.com/competitions/birdclef-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33246/logos/header.png?t=2022-02-08-17-06-27,"Audio,Environment",807,1019,23352,"As the “extinction capital of the world,” Hawai'i has lost 68% of its bird species, the consequences of which can harm entire food chains. Researchers use population monitoring to understand how native birds react to changes in the environment and conservation efforts. But many of the remaining birds across the islands are isolated in difficult-to-access, high-elevation habitats. With physical monitoring difficult, scientists have turned to sound recordings. Known as bioacoustic monitoring, this approach could provide a passive, low labor, and cost-effective strategy for studying endangered bird populations.

<img title=”Oriole” src=""https://storage.googleapis.com/kaggle-media/competitions/Birdsong/Screen%20Shot%202022-02-08%20at%202.04.09%20PM.png"" style=""float: right; width: 250px"">

Current methods for processing large bioacoustic datasets involve manual annotation of each recording. This requires specialized training and prohibitively large amounts of time. Thankfully, recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species, such as those in Hawai'i. 


The Cornell Lab of Ornithology's K. Lisa Yang Center for Conservation Bioacoustics (KLY-CCB) develops and applies innovative conservation technologies across multiple ecological scales to inspire and inform the conservation of wildlife and habitats. KLY-CCB does this by collecting and interpreting sounds in nature and they've joined forces with Google Bioacoustics Group, LifeCLEF, Listening Observatory for Hawaiian Ecosystems (LOHE) Bioacoustics Lab at the University of Hawai'i at Hilo, and Xeno-Canto for this competition.

In this competition, you’ll use your machine learning skills to identify bird species by sound. Specifically, you'll develop a model that can process continuous audio data and then acoustically recognize the species. The best entries will be able to train reliable classifiers with limited training data.

If successful, you'll help advance the science of bioacoustics and support ongoing research to protect endangered Hawaiian birds. Thanks to your innovations, it will be easier for researchers and conservation practitioners to accurately survey population trends. They'll be able to regularly and more effectively evaluate threats and adjust their conservation actions.

&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/birdclef-2022/overview/code-requirements) for details.**","Submissions are evaluated on a metric that is most similar to the [macro F1 score](https://en.wikipedia.org/wiki/F-score). Given the amount of audio data used in this competition it wasn't feasible to label every single species found in every soundscape. Instead only a subset of species are actually scored for any given audio file. After dropping all of the un-scored rows we technically run a weighted classification accuracy with the weights set such that all of the species are assigned the same total weight and the true negatives and true positives for each species have the same weight. The extra complexity exists purely to allow us to have a great deal of control over which birds are scored for a given soundscape. For offline cross validation purposes, the macro F1 is the closest analogue to the actual metric.

## Submission File
For each row_id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:

    row_id,target
    soundscape_1000170626_akiapo_5,False
    soundscape_1000170626_akiapo_10,False
    soundscape_1000170626_akiapo_15,False
    etc.

## Working Note Award Criteria (optional)

Criteria for the BirdCLEF best working note award:

**Originality**. The value of a paper is a function of the degree to which it presents new or novel technical material. Does the paper present results previously unknown? Does it push forward the frontiers of knowledge? Does it present new methods for solving old problems or new viewpoints on old problems? Or, on the other hand, is it a re-hash of information already known?

**Quality**. A paper's value is a function of the innate character or degree of excellence of the work described. Was the work performed, or the study made with a high degree of thoroughness? Was high engineering skill demonstrated? Is an experiment described which has a high degree of elegance? Or, on the other hand, is the work described pretty much of a run-of-the-mill nature?

**Contribution**. The value of a paper is a function of the degree to which it represents an overall contribution to the advancement of the art. This is different from originality. A paper may be highly original but may be concerned with a very minor, or even insignificant, matter or problem. On the other hand, a paper may make a great contribution by collecting and analyzing known data and facts and pointing out their significance. Or, a fine exposition of a known but obscure or complex phenomenon or theory or system or operating technique may be a very real contribution to the art. Obviously, a paper may well score highly on both originality and contribution. Perhaps a significant question is, will the engineer who reads the paper be able to practice his profession more effectively because of having read it?

**Presentation**. The value of the paper is a function of the ease with which the reader can determine what the author is trying to present. Regardless of the other criteria, a paper is not good unless the material is presented clearly and effectively. Is the paper well written? Is the meaning of the author clear? Are the tables, charts, and figures clear? Is their meaning readily apparent? Is the information presented in the paper complete? At the same time, is the paper concise?

*Evaluation of the submitted BirdCLEF working notes:*

Each working note will be reviewed by two reviewers and scores averaged. Maximum score: 15.

a) Evaluation of work and contribution
- 5 points: Excellent work and a major contribution
- 4 points: Good solid work of some importance
- 3 points: Solid work but a marginal contribution
- 2 points: Marginal work and minor contribution
- 1 point: Work doesn't meet scientific standards

b) Originality and novelty
- 5 points Trailblazing
- 4 points: A pioneering piece of work
- 3 points: One step ahead of the pack
- 2 points: Yet another paper about...
- 1 point: It's been said many times before

c) Readability and organization
- 5 points: Excellent
- 4 points: Well written
- 3 points: Readable
- 2 points: Needs considerable work
- 1 point: Work doesn't meet scientific standards
"
Herbarium 2022 - FGVC9,Identify plant species of the Americas from herbarium specimens,https://www.kaggle.com/competitions/herbarium-2022-fgvc9,https://storage.googleapis.com/kaggle-competitions/kaggle/33679/logos/header.png?t=2022-02-14-16-38-02,"Plants,Image",134,174,1534,"[![My-Post.jpg](https://i.postimg.cc/15qZZfvt/My-Post.jpg)](https://postimg.cc/Xp4Pf7SS)

*The Herbarium 2022: Flora of North America* is a part of a project of the [New York Botanical Garden](https://www.nybg.org/) funded by the [National Science Foundation](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2054684&HistoricalAwards=false) to build tools to identify novel plant species around the world. The dataset strives to represent all known vascular plant taxa in North America, using images gathered from 60 different botanical institutions around the world. <br>

In botany, a **‘flora’ is a complete account of the plants found in a geographic region**. The dichotomous keys and detailed descriptions of diagnostic morphological features contained within a flora are used by botanists to determine which names to apply to plant specimens. **This year's competition dataset aims to encapsulate the flora of North America so that we can test the capability of artificial intelligence to replicate this traditional tool** —a crucial first step to harnessing AI’s potential botanical applications.    

*The Herbarium 2022: Flora of North America* dataset comprises **1.05 M images** of **15,501 vascular plants**, which constitute more than **90% of the taxa** documented in North America. Our dataset is constrained to include only **vascular land plants** (lycophytes, ferns, gymnosperms, and flowering plants). <br>

Our dataset has a long-tail distribution. **The number of images per taxon** is as few as seven and as many as 100 images. Although more images are available, we capped the maximum number in an attempt to ensure sufficient but manageable training data size for competition participants. <br>




# About

This is an FGVC competition hosted as part of the [FGVC9](https://sites.google.com/view/fgvc9) workshop at [CVPR 2022](http://cvpr2022.thecvf.com/) and sponsored by [NYBG](https://www.nybg.org/).

Details of this competition are mirrored on the [github](https://github.com/visipedia/herbarium_comp) page. Please post in the forum or open an issue if you have any questions or problems with the dataset.

# Acknowledgements

The images are provided by the [New York Botanical Garden](https://www.nybg.org/) and 59 other institutions around the world. 
![herb22banner](https://i.postimg.cc/g0DJMF52/output.png)","Submissions are evaluated using the [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).

The F1 score is given by

$$
F_1 = 2\frac{precision \cdot recall}{precision+recall}
$$

where:

$$
precision = \frac{TP}{TP+FP},
$$

$$
recall = \frac{TP}{TP+FN}.
$$

In ""macro"" F1 a separate F1 score is calculated for each `species` value and then averaged.
#Submission Format
For each image `Id`, you should predict the corresponding image label (`category_id`) in the `Predicted` column. The submission file should have the following format:
<pre>Id,Predicted<br>0,1<br>1,27<br>2,42<br>...</pre>"
H&M Personalized Fashion Recommendations,Provide product recommendations based on previous purchases,https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations,https://storage.googleapis.com/kaggle-competitions/kaggle/31254/logos/header.png?t=2021-10-26-22-52-52,"Recommender Systems,Retail and Shopping",2952,3759,38854,"[H&M Group](https://www.hmgroup.com/) is a family of brands and businesses with 53 online markets and approximately 4,850 stores. Our online store offers shoppers an extensive selection of products to browse through. But with too many choices, customers might not quickly find what interests them or what they are looking for, and ultimately, they might not make a purchase. To enhance the shopping experience, product recommendations are key. More importantly, helping customers make the right choices also has a positive implications for sustainability, as it reduces returns, and thereby minimizes emissions from transportation. 

In this competition, H&M Group invites you to develop product recommendations based on data from previous transactions, as well as from customer and product meta data. The available meta data spans from simple data, such as garment type and customer age, to text data from product descriptions, to image data from garment images. 

There are no preconceptions on what information that may be useful – that is for you to find out. If you want to investigate a categorical data type algorithm, or dive into NLP and image processing deep learning, that is up to you.


","Submissions are evaluated according to the Mean Average Precision @ 12 (MAP@12):

$$MAP@12 = \frac{1}{U} \sum\_{u=1}^{U} \frac{1}{min(m,12)}  \sum\_{k=1}^{min(n,12)} P(k) \times rel(k)$$

where \\( U \\)  is the number of customers, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number predictions per customer, \\( m \\) is the number of ground truth values per customer, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) label, zero otherwise.

**Notes:**
 - You will be making purchase predictions for all `customer_id` values provided, regardless of whether these customers made purchases in the training data.
 - Customer that did not make any purchase during test period are excluded from the scoring.
 - There is never a penalty for using the full 12 predictions for a customer that ordered fewer than 12 items; thus, it's advantageous to make 12 predictions for each customer.

## Submission File

For each `customer_id` observed in the training data, you may predict up to 12 labels for the `article_id`, which is the predicted items a customer will buy in the next 7-day period after the training time period. The file should contain a header and have the following format:

    customer_id,prediction
    00000dba,0706016001 0706016002 0372860001 ...
    0000423b,0706016001 0706016002 0372860001 ...
    ..."
NBME - Score Clinical Patient Notes,Identify Key Phrases in Patient Notes from Medical Licensing Exams,https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes,https://storage.googleapis.com/kaggle-competitions/kaggle/33607/logos/header.png?t=2022-01-24-18-05-41,"Text,Medicine,Education,NLP",1471,1880,28049,"When you visit a doctor, how they interpret your symptoms can determine whether your diagnosis is accurate. By the time they’re licensed, physicians have had a lot of practice writing patient notes that document the history of the patient’s complaint, physical exam findings, possible diagnoses, and follow-up care. Learning and assessing the skill of writing patient notes requires feedback from other doctors, a time-intensive process that could be improved with the addition of machine learning.

Until recently, the Step 2 Clinical Skills examination was one component of the United States Medical Licensing Examination® (USMLE®). The exam required test-takers to interact with Standardized Patients (people trained to portray specific clinical cases) and write a patient note. Trained physician raters later scored patient notes with rubrics that outlined each case’s important concepts (referred to as features). The more such features found in a patient note, the higher the score (among other factors that contribute to the final score for the exam).

However, having physicians score patient note exams requires significant time, along with human and financial resources. Approaches using natural language processing have been created to address this problem, but patient notes can still be challenging to score computationally because features may be expressed in many ways. For example, the feature ""loss of interest in activities"" can be expressed as ""no longer plays tennis."" Other challenges include the need to map concepts by combining multiple text segments, or cases of ambiguous negation such as “no cold intolerance, hair loss, palpitations, or tremor” corresponding to the key essential “lack of other thyroid symptoms.”

In this competition, you’ll identify specific clinical concepts in patient notes. Specifically, you'll develop an automated method to map clinical concepts from an exam rubric (e.g., “diminished appetite”) to various ways in which these concepts are expressed in clinical patient notes written by medical students (e.g., “eating less,” “clothes fit looser”). Great solutions will be both accurate and reliable.

If successful, you'll help tackle the biggest practical barriers in patient note scoring, making the approach more transparent, interpretable, and easing the development and administration of such assessments. As a result, medical practitioners will be able to explore the full potential of patient notes to reveal information relevant to clinical skills assessment.

This competition is sponsored by the [National Board of Medical Examiners](https://www.nbme.org/)® (NBME®). Through research and innovation, NBME supports medical school and residency program educators in addressing issues around the evolution of teaching, learning, technology, and the need for meaningful feedback. NBME offers high-quality assessments and educational services for students, professionals, educators, regulators, and institutions dedicated to the evolving needs of medical education and health care. To serve these communities, NBME collaborates with a diverse and comprehensive array of practicing health professionals, medical educators, state medical board members, test developers, academic researchers, scoring experts and public representatives.

*NBME gratefully acknowledges the valuable input of Dr Le An Ha from the University of Wolverhampton’s Research Group in Computational Linguistics.*
<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/nbme-score-clinical-patient-notes/overview/code-requirements) for details.**","This competition is evaluated by a [micro-averaged](https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel) [F1](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics) score.

For each instance, we predict a set of character spans. A **character span** is a pair of indexes representing a range of characters within a text. A span `i j` represents the characters with indices `i` through `j`, inclusive of `i` and exclusive of `j`. In Python notation, a span `i j` is equivalent to a slice `i:j`.

For each instance there is a collection of ground-truth spans and a collection of predicted spans. The spans we delimit with a semicolon, like: `0 3; 5 9`.

We score each character index as:
   - TP if it is within both a ground-truth and a prediction,
   - FN if it is within a ground-truth but not a prediction, and,
   - FP if it is within a prediction but not a ground truth.

Finally, we compute an overall F1 score from the TPs, FNs, and FPs aggregated across all instances.

## Example ##

Suppose we have an instance:

```
| ground-truth | prediction    |
|--------------|---------------|
| 0 3; 3 5     | 2 5; 7 9; 2 3 |
```

These spans give the sets of indices:

```
| ground-truth | prediction |
|--------------|------------|
| 0 1 2 3 4    | 2 3 4 7 8  |
```

We therefore compute:

- `TP = size of {2, 3, 4} = 3`
- `FN = size of {0, 1} = 2`
- `FP = size of {7, 8} = 2`

Repeat for all instances, collect the TPs, FNs, and FPs, and compute the final F1 score.

## Sample Submission ##

For each `id` in the test set, you must predict zero or more spans delimited by a semicolon. The file should contain a header and have the following format:

```
id,location
00016_000,0 100
00016_001,
00016_002,200 250;300 500
...
```

For `00016_000` you should give predictions for feature `000` in patient note `00016`."
Happywhale - Whale and Dolphin Identification,Identify whales and dolphins by unique characteristics,https://www.kaggle.com/competitions/happy-whale-and-dolphin,https://storage.googleapis.com/kaggle-competitions/kaggle/22962/logos/header.png?t=2021-03-17-22-44-09,"Image,Animals",1588,2070,39284,"We use fingerprints and facial recognition to identify people, but can we use similar approaches with animals? In fact, researchers manually track marine life by the shape and markings on their tails, dorsal fins, heads and other body parts. Identification by natural markings via photographs—known as photo-ID—is a powerful tool for marine mammal science. It allows individual animals to be tracked over time and enables assessments of population status and trends. With your help to automate whale and dolphin photo-ID, researchers can reduce image identification times by over 99%. More efficient identification could enable a scale of study previously unaffordable or impossible.

<img title=”ID 52” src=""https://storage.googleapis.com/kaggle-media/competitions/Happywhale/AU%20Kaggle%20Competition%20Description%20Image-03.jpg"" style=""float: right; width: 300px"">

Currently, most research institutions rely on time-intensive—and sometimes inaccurate—manual matching by the human eye. Thousands of hours go into manual matching, which involves staring at photos to compare one individual to another, finding matches, and identifying new individuals. While researchers enjoy looking at a whale photo or two, manual matching limits the scope and reach.

Algorithms developed in this competition will be implemented in Happywhale, a research collaboration and citizen science web platform. Its mission is to increase global understanding and caring for marine environments through high quality conservation science and education. Happywhale aims to make it easy and rewarding for the public to participate in science by building innovative tools to engage anyone interested in marine mammals. The platform also serves the research community with powerful collaborative tools.

In this competition, you’ll develop a model to match individual whales and dolphins by unique—but often subtle—characteristics of their natural markings. You'll pay particular attention to dorsal fins and lateral body views in image sets from a multi-species dataset built by 28 research institutions. The best submissions will suggest photo-ID solutions that are fast and accurate.

If successful, you'll have a hand in building advanced technology to better understand and manage the impact on the world’s changing oceans. Previous automation attempts resulted in a global database of over 50,000 whales and an agreement with cruise ships to operate at a maximum speed of 11 mph in the most whale-rich region. Your ideas to automate the identification of marine life will help overcome increasing human impacts on oceans, providing a critical tool for conservation science. If there's a whale, there's a way!
","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):

$$MAP@5 = \frac{1}{U} \sum\_{u=1}^{U} \sum\_{k=1}^{min(n,5)} P(k) \times rel(k)$$

where \\( U \\)  is the number of images, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number predictions per image, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) label, zero otherwise.

Once a correct label has been scored for *an observation*, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.

    [A, B, C, D, E]
    [A, A, A, A, A]
    [A, B, A, C, A]


## Submission File

For each `image` in the test set, you may predict up to 5 `individual_id` labels. There are individuals in the test set that are not seen in the training data; these should be predicted as `new_individual`. The file should contain a header and have the following format:

    image,predictions 
    000188a72f2562.jpg,37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d new_individual 
    000ba09273d6f3.jpg,37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d new_individual 
    ...
"
Tabular Playground Series - Feb 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-feb-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33102/logos/header.png?t=2021-12-30-01-24-25,Tabular,1255,1312,11766,"For the February 2022 Tabular Playground Series competition, your task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the *histogram* of base count. In other words, the DNA segment \\(\text{ATATGGCCTT}\\) becomes  \\(\text{A}\_2\text{T}\_4\text{G}\_2\text{C}\_2\\). Can you use this lossy information to accurately predict bacteria species?

## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.

### Getting Started
 
For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.
 
*Good luck and have fun!*

## Acknowledgements

The idea for this competition came from the following [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full):

```
@ARTICLE{10.3389/fmicb.2020.00257,
AUTHOR={Wood, Ryan L. and Jensen, Tanner and Wadsworth, Cindi and Clement, Mark and Nagpal, Prashant and Pitt, William G.},   
TITLE={Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers},      
JOURNAL={Frontiers in Microbiology},      
VOLUME={11},      
YEAR={2020},      
URL={https://www.frontiersin.org/article/10.3389/fmicb.2020.00257},       
DOI={10.3389/fmicb.2020.00257},      
ISSN={1664-302X}}
```
","##Evaluation

Submissions will be evaluated based on their [categorization accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy).

## Submission Format

The submission format for the competition is a csv file with the following format:

    row_id,target
    200000,Streptococcus_pneumoniae
    200001,Enterococcus_hirae
    etc."
Ubiquant Market Prediction,Make predictions against future market data,https://www.kaggle.com/competitions/ubiquant-market-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/32053/logos/header.png?t=2021-11-05-23-54-43,"Tabular,Finance",2893,2949,4159,"Regardless of your investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/ubiquant/6.jpg"" >

Ubiquant Investment (Beijing) Co., Ltd is a leading domestic quantitative hedge fund based in China. Established in 2012, they rely on international talents in math and computer science along with cutting-edge technology to drive quantitative financial market investment. Overall, Ubiquant is committed to creating long-term stable returns for investors.

In this competition, you’ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices. Top entries will solve this real-world data science problem with as much accuracy as possible.

If successful, you could improve the ability of quantitative researchers to forecast returns. This will enable investors at any scale to make better decisions. You may even discover you have a knack for financial datasets, opening up a world of new opportunities in many industries.

See more information about Ubiquant below:

[<img src=""https://storage.googleapis.com/kaggle-media/competitions/ubiquant/Screen%20Shot%202021-11-11%20at%2011.16.11%20PM.png"" alt=""logo"" width=""720"" >](https://www.youtube.com/watch?v=PCzi76d-W6o)

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/ubiquant-market-prediction/overview/code-requirements) for details.**","Submissions are evaluated on the mean of the [Pearson correlation coefficient] (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) for each time ID. 

You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:

```
import ubiquant
env = ubiquant.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission
for (test_df, sample_prediction_df) in iter_test:
    sample_prediction_df['target'] = 0  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
```

You will get an error if you submission includes nulls or infinities and submissions that only include one prediction value will receive a score of -1."
Tabular Playground Series - Jan 2022,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-jan-2022,https://storage.googleapis.com/kaggle-competitions/kaggle/33101/logos/header.png?t=2021-12-30-01-23-41,"Tabular,Time Series Analysis",1591,1646,16151,"We've heard your feedback from the 2021 Tabular Playground Series, and now Kaggle needs your help going forward in 2022!

There are two (fictitious) independent store chains selling Kaggle merchandise that want to become **the** official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build forecasting models to help us decide. 

Help us figure out whether KaggleMart or KaggleRama should become the official Kaggle outlet!

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F59561%2Fcc6d9a21f0c3ed71b00113b33efb2b66%2Fkaggle_sweater.png?generation=1640900016906235&alt=media)


## About the Tabular Playground Series

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.

The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.","Submissions are evaluated on [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.


## Submission File
For each `row_id` in the test set, you must predict the corresponding `num_sold`. The file should contain a header and have the following format:

    row_id,num_sold
    26298,100
    26299,100
    26300,100
    etc.
"
Feedback Prize - Evaluating Student Writing,Analyze argumentative writing elements from students grade 6-12 ,https://www.kaggle.com/competitions/feedback-prize-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/31779/logos/header.png?t=2021-11-12-22-52-17,"Text,Primary and Secondary Schools,NLP",2058,2598,33831,"Writing is a critical skill for success. However, less than a third of high school seniors are proficient writers, according to the National Assessment of Educational Progress. Unfortunately, low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback.

There are currently numerous  automated writing feedback tools, but they all have limitations. Many often fail to identify writing structures, such as thesis statements and support for claims, in essays or do not do so thoroughly. Additionally, the majority of the available tools are proprietary, with algorithms and feature claims that cannot be independently backed up. More importantly, many of these writing tools are inaccessible to educators because of their cost. This problem is compounded for  under-serviced schools which serve a disproportionate number of students of color and from low-income backgrounds. In short, the field of automated writing feedback is ripe for innovation that could help democratize education.

Georgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor’s degrees to African-Americans than any other non-profit college or university in the country. GSU and [The Learning Agency Lab](https://the-learning-agency-lab.com/), an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.

In this competition, you’ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.

![Description Image](https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/Kaggle%20Description%20Image.png)

If successful, you'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors  and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.

###Acknowledgements    

Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures and Chan Zuckerberg Initiative for their support in making this work possible. 


<a href=""https://www.gatesfoundation.org/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/BMGF_logo_black_300dpi%20(1).jpg"" style=""width: 200px""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://schmidtfutures.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/Schmidt%20Futures%20Logo.png"" style=""width: 250px"">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://chanzuckerberg.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/The%20Learning%20Agency/1200px-Chan_Zuckerberg_Initiative.svg.png"" style=""width: 100px"">&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</a>


<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/feedback-prize-2021/overview/code-requirements) for details.**","Submissions are evaluated on the overlap between ground truth and predicted word indices.

1. For each sample, all ground truths and predictions for a given class are compared.
2. If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a `true positive`. If multiple matches exist, the match with the highest pair of overlaps is taken.
3. Any unmatched ground truths are `false negatives` and any unmatched predictions are `false positives`.

Example:

Ground Truth

    id,class,predictionstring
    1,Claim,1 2 3 4 5
    1,Claim,6 7 8
    1,Claim,21 22 23 24 25

Prediction

    id,class,predictionstring
    1,Claim,1 2
    1,Claim,6 7 8

The first prediction would not have >= 0.5 overlap with either ground truth and would be a `false positive`. The second prediction would overlap perfectly with the second ground truth and be a `true positive`. The third ground truth would be unmatched, and would be a `false negative`.

The final score is arrived at by calculating TP/FP/FN for each class, then taking the [macro F1 score](https://en.wikipedia.org/wiki/F-score) across all classes.

The word indices are calculated by using Python's `.split()` function and taking the indices in the resulting list. The two overlaps are calculated by taking the `set()` of each list of indices in a ground truth / prediction pair and calculating the intersection between the two sets divided by the length of each set.

## Submission File
For each sample in the test set, you must extract any strings from the document that you feel aligns with a `class`, then submit the sample `id`, `class` and word indices `predictionstring` of that string. If you have multiple predictions for a class or sample, simply submit multiple rows. The file should contain a header and have the following format:

    id,class,predictionstring
    2,Claim,300 301 302 303
    5,Evidence,56 57 58 59 60 61 62
    6,Lead,0 1 2 3 4 5 6
    6,Lead,9 10 11 12
    etc.
"
Tabular Playground Series - Dec 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-dec-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/28012/logos/header.png?t=2021-06-30-01-16-12,Tabular,1188,1234,12524,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). This dataset is based off of the original [Forest Cover Type Prediction
](https://www.kaggle.com/c/forest-cover-type-prediction/overview) competition.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated on multi-class classification accuracy.
## Submission File
For each `Id` in the test set, you must predict the `Cover_Type` class. The file should contain a header and have the following format:

    Id,Cover_Type
    4000000,2
    4000001,1
    4000001,3
    etc.
"
TensorFlow - Help Protect the Great Barrier Reef ,Detect crown-of-thorns starfish in underwater image data,https://www.kaggle.com/competitions/tensorflow-great-barrier-reef,https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png?t=2021-10-29-00-30-04,"Image,Earth and Nature",2025,2608,60934,"## Goal of the Competition

The goal of this competition is to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.

Your work will help researchers identify species that are threatening Australia's Great Barrier Reef and take well-informed action to protect the reef for future generations.

## Context

Australia's stunningly beautiful Great Barrier Reef is the world’s largest coral reef and home to 1,500 species of fish, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life.
 
Unfortunately, the reef is under threat, in part because of the overpopulation of one particular starfish – the coral-eating crown-of-thorns starfish (or COTS for short). Scientists, tourism operators and reef managers established a large-scale intervention program to control COTS outbreaks to ecologically sustainable levels.

[<img src=""https://storage.googleapis.com/kaggle-media/competitions/Google-Tensorflow/video_thumb_kaggle.png"" alt=""video"" width=""720"" >](https://www.youtube.com/watch?v=UT2noVDFoaA)
 
To know where the COTS are, a traditional reef survey method, called ""Manta Tow"", is performed by a snorkel diver. While towed by a boat, they visually assess the reef, stopping to record variables observed every 200m. While generally effective, this method faces clear limitations, including operational scalability, data resolution, reliability, and traceability.
 
The Great Barrier Reef Foundation established an [innovation program](https://www.barrierreef.org/what-we-do/reef-trust-partnership/crown-of-thorns-starfish-control) to develop new survey and intervention methods to provide a step change in COTS Control. Underwater cameras will collect thousands of reef images and AI technology could drastically improve the efficiency and scale at which reef managers detect and control COTS outbreaks.

To scale up video-based surveying systems, Australia’s national science agency, CSIRO has teamed up with Google to develop innovative machine learning technology that can analyse large image datasets accurately, efficiently, and in near real-time.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview/code-requirements) for details.**

## Citation
Please cite [this short paper](https://arxiv.org/abs/2111.14311) if you are using this dataset for research purposes.
```
@misc{liu2021csiro,
      title={The CSIRO Crown-of-Thorn Starfish Detection Dataset}, 
      author={Jiajun Liu and Brano Kusy and Ross Marchant and Brendan Do and Torsten Merz and Joey Crosswell and Andy Steven and Nic Heaney and Karl von Richter and Lachlan Tychsen-Smith and David Ahmedt-Aristizabal and Mohammad Ali Armin and Geoffrey Carlin and Russ Babcock and Peyman Moghadam and Daniel Smith and Tim Davis and Kemal El Moujahid and Martin Wicke and Megha Malpani},
      year={2021},
      eprint={2111.14311},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
","This competition is evaluated on the [F2 Score](https://en.wikipedia.org/wiki/F-score) at different intersection over union (IoU) thresholds. The F2 metric weights recall more heavily than precision, as in this case it makes sense to tolerate some false positives in order to ensure very few starfish are missed.

The metric sweeps over IoU thresholds in the range of 0.3 to 0.8 with a step size of 0.05, calculating an F2 score at each threshold. For example, at a threshold of 0.5, a predicted object is considered a ""hit"" if its IoU with a ground truth object is at least 0.5.

A true positive is the first (in `confidence` order, see details below) submission box in a sample with an IoU greater than the threshold against an _unmatched_ solution box.

Once all submission boxes have been evaluated, any unmatched submission boxes are false positives; any unmatched solution boxes are false negatives.

The final F2 Score is calculated as the mean of the F2 scores at each IoU threshold. Within each IoU threshold the competition metric uses micro averaging; every true positive, false positive, and false negative has equal weight compared to each other true positive, false positive, and false negative.

In your submission, you are also asked to provide a `confidence` level for each bounding box. Bounding boxes are evaluated in order of their confidence levels. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.


You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:

## Submission Format

```
import greatbarrierreef
env = greatbarrierreef.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission
for (pixel_array, sample_prediction_df) in iter_test:
    sample_prediction_df['annotations'] = '0.5 0 0 100 100'  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
```

The submission format requires a space delimited set of bounding boxes. For example:

`0.5 0 0 100 100`

indicates that the image has a bounding box with a `confidence` of 0.5, at `x` == 0 and `y` == 0, with a `width` and `height` of 100.

`0.3 0 0 50 50 0.5 10 10 30 30` 

would predict two bounding boxes in the image. Each prediction row needs to include all bounding boxes for the image.
"
Santa 2021 - The Merry Movie Montage,Optimize television programming for the winter season,https://www.kaggle.com/competitions/santa-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/31644/logos/header.png?t=2021-10-28-21-22-46,"Optimization,Movies and TV Shows,Holidays and Cultural Events",867,1003,6849,"<img title=”family” src=""https://storage.googleapis.com/kaggle-media/competitions/Santa/diljaz-tm-G3D6oxB_dMM-unsplash%20(1).jpeg"" style=""float: right; height: 325px"">

*You’ve gotta watch out <br>
You probably will cry <br>
You’ll smile throughout <br>
I'm telling you why <br>
SantaTV’s coming to town*
 
*So give them a list, <br>
They’re watching it thrice <br>
They’re gonna find out what’s sugar, what’s spice.  <br>
SantaTV’s is coming to town*
 
*The elves have lots of movies <br>
Let’s hope they stay awake <br>
Cause Christmas season’s coming soon<br>
And there’s toys for them to make!*
<br><br>

People seem to be getting in the Christmas spirit earlier and earlier each year. Decorations appear for sale in stores in the fall, Christmas songs are on the radio in October…

The Elves at the North Pole are starting to recognize this, and need to work as fast as possible to launch their latest holiday offering: SantaTV+! A 24/7 streaming television channel where it’s “Always Christmas, All the Time.” To debut their new station, they’ve decided to kick things off with a made-for-television Christmas movie marathon! They’re excited for the premiere of such movies as 🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀!

But elves know that just as important as the movie themselves is the order they’ll be aired. So the elves have decided the best way to figure out which order is best is to watch all the movies in every possible combination to see which feels the most Christmas-y.

Your job is to help the elves by giving them the shortest viewing schedules that shows them every combination of movies so they can get SantaTV+ live as soon as possible! The elves have formed three movie-watching teams to lighten the load, so every combination must be seen by at least one of their groups. But they’re also pretty sure they want to kick off the movie marathon with the :santa: and :mrs_claus: movies back-to-back, so be sure that each group has all the combinations that start with those. And finally, the elves have agreed to two sugar breaks, so you’re allowed to give each group up to two 🌟 wildcards, which will play all the movies at once while they’re snacking, which will help speed things along.

They can’t launch SantaTV+ until all the groups have finished watching - so help give them the most efficient schedule to see every Christmas movie combination, and help them get back to making toys!


### Acknowledgments

Photos by <a href=""https://unsplash.com/@erwanhesry?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Erwan Hesry</a> and <a href=""https://unsplash.com/@djzxcvi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Diljaz TM</a> on Unsplash.","## Objective

Your objective is to find a set of three strings containing every permutation of the seven symbols      🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀 as substrings, subject to the following conditions:

- Every permutation must be in at least one string.
- Each permutation beginning with 🎅🤶 must be in all three strings.
- Each string may have up to two wildcards 🌟, which will match any symbol in a permutation. No string of length seven containing more than one wildcard will count as a permutation.

Your score is the length of the longest of the three strings. This is a minimization problem, so lower scores are better.

## Example

Let's consider a simplified problem where we only use three symbols 🎅, 🤶, 🦌and no wildcard, and where our solution consists of only two strings.

There are six permutations of these three symbols: 🎅🤶🦌, 🎅🦌🤶, 🤶🎅🦌,  🤶🦌🎅,  🦌🎅🤶,  and 🦌🤶🎅. The permutation 🎅🤶🦌 must be a substring of both solution strings while the other five permutations must be in at least one of the strings.

A valid solution for this problem is:
1. 🤶🎅🦌🤶🎅🤶🦌
2. 🎅🤶🦌🎅🤶

which would have a score of 7, the length of string 1.

If we were allowed the use of one wildcard, we could have the solution:
1. 🎅🤶🌟🦌🤶🎅
2. 🎅🤶🦌🎅🤶

with a score of 6. The wildcard can represent different symbols in different permutations.

## Submission File

Your solution should consist of three `schedule`s containing permutations of the seven symbols with optional wildcards as described above. The file should contain a header and have the following format:

```
schedule
🎅🌟🦌🎁🎀🎅🧝🎄🦌🤶🎅🧝🎄🎁🎀...
🤶🎁🎀🎅🧝🎄🦌🤶🎅🧝🎅🦌🤶🌟🎅...
🦌🎅🤶🦌🤶🎅🦌🌟🤶🎅🧝🎄🎁🎀🎄...

```"
Jigsaw Rate Severity of Toxic Comments   ,Rank relative ratings of toxicity between comments,https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating,https://storage.googleapis.com/kaggle-competitions/kaggle/27935/logos/header.png?t=2021-10-15-13-42-55,"NLP,Text",2301,2880,49047,"In Jigsaw's fourth Kaggle competition, we return to the Wikipedia Talk page comments featured in our [first Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). When we ask human judges to look at individual comments, without any context, to decide which ones are toxic and which ones are innocuous, it is rarely an easy task. In addition, each individual may have their own bar for toxicity. We've tried to work around this by aggregating the decisions with a majority vote. But many researchers have rightly pointed out that this discards meaningful information.
## 😄 🙂 😐 😕 😞

A much easier task is to ask individuals which of two comments they find more toxic. But if both comments are non-toxic, people will often select randomly. When one comment is obviously the correct choice, the inter-annotator agreement results are much higher.

In this competition, we will be asking you to score a set of about fourteen thousand comments. Pairs of comments were presented to expert raters, who marked one of two comments more harmful — each according to their own notion of toxicity. In this contest, when you provide scores for comments, they will be compared with several hundred thousand rankings. Your average agreement with the raters will determine your individual score. In this way, we hope to focus on ranking the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.

Can you build a model that produces scores that rank each pair of comments the same way as our professional raters?

*Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.*   


### Related Work

The paper ""[Ruddit: Norms of Offensiveness for English Reddit Comments](https://aclanthology.org/2021.acl-long.210/)"" by Hada et al. introduced a similar dataset that involved tuples of four sentences that were marked with best-worst scoring, and this data may be directly useful for building models.

We also note ""[Constructing Interval Variables via Faceted Rasch Measurement
and Multitask Deep Learning: a Hate Speech Application](https://arxiv.org/pdf/2009.10277.pdf)"" by Kennedy et al. which compares a variety of different rating schemes and argues that binary classification as typically done in NLP tasks discards valuable information. Combining data from multiple sources, even with different annotation guidelines, may be essential for success in this competition.

### Resources

The English language resources from our [first Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge), and our [second Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification), which are both available in the TensorFlow datasets [Wikipedia Toxicity Subtypes](https://www.tensorflow.org/datasets/catalog/wikipedia_toxicity_subtypes) and [Civil Comments](https://www.tensorflow.org/datasets/catalog/civil_comments) can be used to build models.

One example of a starting point is the open source [UnitaryAI model](https://github.com/unitaryai/detoxify).

### Google Jigsaw

Google's [Jigsaw](https://jigsaw.google.com/) team explores threats to open societies and builds technology that inspires scalable solutions. One Jigsaw product is [PerspectiveAPI](https://www.perspectiveapi.com/) which is used by publishers and platforms worldwide as part of their overall moderation strategy.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/overview/code-requirements) for details.**","Submissions are evaluated on **Average Agreement with Annotators**. For the ground truth, annotators were shown two comments and asked to identify which of the two was more toxic. Pairs of comments can be, and often are, rated by more than one annotator, and may have been ordered differently by different annotators.

For each of the approximately 200,000 pair ratings in the ground truth test data, we use your predicted toxicity `score` to rank the comment pair. The pair receives a `1` if this ranking matches the annotator ranking, or `0` if it does not match.

The final score is the average across all the pair evaluations.

Please note the following:
 - `score` is *not* constrained to any numeric range (e.g., you can predict `[0, 1]` or `[-999, 999]`).
 - There is no tie breaking; tied comment scores will always be evaluated as `0`. You could consider using something like `scipy.stats.rankdata` to force unique value.

## Submission File
For each `comment_id` found in the `comments_to_score.csv` file, you must predict the toxic severity `score` associated with the comment `text`. The submission file should contain a header and have the following format:

    comment_id,score
    114890,0.43
    732895,0.98
    1139051,0.27
    etc.
"
G-Research Crypto Forecasting ,Use your ML expertise to predict real crypto market data,https://www.kaggle.com/competitions/g-research-crypto-forecasting,https://storage.googleapis.com/kaggle-competitions/kaggle/30894/logos/header.png?t=2021-09-14-17-32-48,"Tabular,Finance,Time Series Analysis",1946,2398,3141,"Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance?

In this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model.   Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected.

The simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting.

[G-Research] (https://www.gresearch.co.uk/) is Europe’s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, [Cambridge Spark] (https://www.cambridgespark.com/) is partnering with G-Research for this competition. Watch our introduction to the competition below:

[<img src=""https://storage.googleapis.com/kaggle-media/competitions/G-Research/Screenshot%202021-11-02%20at%2011.15.16.png"" alt=""logo"" width=""720"" >](https://www.youtube.com/watch?v=GW84uCnYr30)


<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/g-research-crypto-forecasting/overview/code-requirements) for details.**","Submissions are evaluated on a weighted version of the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). You can find additional details in [the 'Prediction Details and Evaluation' section of this tutorial notebook](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition).

You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:

```
import gresearch_crypto
env = gresearch_crypto.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission
for (test_df, sample_prediction_df) in iter_test:
    sample_prediction_df['Target'] = 0  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
```

[A more detailed introduction to the API is available here](https://www.kaggle.com/sohier/detailed-api-introduction).

You will get an error if you submission includes nulls or infinities."
Tabular Playground Series - Nov 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-nov-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/28011/logos/header.png?t=2021-06-30-01-14-31,Tabular,1362,1422,14710,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    600000,0.5
    600001,0.9
    600002,0.1
    etc.
"
Sartorius - Cell Instance Segmentation,Detect single neuronal cells in microscopy images,https://www.kaggle.com/competitions/sartorius-cell-instance-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/30201/logos/header.png?t=2021-09-03-15-27-46,"Image,Biology",1505,1984,32813,"Neurological disorders, including neurodegenerative diseases such as Alzheimer's and brain tumors, are a leading cause of death and disability across the globe. However, it is hard to quantify how well these deadly disorders respond to treatment. One accepted method is to review neuronal cells via light microscopy, which is both accessible and non-invasive. Unfortunately, segmenting individual neuronal cells in microscopic images can be challenging and time-intensive. Accurate instance segmentation of these cells—with the help of computer vision—could lead to new and effective drug discoveries to treat the millions of people with these disorders.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/Sartorius/Sartorius_Competition%20Description%20Image%20350x379.png"" style=""float: right; width: 250px;"">

Current solutions have limited accuracy for neuronal cells in particular. In internal studies to develop cell instance segmentation models, the neuroblastoma cell line SH-SY5Y consistently exhibits the lowest precision scores out of eight different cancer cell types tested. This could be because neuronal cells have a very unique, irregular and concave morphology associated with them, making them challenging to segment with commonly used mask heads.

Sartorius is a partner of the life science research and the biopharmaceutical industry. They empower scientists and engineers to simplify and accelerate progress in life science and bioprocessing, enabling the development of new and better therapies and more affordable medicine. They're a magnet and dynamic platform for pioneers and leading experts in the field. They bring creative minds together for a common goal: technological breakthroughs that lead to better health for more people.

In this competition, you’ll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy.

If successful, you'll help further research in neurobiology thanks to the collection of robust quantitative data. Researchers may be able to use this to more easily measure the effects of disease and treatment conditions on neuronal cells. As a result, new drugs could be discovered to treat the millions of people with these leading causes of death and disability.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/sartorius-cell-instance-segmentation/overview/code-requirements) for details.**","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: `(0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)`. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.

## Submission File

In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).

The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed 
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.

The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. **It also checks that no two predicted masks for the same image are overlapping.**

The file should contain a header and have the following format. Each row in your submission represents a single predicted nucleus segmentation for the given `ImageId`.

    id,predicted  
    0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5,1 1  
    0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,1 1  
    0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,2 3 8 9  
    etc...

Submission files may take several minutes to process due to the size."
Store Sales - Time Series Forecasting,Use machine learning to predict grocery sales,https://www.kaggle.com/competitions/store-sales-time-series-forecasting,https://storage.googleapis.com/kaggle-competitions/kaggle/29781/logos/header.png?t=2021-09-22-19-59-35,"Tabular,Time Series Analysis,Beginner",797,976,5181,"<h2>Goal of the Competition</h2>

In this “getting started” competition, you’ll use time-series forecasting to forecast store sales on data from Corporación Favorita, a large Ecuadorian-based grocery retailer.

Specifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.

<div class=""note"">
<strong>Get Started</strong><br>
We highly recommend the <a href=""https://www.kaggle.com/learn/time-series"">Time Series course</a>, which walks you through how to make your first submission.  The lessons in this course are inspired by winning solutions from past Kaggle time series forecasting competitions.
</div>

<h2>Context</h2>
Forecasts aren’t just for meteorologists. Governments forecast economic growth. Scientists attempt to predict the future population. And businesses forecast product demand—a common task of professional data scientists. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time.

Current subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing. 

<h2>Potential Impact</h2>
If successful, you'll have flexed some new skills in a real world example. For grocery stores, more accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.
","The evaluation metric for this competition is **Root Mean Squared Logarithmic Error**.

The **RMSLE** is calculated as:
\[\sqrt{ \frac{1}{n} \sum\_{i=1}^n \left(\log (1 + \hat{y}\_i) - \log (1 + y\_i)\right)^2}\]
where:
- \\(n\\) is the total number of instances,
- \\(\hat{y}\_i\\) is the predicted value of the target for instance \(i\),
- \\(y\_i\\) is the actual value of the target for instance \(i\), and,
- \\(\log\\) is the natural logarithm.


## Submission File
For each **id** in the test set, you must predict a value for the **sales** variable. The file should contain a header and have the following format:

```
id,sales
3000888,0.0
3000889,0.0
3000890,0.0
3000891,0.0
3000892,0.0
etc.
```
"
Tabular Playground Series - Oct 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-oct-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/28010/logos/header.png?t=2021-06-30-01-13-23,"Tabular,Binary Classification",1089,1124,10119,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    100000,0.5
    100001,0.9
    100002,0.1
    etc.
"
PetFinder.my - Pawpularity Contest,Predict the popularity of shelter pet photos,https://www.kaggle.com/competitions/petfinder-pawpularity-score,https://storage.googleapis.com/kaggle-competitions/kaggle/25383/logos/header.png?t=2021-08-31-18-49-29,Image,3537,4334,47037,"<p><a href=""https://www.petfinder.my/"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-media/competitions/Petfinder/PetFinder%20-%20Logo.png
"" alt=""PetFinder"" style=""width: 50%""></a></p>

<p><img src=""https://www.petfinder.my/images/cuteness_meter.jpg""></p>

<p>&nbsp;</p>

<p>A picture is worth a thousand words. But did you know a picture can save a thousand lives? Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo’s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.</p>

<p><a href=""https://PetFinder.my/"" target=""_blank"">PetFinder.my</a> is Malaysia’s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.</p>

<p>Currently, PetFinder.my uses a basic <a href=""https://PetFinder.my/cutenessmeter"" target=""_blank"">Cuteness Meter</a> to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.</p>

<p>In this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.</p>

<p>If successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their ""furever"" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.</p>

<p>Top participants may be invited to collaborate on implementing their solutions and creatively improve global animal welfare with their AI skills.</p>

<p>&nbsp;</p>

<p><img src=""https://www.petfinder.my/images/cuteness_meter-showcase.jpg""></p>

<br>
&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/petfinder-pawpularity-score/overview/code-requirements) for details.**","## Root Mean Squared Error 𝑅𝑀𝑆𝐸
Submissions are scored on the root mean squared error. RMSE is defined as:

\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}_i)^2} \]

where \( \hat{y}\_i \) is the predicted value and \(y\_i\) is the original value for each instance \(i\).

## Submission File
For each `Id` in the test set, you must predict a probability for the target variable, `Pawpularity`. The file should contain a header and have the following format:

```
Id, Pawpularity
0008dbfb52aa1dc6ee51ee02adf13537, 99.24
0014a7b528f1682f0cf3b73a991c17a0, 61.71
0019c1388dfcd30ac8b112fb4250c251, 6.23
00307b779c82716b240a24f028b0031b, 9.43
00320c6dd5b4223c62a9670110d47911, 70.89
etc.
```"
Google Brain - Ventilator Pressure Prediction,"Simulate a ventilator connected to a sedated patient's lung
",https://www.kaggle.com/competitions/ventilator-pressure-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/29594/logos/header.png?t=2021-07-29-12-44-09,"Tabular,Medicine,Biology",2605,3118,46281,"What do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier. 

Current simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs. 

Partnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.  

In this competition, you’ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.

If successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.


Photo by <a href=""https://unsplash.com/@ninoliverani?utm_source=unsplash"">Nino Liverani</a> on <a href=""https://unsplash.com/s/photos/lung?utm_source=unsplash"">Unsplash</a>
  
  ","The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:

$$|X-Y|$$

where \\(X\\) is the vector of predicted pressure and \\(Y\\) is the vector of actual pressures across all breaths in the test set.

## Submission File
For each `id` in the test set, you must predict a value for the `pressure` variable. The file should contain a header and have the following format:

    id,pressure
    1,20
    2,23
    3,24
    etc.
"
Wikipedia - Image/Caption Matching,Retrieve captions based on images,https://www.kaggle.com/competitions/wikipedia-image-caption,https://storage.googleapis.com/kaggle-competitions/kaggle/29705/logos/header.png?t=2021-08-09-04-38-12,"Image,Text",105,121,548,"A picture is worth a thousand words, yet sometimes a few will do. We all rely on online images for knowledge sharing, learning, and understanding. Even the largest websites are missing visual content and metadata to pair with their images. Captions and “[alt text](https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Accessibility/Alternative_text_for_images)” increase accessibility and enable better search. The majority of images on Wikipedia articles, for example, don't have any written context connected to the image. Open models could help anyone improve accessibility and learning for all.

Current solutions rely on simple methods based on translations or page interlinks, which have limited coverage. Even the most advanced computer vision image captioning isn't suitable for images with complex semantics.


In this competition, you’ll build a model that automatically retrieves the text closest to an image. Specifically, you'll train your model to associate given images with article titles or complex captions, in multiple languages. The best models will account for the semantic granularity of Wikipedia images.

If successful, you'll be contributing to the accessibility of the largest online encyclopedia. The millions of Wikipedia readers and editors will be able to more easily understand, search, and describe media at scale. As a result, you’ll contribute to an open model to improve learning for all.

--
This competition is organized by the [Research](https://research.wikimedia.org/) team at the [Wikimedia Foundation](https://wikimediafoundation.org/). This competition is based on the [WIT dataset](https://github.com/google-research-datasets/wit) published by Google Research as detailed in this [SIGIR paper](https://dl.acm.org/doi/abs/10.1145/3404835.3463257).

","Submissions will be evaluated using [NDCG@5](http://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) (Normalized Discounted Cumulative Gain).

## Submission File
The submission should be a list of `id`,`caption_title_and_reference_description` pairs ranked from top to bottom according to their relevance (i.e., the top `id` is the most relevant `caption_title_and_reference_description`), with up to 5 predictions per `id`. Each line should be a single `id`,`caption_title_and_reference_description` pair.

The file should contain a header and have the following format:

    id,caption_title_and_reference_description
    0,kaggle the home of data science
    0,lorem ipsum
    ...
    1,the quick brown fox
    etc."
Tabular Playground Series - Sep 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-sep-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/28009/logos/header.png?t=2021-06-30-01-12-34,"Tabular,Binary Classification",1942,2060,18394,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `claim` variable. The file should contain a header and have the following format:

```
id,claim
957919,0.5
957920,0.5
957921,0.5
etc.
```"
Lux AI,Gather the most resources and survive the night!,https://www.kaggle.com/competitions/lux-ai-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/30067/logos/header.png?t=2021-07-20-15-37-18,"Simulations,Video Games",1178,1449,22331,"## Introduction
The night is dark and full of terrors. Two teams must fight off the darkness, collect resources, and advance through the ages. Daytime finds a desperate rush to gather the resources that can carry you through the impending night whilst growing your city. Plan and expand carefully -- any city that fails to produce enough light will be consumed by darkness.
 
**Welcome to the Lux AI Challenge Season 1!**

<img src=""https://github.com/Lux-AI-Challenge/Lux-Design-2021/raw/master/assets/daynightshift.gif"" width=""50%"" />

The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.
 
All code can be found at our [Github](https://github.com/Lux-AI-Challenge/Lux-Design-2021), make sure to give it a star while you are there!
 
Make sure to join our community [discord](https://discord.gg/aWJt3UAcgn) to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.
 
## Sponsors
We would like to thank our 3 sponsors, [QuantCo](https://quantco.com/), [J Ventures](http://thejiangmen.com), and [QAImera](https://qaimera.com) this year for allowing us to provide a prize pool and exciting opportunities to our competitors!  For more information on them, go to the [sponsors tab](https://www.kaggle.com/c/lux-ai-2021/overview/sponsors)
","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties. 

Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.

Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.

When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.

We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.

After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.

## Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.

## Final Evaluation

At the submission deadline on December 6th, additional submissions will be locked. From December 7th to December 20th, we will continue to run games. At the conclusion of this period, the leaderboard is final and is used to determine who gets various ranking based prizes."
chaii - Hindi and Tamil Question Answering,Identify the answer to questions found in Indian language passages,https://www.kaggle.com/competitions/chaii-hindi-and-tamil-question-answering,https://storage.googleapis.com/kaggle-competitions/kaggle/30060/logos/header.png?t=2021-07-29-17-23-51,"Text,Languages",943,1148,18727,"With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users. With more attention from the Kaggle community and your novel machine learning solutions, we can help Indian users make the most of the web.

Predicting answers to questions is a common NLU task, but not for Hindi and Tamil. Current progress on multilingual modeling requires a concentrated effort to generate high-quality datasets and modelling improvements. Additionally, for languages that are typically underrepresented in public datasets, it can be difficult to build trustworthy evaluations. We hope the dataset provided for this competition—and [additional datasets generated by participants](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/overview/sharing-datasets)—will enable future machine learning for Indian languages.

In this competition, your goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. You will be provided with a [baseline model](https://www.kaggle.com/deeplearning10/chaii-1-starter-notebook) and [inference code](https://www.kaggle.com/deeplearning10/chaii-1-inference) to build upon.

If successful, you'll improve upon the baseline performance of NLU models in Indian languages. The results could improve the web experience for many of the nearly 1.4 billion people of India. Additionally, you’ll contribute to multilingual NLP, which could be applied beyond the languages in this competition.

### Acknowledgments

**Google Research India** contributes fundamental advances in computer science and applies their research to big problems impacting India, Google, and communities around the world. The Natural Language Understanding group at Google Research India works specifically with ML to address the unique challenges in the Indian context (such as code mixing in Search, diversity of languages, dialects and accents in Assistant), learning from limited resources and advancing multilingual models. 

**chaii ([Challenge in AI for India](https://events.withgoogle.com/chaii2021))** is a [Google Research India](https://research.google/teams/india-research-lab/) initiative created with the purpose of sparking AI applications to address some of the pressing problems in India and to find unique ways to address them. Starting with a focus on NLU, chaii hopes to make progress towards multilingual modelling, as language diversity is significantly underserved on the web. Google Research India is  working on transformational approaches to healthcare, agriculture and education, and also improving apps and services such as search, assistant and payments, e.g., to deal with challenges arising out of the diversity of languages in India. We also acknowledge the support from the [AI4Bharat](https://indicnlp.ai4bharat.org/home/) Team at the Indian Institute of Technology Madras.","The metric in this competition is the [word-level Jaccard score](https://en.wikipedia.org/wiki/Jaccard_index). A good description of Jaccard similarity for strings is [here](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50).

A Python implementation based on the links above, and matched with the output of the C# implementation on the back end, is provided below.

```
def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
```
The formula for the overall metric, then, is:
$$
score = \frac{1}{n} \sum_{i=1}^n jaccard( gt\_i, dt\_i )
$$
where:
$$
n = \textrm{number of documents}
$$
$$
jaccard = \textrm{the function provided above}
$$
$$
gt\_i = \textrm{the ith ground truth}
$$
$$
dt\_i = \textrm{the ith prediction}
$$

## Submission File
For each ID in the test set, you must predict the string that best answers the provided `question` based on the `context`. Note that the selected text _needs_ to be **quoted** and **complete** to work correctly. Include punctuation, etc. - the above code splits ONLY on whitespace. The file should contain a header and have the following format:

    id,PredictionString
    8c8ee6504,""1""
    3163c22d0,""2 string""
    66aae423b,""4 word 6""
    722085a7b,""1""
    etc."
Google Landmark Retrieval 2021,"Given an image, can you find all of the same landmarks in a dataset?",https://www.kaggle.com/competitions/landmark-retrieval-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/29761/logos/header.png?t=2021-07-21-17-07-00,"Image,Computer Vision",263,392,6782,"Welcome to the fourth Landmark Retrieval competition! This year, we introduce a lot more diversity in the challenge’s test images in order to measure global landmark retrieval performance in a fairer manner. And following last year’s success, we set this up as a code competition.

Image retrieval is a central problem in computer vision, relevant to many applications. The problem is usually posed as follows: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.

In this competition, the developed models are expected to retrieve relevant database images to a given query image (i.e., the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the [Landmark Recognition Challenge 2021](https://www.kaggle.com/c/landmark-recognition-2021/). Both challenges will be discussed at the [Instance-Level Recognition workshop](https://ilr-workshop.github.io/ICCVW2021/) in ICCV 21.

In contrast to previous editions of this challenge ([2018](https://www.kaggle.com/c/landmark-retrieval-challenge), [2019](https://www.kaggle.com/c/landmark-retrieval-2019), and [2020](https://www.kaggle.com/c/landmark-retrieval-2020)), this year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/landmark-retrieval-2021/overview/code-requirements) for details.**
","Submissions are evaluated according to mean Average Precision @ 100, commonly referred to as (mAP@100):

$$mAP@100 = \frac{1}{Q} \sum\_{q=1}^{Q} \frac{1}{min(m\_q, 100)} \sum\_{k=1}^{min(n\_q,100)} P\_q(k) rel_q(k)$$

where:

- \\(Q\\) is the number of query images
- \\(m\_q\\) is the number of index images containing a landmark in common with the query image \\(q\\). Note that \(m_q \gt 0\).
- \\(n_q\\) is the number of predictions made by the system for query \\(q\\)
- \\(P_q(k)\\) is the precision at rank \\(k\\) for the \\(q\\)-th query
- \\(rel_q(k)\\) denotes the relevance of prediciton \\(k\\) for the \\(q\\)-th query: it’s 1 if the \\(k\\)-th prediction is correct, and 0 otherwise

## Submission File

For each query id in the test set, you must predict a space-delimited list of index images that depict the same landmarks as the query. The list should be sorted, such that the first index image is considered the most relevant one, and the last the least relevant one. The file should contain a header and have the following format:

    id,images
    000088da12d664db,0370c4c856f096e8 766677ab964f4311 e3ae4dcee8133159...
    etc.
"
Google Landmark Recognition 2021,"Label famous, and not-so-famous, landmarks in images",https://www.kaggle.com/competitions/landmark-recognition-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/29762/logos/header.png?t=2021-07-21-17-09-45,"Image,Computer Vision",383,525,8717,"Welcome to the fourth Landmark Recognition competition! This year, we introduce a lot more diversity in the challenge’s test images in order to measure global landmark recognition performance in a fairer manner. And following last year’s success, we set this up as a code competition.

Have you ever gone through your vacation photos and asked yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.

Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.

This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This is similar to the [2020 version of the competition](https://www.kaggle.com/c/landmark-recognition-2020). In older editions ([2018](https://www.kaggle.com/c/landmark-recognition-challenge) and  [2019](https://www.kaggle.com/c/landmark-recognition-2019)), submissions had been handled by uploading prediction files to the system. 

This challenge is organized in conjunction with the [Landmark Retrieval Challenge 2021](https://www.kaggle.com/c/landmark-retrieval-2021). Both challenges will be discussed at the [Instance-Level Recognition workshop](https://ilr-workshop.github.io/ICCVW2021/) in ICCV 21.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/landmark-recognition-2021/overview/code-requirements) for details.**

Cover image credits: Muhammad Mahdi Karim. The original is available on Wikimedia [here](https://commons.wikimedia.org/wiki/File:Mount_Kilimanjaro.jpg). License: [GNU Free Documentation License](https://commons.wikimedia.org/wiki/Commons:GNU_Free_Documentation_License,_version_1.2).","Submissions are evaluated using Global Average Precision (GAP) at (k), where (k=1). This metric is also known as micro Average Precision (\mu AP), as per references 1 and 2 below. It works as follows:

For each test image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions, sorted in descending order by confidence scores, and computes the Average Precision based on this list.

If a submission has (N) predictions, formatted as label/confidence pairs, sorted in descending order by their confidence scores, then the Global Average Precision is computed as:

$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$</p>

where:
<ul>
<li> (N) is the total number of predictions returned by the system, across all queries</li>
<li> (M) is the total number of queries with at least one landmark from the training set visible in it. Note that some queries may not depict landmarks. </li>
<li> (P(i)) is the precision at rank (i)</li>
<li> (rel(i)) denotes the relevance of prediciton (i): it’s 1 if the (i)-th prediction is correct, and 0 otherwise</li>
</ul>

References:
<p> 1) F. Perronnin, Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,"" Proc. CVPR'09</p>
<p> 2) T. Weyand, A. Araujo, B. Cao and J. Sim, ""Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,"" Proc. CVPR'20</p>

<h2>Submission File</h2>
<p>For each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some query images may contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. The submission file should contain a header and have the following format &mdash; note that larger scores denote more confident matches:</p>
<pre>id,landmarks
000088da12d664db,8815 0.03
0001623c6d808702,
0001bbb682d45002,5328 0.5
etc.</pre>
"
NFL Health & Safety - Helmet Assignment,Segment and label helmets in video footage,https://www.kaggle.com/competitions/nfl-health-and-safety-helmet-assignment,https://storage.googleapis.com/kaggle-competitions/kaggle/12125/logos/header.png?t=2018-11-30-18-08-32,"Health,Football",825,1028,12600,"The National Football League (NFL) and Amazon Web Services (AWS) are teaming up to develop the best sports injury surveillance and mitigation program. In previous competitions, Kaggle has helped detect helmet impacts. As a next step, the NFL wants to assign specific players to each helmet, which would help accurately identify each player's “exposures” throughout a football play.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/NFL%20player%20safety%20analytics/assingment_example.gif.gif"" style=""display:block; margin: 20px auto;"">

Currently, the NFL manually annotates a subset of plays each year to determine a sample of exposures for each player. To expand this program, the current player assignment requires a field map to determine player locations. The NFL is interested in matching this model's accuracy without the need for the mapping step. The league is calling on Kagglers to invent a better way to identify individual players.

The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit [www.NFL.com/PlayerHealthandSafety](http://www.NFL.com/PlayerHealthandSafety).

In this competition, you’ll identify and assign football players’ helmets from video footage. In particular, you'll create algorithms capable of assigning detected helmet impacts to correct players via tracking information. Successful submissions should aim for 90% accuracy.

If successful, you'll support the NFL in its efforts to efficiently improve player safety. If the league no longer has to manually label each exposure, it would dramatically increase the speed and scale at which they could answer complex research questions related to helmet impact. Automatic player detection would also allow the NFL to back-calculate historic exposure trends, allowing for deeper insights into how to mitigate them in the future.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/overview/code-requirements) for details.**","Submissions are evaluated using a weighted accuracy metric for all active players’ helmets in the provided videos. Helmet boxes associated with a definitive impact will be weighted 1000x more than helmets not involved in an impact. Ground truth helmet boxes will be paired to the submission box that has the the largest Intersection over Union (IoU) from the submission. The IoU between the ground truth box and submission box must meet a minimum IoU threshold of 0.35. The IoU of a proposed bounding box and a ground truth bounding box is calculated as:

$$ IoU (A,B) = \frac{A \cap B}{ A \cup B} $$

The metric will assign each ground truth helmet to a single submitted box based on IoU.

Accuracy will be calculated for all ground truth helmet boxes excluding sideline players (labels `H00` and `V00`). Helmet boxes are counted once for each frame they appear in a video. Helmet boxes at the moment of a definitive impact are given a weight of 1000. All other helmet boxes are given a weight of 1.

$$ Weighted Accuracy = \frac{TotalCorrect\_{nonimp}+(TotalCorrect\_{imp} * 1000)}{TotalHelmets\_{nonimp}+(TotalHelmets\_{imp} * 1000)} $$

Where: 
* `TotalCorrect_nonimp` is the number of correctly assigned non-definitive impact helmet boxes.
* `TotalCorrect_imp` is the number of correctly assigned definitive impact helmet boxes.
* `TotalHelmets_nonimp` is the total number of non-definitive helmets boxes.
* `TotalHelmets_imp` is the total number of definitive helmet impact boxes.

Submissions must also meet the following requirements:
* No more than 22 helmet predictions per video frame. Predictions should not be submitted for players not actively participating in the play who are on the sideline. Sideline players are labeled “H00” and “V00” in the training dataset.
* A players’ helmet label must only be predicted once per video frame, i.e. no duplicated labels per frame.
* All submitted helmet boxes must be unique per video frame.
* Boxes must be within video area:
        - `top` and `left` must be >= 0
        - The sum of `left` and `width` must be <= 1280
        - The sum of `top` and `height` must be must be <= 720

# Submission File
For each `video_frame` in the test set, you must predict a bounding box left, width, top, height, and predicted label of the assigned player. The file should contain a header and have the following format:
```
video_frame,left,width,top,height,label
57590_003607_Endzone_1,1,1,1,1,H1
57590_003607_Sideline_1,1,1,1,1,V59
57595_001252_Endzone_1,1,1,1,1,V52
```"
Tabular Playground Series - Aug 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-aug-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/28008/logos/header.png?t=2021-06-30-01-11-56,"Tabular,Regression,Banking",1753,1841,16696,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are scored on the **root mean squared error**. RMSE is defined as:

$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2} $$

where \\( \hat{y} \\) is the predicted value, \\( y \\) is the ground truth value, and \\( n \\) is the number of rows in the test data.

## Submission File
For each row `id` in the test set, you must predict the value of the target `loss` as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:

    id,loss
    250000,0.0
    250001,10.3
    250002,42.42
    etc.
"
RSNA-MICCAI Brain Tumor Radiogenomic Classification,Predict the status of a genetic biomarker important for brain cancer treatment,https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/29653/logos/header.png?t=2021-07-07-17-26-56,"Image,Binary Classification,Healthcare",1555,1958,27466,"A malignant tumor in the brain is a life-threatening condition. Known as glioblastoma, it's both the most common form of brain cancer in adults and the one with the worst prognosis, with median survival being less than a year. The presence of a specific genetic sequence in the tumor known as MGMT promoter methylation has been shown to be a favorable prognostic factor and a strong predictor of responsiveness to chemotherapy. 

<img src=""https://storage.googleapis.com/kaggle-media/competitions/RSNA-2021/image2.png"" style=""float: right; width: 200px; height: 250px"">

Currently, genetic analysis of cancer requires surgery to extract a tissue sample. Then it can take several weeks to determine the genetic characterization of the tumor. Depending upon the results and type of initial therapy chosen, a subsequent surgery may be necessary. If an accurate method to predict the genetics of the cancer through imaging (i.e., radiogenomics) alone could be developed, this would potentially minimize the number of surgeries and refine the type of therapy required.

The Radiological Society of North America (RSNA) has teamed up with the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) to improve diagnosis and treatment planning for patients with glioblastoma. In this competition you will predict the genetic subtype of glioblastoma using MRI (magnetic resonance imaging) scans to train and test your model to detect for the presence of MGMT promoter methylation.

If successful, you'll help brain cancer patients receive less invasive diagnoses and treatments. The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer.

###Acknowledgments

**The Radiological Society of North America (RSNA®)** is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation.

RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world’s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions.

**The Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society)** is dedicated to the promotion, preservation and facilitation of research, education and practice in the field of medical image computing and computer assisted medical interventions including biomedical imaging and medical robotics. The Society achieves this aim through the organization and operation of annual high quality international conferences, workshops, tutorials and publications that promote and foster the exchange and dissemination of advanced knowledge, expertise and experience in the field produced by leading institutions and outstanding scientists, physicians and educators around the world. 

[A full set of acknowledgments can be found on this page](https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/overview/acknowledgments).

![](https://storage.googleapis.com/kaggle-media/competitions/RSNA-2021/sponsors.png)","Submissions are evaluated on the [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `BraTS21ID` in the test set, you must predict a probability for the target `MGMT_value`. The file should contain a header and have the following format:

    BraTS21ID,MGMT_value
    00001,0.5
    00013,0.5
    00015,0.5
    etc.
"
Tabular Playground Series - Jul 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-jul-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/28007/logos/header.png?t=2021-06-30-01-10-51,"Tabular,Pollution,Time Series Analysis",1293,1346,13534,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is based on a real dataset, but has synthetic-generated aspects to it. The original dataset deals with predicting air pollution in a city via various input sensor values (e.g., a time series).

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated using the mean column-wise root mean squared  logarithmic error.

The RMSLE for a single column calculated as:

$$ \sqrt{\frac{1}{n} \sum\_{i=1}^n (\log(p\_i + 1) - \log(a\_i+1))^2 },$$

where:
 
\\( n \\) is the total number of observations
\\( p\_i \\) is your prediction
\\( a\_i \\) is the actual value
\\( \log(x) \\) is the natural logarithm of \\( x \\)

The final score is the mean of the RMSLE over all columns, in this case, 3.


## Submission File
For each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:

    date_time,target_carbon_monoxide,target_benzene,target_nitrogen_oxides
    2011-01-01 01:00:00,2.0,10.0,300.0
    2011-01-01 02:00:00,2.0,10.0,300.0
    2011-01-01 03:00:00,2.0,10.0,300.0
    etc.
"
G2Net Gravitational Wave Detection,Find gravitational wave signals from binary black hole collisions,https://www.kaggle.com/competitions/g2net-gravitational-wave-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/23249/logos/header.png?t=2021-05-26-16-18-03,"Signal Processing,Astronomy,NumPy",1219,1501,28222,"It's been said that teamwork makes the dream work. This couldn't be truer for the breakthrough discovery of gravitational waves (GW), signals from colliding binary black holes in 2015. It required the collaboration of experts in physics, mathematics, information science, and computing. GW signals have led researchers to observe a new population of massive, stellar-origin black holes, to unlock the mysteries of neutron star mergers, and to measure the expansion of the Universe. These signals are unimaginably tiny ripples in the fabric of space-time and even though the global network of GW detectors are some of the most sensitive instruments on the planet, the signals are buried in detector noise. Analysis of GW data and the detection of these signals is a crucial mission for the growing global network of increasingly sensitive GW detectors. These challenges in data analysis and noise characterization could be solved with the help of data science.

As with the multi-disciplined approach to the discovery of GWs, additional expertise will be needed to further GW research. In particular, social and natural sciences have taken an interest in machine learning, deep learning, classification problems, data mining, and visualization to develop new techniques and algorithms to efficiently handle complex and massive data sets. The increase in computing power and the development of innovative techniques for the rapid analysis of data will be vital to the exciting new field of GW Astronomy. Potential outcomes may include increased sensitivity to GW signals, application to control and feedback systems for next-generation detectors, noise removal, data conditioning tools, and signal characterization.

G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.

In this competition, you’ll aim to detect GW signals from the mergers of binary black holes. Specifically, you'll build a model to analyze simulated GW time-series data from a network of Earth-based detectors. 




![](https://storage.googleapis.com/kaggle-media/competitions/G2Net-gravitational-waves/800px-LIGO_measurement_of_gravitational_waves.svg.png)
*The series of images above were taken from the 2015 [paper](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.116.061102) announcing the discovery of gravitational waves from a pair of merging black holes.*

If successful, you'll play a part in solving a crucial mission in the exciting new field of GW science. With the development of new algorithms, scientists will have a better handle on the potential power of the data science community and their innovative approaches to data analysis. Moreover, it will enable closer interaction between computer science and physics, which could benefit both disciplines. Your participation can further this collaboration and the help advance this breakthrough discovery.

### Acknowledgments

We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the software resource [lalsuite](https://git.ligo.org/lscsoft/lalsuite).","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    00005bced6,0.5
    0000806717,0.5
    0000ef4fe1,0.5
    etc.
"
Tabular Playground Series - Jun 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-jun-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/26480/logos/header.png?t=2021-04-09-00-57-05,"Beginner,Tabular,Multiclass Classification,Classification",1171,1209,11888,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true `Class`. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:

$$ \text{log loss} = -\frac{1}{N}\sum\_{i=1}^N\sum\_{j=1}^My_{ij}\log(p\_{ij}), $$

where \\(N\\) is the number of rows in the test set, \\(M\\) is the number of class labels, \\( \text{log}\\) is the natural logarithm, \\(y\_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p\_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).

The submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the \\(\text{log}\\) function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).

## Submission File
You must submit a csv file with the product `id` and the predicted probability that the product belongs to each of the classes seen in the dataset. The order of the rows does not matter. The file must have a header and should look like the following:

    id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9
    200000,0.05,0.14,0.21,0.05,0.20,0.04,0.00,0.20,0.11
    200001,0.21,0.06,0.10,0.20,0.13,0.01,0.04,0.10,0.15
    200002,0.15,0.12,0.18,0.10,0.16,0.16,0.03,0.01,0.09
    etc.
"
SIIM-FISABIO-RSNA COVID-19 Detection,Identify and localize COVID-19 abnormalities on chest radiographs,https://www.kaggle.com/competitions/siim-covid19-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/26680/logos/header.png?t=2021-04-23-22-04-05,"Image,Multilabel Classification",1305,1786,32307,"Five times more deadly than the flu, COVID-19 causes significant morbidity and mortality. Like other pneumonias, pulmonary infection with COVID-19 results in inflammation and fluid in the lungs. COVID-19 looks very similar to other viral and bacterial pneumonias on chest radiographs, which makes it difficult to diagnose. Your computer vision model to detect and localize COVID-19 would help doctors provide a quick and confident diagnosis. As a result, patients could get the right treatment before the most severe effects of the virus take hold.

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2Fd514aaf604bc9667b518b232a77d1aa7%2FCXR%20image1.jpg?generation=1620769201081719&alt=media"" width=""200"" style=""float: right""></img>

Currently, COVID-19 can be diagnosed via polymerase chain reaction to detect genetic material from the virus or chest radiograph. However, it can take a few hours and sometimes days before the molecular test results are back. By contrast, chest radiographs can be obtained in minutes. While guidelines exist to help radiologists differentiate COVID-19 from other types of infection, their assessments vary. In addition, non-radiologists could be supported with better localization of the disease, such as with a visual bounding box.

As the leading healthcare organization in their field, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation. SIIM has partnered with the Foundation for the Promotion of Health and Biomedical Research of Valencia Region (FISABIO), Medical Imaging Databank of the Valencia Region (BIMCV) and the Radiological Society of North America (RSNA) for this competition.

In this competition, you’ll identify and localize COVID-19 abnormalities on chest radiographs. In particular, you'll categorize the radiographs as negative for pneumonia or typical, indeterminate, or atypical for COVID-19. You and your model will work with imaging data and annotations from a group of radiologists.

If successful, you'll help radiologists diagnose the millions of COVID-19 patients more confidently and quickly. This will also enable doctors to see the extent of the disease and help them make decisions regarding treatment. Depending upon severity, affected patients may need hospitalization, admission into an intensive care unit, or supportive therapies like mechanical ventilation. As a result of better diagnosis, more patients will quickly receive the best care for their condition, which could mitigate the most severe effects of the virus.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/siim-covid19-detection/overview/code-requirements) for details.**

<br>
### Host Organizations

**FISABIO, The Foundation for the Promotion of Health and Biomedical Research of Valencia Region**

The Foundation for the Promotion of Health and Biomedical Research of Valencia Region, FISABIO, is a non-profit scientific and healthcare entity, whose primary purpose is to encourage, to promote and to develop scientific and technical health and biomedical research in Valencia Region. FISABIO integrates and manages the Health Research Map of the Centre for Public Health Research, Dr. Peset University Hospital Foundation, Alicante University General Hospital Foundation, Elche University General Hospital Foundation, and the Mediterranean Ophthalmological Foundation. The BIMCV facility is connected with a multi-level vendor neutral archive (VNA). The imaging population facility is storing data from the Valencia Region, which accounts for more than 5.1 million habitants.

**Radiological Society of North America (RSNA)**

The Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation.

RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world’s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions.
","The challenge uses the standard PASCAL VOC 2010 [mean Average Precision (mAP) ](https://storage.googleapis.com/kaggle-media/competitions/SIIM2021/VOC2012_doc.pdf) at IoU > `0.5`. Note that the linked document describes VOC 2012, which differs in some minor ways (e.g. there is no concept of ""difficult"" classes in VOC 2010). The P/R curve and AP calculations remain the same.

In this competition, we are making predictions at both a study (multi-image) and image level.

### Study-level labels
Studies in the test set may contain more than one label. They are as follows:

> ""negative"", ""typical"", ""indeterminate"", ""atypical""

Please see the Data page for further details.

For each study in the test set, you should predict at least one of the above labels. The format for a given label's prediction would be a class ID from the above list, a `confidence` score, and `0 0 1 1` is a one-pixel bounding box.

### Image-level labels
Images in the test set may contain more than one object. For each object in a given test image, you must predict a class ID of ""opacity"", a `confidence` score, and bounding box in format `xmin ymin xmax ymax`. If you predict that there are NO objects in a given image, you should predict `none 1.0 0 0 1 1`, where `none` is the class ID for ""No finding"", 1.0 is the confidence, and `0 0 1 1` is a one-pixel bounding box.

## Submission File

The submission file should contain a header and have the following format:

    Id,PredictionString
    2b95d54e4be65_study,negative 1 0 0 1 1
    2b95d54e4be66_study,typical 1 0 0 1 1
    2b95d54e4be67_study,indeterminate 1 0 0 1 1 atypical 1 0 0 1 1
    2b95d54e4be68_image,none 1 0 0 1 1
    2b95d54e4be69_image,opacity 0.5 100 100 200 200 opacity 0.7 10 10 20 20
    etc.
"
Google Smartphone Decimeter Challenge,Improve high precision GNSS positioning and navigation accuracy on smartphones,https://www.kaggle.com/competitions/google-smartphone-decimeter-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/26933/logos/header.png?t=2021-04-01-16-13-07,"Geospatial Analysis,Tabular,Mobile and Wireless,Research,Signal Processing",810,985,16802,"Have you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2F88764e97cd9c2df83195454039b9e544%2Fsmartphone-dec.png?generation=1620664283744618&alt=media"" width=""200"" style=""float: right""></img>

Global Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a “jumpy” experience. For many use cases the results are not fine nor stable enough to be reliable.

This competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them. 

In this competition, you'll use data collected from the host team’s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions. 

If successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go.

### Acknowledgments
The Android GPS team would like to show its appreciation to Verizon Hyper Precise Location Service and Swift Navigation Skylark Correction Service who provided assistance data for datasets in the challenge.
","Submissions are scored on the mean of the 50th and 95th percentile distance errors. For every `phone` and at every `millisSinceGpsEpoch`, the horizontal distance (in meters) is computed between the predicted lat/lng and the ground truth lat/lng. These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller). The 50th and 95th percentile errors are then averaged for each phone. Lastly, the mean of these averaged values is calculated across all phones in the test set.

## Submission File
For each `phone` and `millisSinceGpsEpoch` in the test set, you must predict the latitude and longitude. You must provide estimations for all timestamps that have at least four valid GNSS signals. The sample submission contains a list of these timestamps. The submission file should contain a header and have the following format:

```
phone,millisSinceGpsEpoch,latDeg,lngDeg
2020-05-15-US-MTV-1_Pixel4,1273608785432,53.599227001298125,-2.4339795741464334
2020-05-15-US-MTV-1_Pixel4,1273608786432,53.599227001298125,-2.4339795741464334
2020-05-15-US-MTV-1_Pixel4,1273608787432,53.599227001298125,-2.4339795741464334    
```
etc.
"
SETI Breakthrough Listen - E.T. Signal Search,Find extraterrestrial signals in data from deep space  ,https://www.kaggle.com/competitions/seti-breakthrough-listen,https://storage.googleapis.com/kaggle-competitions/kaggle/23652/logos/header.png?t=2021-02-24-19-15-30,"Astronomy,Signal Processing,Science and Technology",768,979,17367,"####**“Are we alone in the Universe?”**     
It’s one of the most profound—and perennial—human questions. As technology improves, we’re finding new and more powerful ways to seek answers. The Breakthrough Listen team at the University of California, Berkeley, employs the world’s most powerful telescopes to scan millions of stars for signs of technology. Now it wants the Kaggle community to help interpret the signals they pick up.

The Listen team is part of the Search for ExtraTerrestrial Intelligence (SETI) and uses the largest steerable dish on the planet, the 100-meter diameter Green Bank Telescope. Like any SETI search, the motivation to communicate is also the major challenge. Humans have built enormous numbers of radio devices. It’s hard to search for a faint needle of alien transmission in the huge haystack of detections from modern technology. 

Current methods use two filters to search through the haystack. First, the Listen team intersperses scans of the target stars with scans of other regions of sky. Any signal that appears in both sets of scans probably isn’t coming from the direction of the target star. Second, the pipeline discards signals that don’t change their frequency, because this means that they are probably nearby the telescope. A source in motion should have a signal that suggests movement, similar to the change in pitch of a passing fire truck siren. These two filters are quite effective, but we know they can be improved. The pipeline undoubtedly misses interesting signals, particularly those with complex time or frequency structure, and those in regions of the spectrum with lots of interference.

In this competition, use your data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call “needles”) in the haystack of data from the telescope. They have identified some of the hidden needles so that you can train your model to find more. The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more. The algorithm that’s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.

![](https://storage.googleapis.com/kaggle-media/competitions/SETI-Berkeley/DSC_4014-Edit_2.jpg)


### Acknowledgments 

The Breakthrough Listen science and engineering effort is headquartered at the University of California, Berkeley SETI Research Center. The Breakthrough Prize Foundation funds the Breakthrough Initiatives which manages Breakthrough Listen. The Green Bank Observatory is supported by the National Science Foundation, and is operated by Associated Universities, Inc. under a cooperative agreement.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    00034abb3629,0.5
    0004be0baf70,0.5
    0005be4d0752,0.5
    etc.
"
CommonLit Readability Prize,Rate the complexity of literary passages for grades 3-12 classroom use,https://www.kaggle.com/competitions/commonlitreadabilityprize,https://storage.googleapis.com/kaggle-competitions/kaggle/25914/logos/header.png?t=2021-04-01-15-58-06,"Text,Regression",3633,4393,72150,"Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success.  When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.

Currently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.

CommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.

In this competition, you’ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.

If successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills. 


### Acknowledgements    

CommonLit would like to extend a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project. 
 
The organizers would like to thank Schmidt Futures for their advice and support for making this work possible.

<a href=""https://schmidtfutures.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/CommonLit/Schmidt%20Futures%20Logo.png"" style=""width: 350px""><a href=""https://alsl.gsu.edu/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/CommonLit/GSU-PrimaryLogo3color.jpeg"" style=""width: 200px""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/commonlitreadabilityprize/overview/code-requirements) for details.**","Submissions are scored on the **root mean squared error**. RMSE is defined as:

$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2} $$

where \\( \hat{y} \\) is the predicted value, \\( y \\) is the original value, and \\( n \\) is the number of rows in the test data.

## Submission File
For each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:

    id,target
    eaf8e7355,0.0
    60ecc9777,0.5
    c0f722661,-2.0
    etc."
Tabular Playground Series - May 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-may-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/26479/logos/header.png?t=2021-04-09-00-55-58,"Tabular,Beginner,Multiclass Classification,Classification",1097,1150,11614,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams, to inspire broad participation we are limiting winner's of swag to once per person for this series. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https://github.com/sdv-dev/CTGAN). The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

For ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true `Class`. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:

$$ \text{log loss} = -\frac{1}{N}\sum\_{i=1}^N\sum\_{j=1}^My_{ij}\log(p\_{ij}), $$

where \\(N\\) is the number of rows in the test set, \\(M\\) is the number of class labels, \\( \text{log}\\) is the natural logarithm, \\(y\_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p\_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).

The submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the \\(\text{log}\\) function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).

## Submission File
You must submit a csv file with the product `id` and the predicted probability that the product belongs to each of the classes seen in the dataset. The order of the rows does not matter. The file must have a header and should look like the following:

    id,Class_1,Class_2,Class_3,Class_4
    100000,0.1,0.3,0.2,0.4
    100001,0.5,0.1,0.1,0.3
    100002,0.4,0.4,0.1,0.1
    etc.
"
BirdCLEF 2021 - Birdcall Identification,"Identify bird calls in soundscape recordings
",https://www.kaggle.com/competitions/birdclef-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/25954/logos/header.png?t=2021-03-19-18-32-57,"Audio,Environment",816,1001,9307,"Birds of a feather flock together. Thankfully, this makes it easier to hear them! There are over 10,000 bird species around the world. Identifying the red-winged blackbirds or Bewick’s wrens in an area, for example, can provide important information about the habitat. As birds are high up in the food chain, they are excellent indicators of deteriorating environmental quality and pollution. Monitoring the status and trends of biodiversity in ecosystems is no small task. With proper sound detection and classification—aided by machine learning—researchers can improve their ability to track the status and trends of biodiversity in important ecosystems, enabling them to better support global conservation efforts.

<img title=”Oriole” src=""https://storage.googleapis.com/kaggle-media/competitions/Birdsong/Oriole.png"" style=""float: right; width: 250px"">

Recent advances in machine listening have improved acoustic data collection. However, it remains a challenge to generate analysis outputs with high precision and recall. The majority of data is unexamined due to a lack of effective tools for efficient and reliable extraction of the signals of interests (e.g., bird calls).

The Cornell Lab of Ornithology is dedicated to advancing the understanding and protection of birds and the natural world. The Lab joins with people from all walks of life to make new scientific discoveries, share insights, and galvanize conservation action. For this competition, they're collaborating with Google Research, LifeCLEF, and Xeno-canto.

In this competition, you’ll automate the acoustic identification of birds in soundscape recordings.  You'll examine an acoustic dataset to  build detectors and classifiers to extract the signals of interest (bird calls). Innovative solutions will be able to do so efficiently and reliably.

The ornithology community is collecting many petabytes of acoustic data every year, but the majority of data remains unexamined. If successful, you'll help researchers properly detect and classify bird sounds, significantly improving their ability to monitor the status and trends of biodiversity in important ecosystems. Researchers will better be able to infer factors about an area’s quality of life based on a changing bird population, which allows them to identify how they can best support global conservation efforts.

&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/birdclef-2021/overview/code-requirements) for details.**

The LifeCLEF Bird Recognition Challenge ([BirdCLEF](https://www.imageclef.org/BirdCLEF2021)) focuses on developing machine learning algorithms to identify avian vocalizations in continuous soundscape data to aid conservation efforts worldwide. Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity.
","Submissions will be evaluated based on their row-wise micro averaged [F1 score](https://en.wikipedia.org/wiki/F1_score).

For each `row_id`/time window, you need to provide a **space delimited** list of the set of unique `birds` that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code `nocall`.

The submission file must have a header and should look like the following:

## Submission File
```
row_id,birds
3575_COL_5,wewpew batpig1
3575_COL_10,wewpew batpig1
3575_COL_15,wewpew batpig1
...
```

## Working Note Award Criteria (optional)

Criteria for the BirdCLEF best working note award:

**Originality**. The value of a paper is a function of the degree to which it presents new or novel technical material. Does the paper present results previously unknown? Does it push forward the frontiers of knowledge? Does it present new methods for solving old problems or new viewpoints on old problems? Or, on the other hand, is it a re-hash of information already known?

**Quality**. A paper's value is a function of the innate character or degree of excellence of the work described. Was the work performed, or the study made with a high degree of thoroughness? Was high engineering skill demonstrated? Is an experiment described which has a high degree of elegance? Or, on the other hand, is the work described pretty much of a run-of-the-mill nature?

**Contribution**. The value of a paper is a function of the degree to which it represents an overall contribution to the advancement of the art. This is different from originality. A paper may be highly original but may be concerned with a very minor, or even insignificant, matter or problem. On the other hand, a paper may make a great contribution by collecting and analyzing known data and facts and pointing out their significance. Or, a fine exposition of a known but obscure or complex phenomenon or theory or system or operating technique may be a very real contribution to the art. Obviously, a paper may well score highly on both originality and contribution. Perhaps a significant question is, will the engineer who reads the paper be able to practice his profession more effectively because of having read it?

**Presentation**. The value of the paper is a function of the ease with which the reader can determine what the author is trying to present. Regardless of the other criteria, a paper is not good unless the material is presented clearly and effectively. Is the paper well written? Is the meaning of the author clear? Are the tables, charts, and figures clear? Is their meaning readily apparent? Is the information presented in the paper complete? At the same time, is the paper concise?

*Evaluation of the submitted BirdCLEF working notes:*

Each working note will be reviewed by two reviewers and scores averaged. Maximum score: 15.

a) Evaluation of work and contribution	
- 5 points: Excellent work and a major contribution
- 4 points: Good solid work of some importance
- 3 points: Solid work but a marginal contribution
- 2 points: Marginal work and minor contribution
- 1 point: Work doesn't meet scientific standards

b) Originality and novelty	
- 5 points Trailblazing
- 4 points: A pioneering piece of work
- 3 points: One step ahead of the pack
- 2 points: Yet another paper about...
- 1 point: It's been said many times before

c) Readability and organization
- 5 points: Excellent
- 4 points: Well written
- 3 points: Readable
- 2 points: Needs considerable work
- 1 point: Work doesn't meet scientific standards
"
Tabular Playground Series - Apr 2021,Synthanic - You're going to need a bigger boat,https://www.kaggle.com/competitions/tabular-playground-series-apr-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/26478/logos/header.png?t=2021-03-29-17-07-03,"Beginner,Tabular,Binary Classification",1244,1306,15345,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset.  These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual [Titanic data](https://www.kaggle.com/c/titanic/data)!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to ""cheat"" by using public labels for predictions. How well does your model perform on truly private test labels?

Good luck and have fun!

### Getting Started

Check out the original [Titanic competition](https://www.kaggle.com/c/titanic/overview) which walks you through how to build various models.

For more ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","## Goal
Your task is to predict whether or not a passenger survived the sinking of the Synthanic (a synthetic, much larger dataset based on the actual Titanic dataset). For each `PasengerId` row in the test set, you must predict a 0 or 1 value for the `Survived ` target.</p>

Your score is the percentage of passengers you correctly predict. This is known as [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification).

## Submission File
You should submit a csv file with exactly 100,000 rows _plus_ a header row. Your submission will show an error if you have extra columns or extra rows.

The file should have exactly 2 columns:
 - `PassengerId` (sorted in any order)
 - `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)

You can download an example submission file (sample_submission.csv) on the [Data page](https://www.kaggle.com/c/tabular-playground-series-apr-2021/data)

    PassengerId,Survived
    100000,0
    100001,1
    100002,0
    etc."
Coleridge Initiative - Show US the Data ,Discover how data is used for the public good,https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data,https://storage.googleapis.com/kaggle-competitions/kaggle/25925/logos/header.png?t=2021-03-24-14-11-29,"Text,Research",1610,1948,25957,"This competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including;  pandemics, climate change,  Alzheimer’s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications. 

Can natural language processing find the hidden-in-plain-sight data citations? Can machine learning find the link between the words used in research articles and the data referenced in the article? 

Now is the time for data scientists to help restore trust in data and evidence. In the United States, federal agencies are now mandated to show how their data are being used. The new [Foundations of Evidence-based Policymaking Act](https://www.cio.gov/policies-and-priorities/evidence-based-policymaking/) requires agencies to modernize their data management. New [Presidential Executive Orders](https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/27/memorandum-on-restoring-trust-in-government-through-scientific-integrity-and-evidence-based-policymaking/) are pushing government agencies to make evidence-based decisions based on the best available data and science. And the government is working to respond in an [open and transparent way](https://www.bea.gov/evidence).

This competition will build just such an open and transparent approach. The results will show how public data are being used in science and help the government make wiser, more transparent public investments. It will help move researchers and governments from using ad-hoc methods to automated ways of finding out what datasets are being used to solve problems, what measures are being generated, and which researchers are the experts. Previous competitions have shown that it is possible to develop algorithms to automate the search and discovery of references to data. Now, we want the Kaggle community to develop the best approaches to identify critical datasets used in scientific publications.    

In this competition, you'll use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications. Utilizing the full text of scientific publications from numerous research areas gathered from [CHORUS](https://www.chorusaccess.org/) publisher members and other sources, you'll identify data sets that the publications' authors used in their work.  

If successful, you'll help support evidence in government data. Automated NLP approaches will enable government agencies and researchers to quickly find the information they need. The approach will be used to develop data usage scorecards to better enable agencies to show how their data are used and bring down a critical barrier to the access and use of public data.    

The Coleridge Initiative is a not-for-profit that has been established to use data for social good. One way in which the organization does this is by furthering science through publicly available research.    

### Resources 
[Coleridge Data Examples](https://coleridgeinitiative.org/data-products/)
[Rich Search and Discovery for Research Datasets](https://study.sagepub.com/richcontext)
[Democratizing Our Data](https://mitpress.mit.edu/books/democratizing-our-data)
[NSF""Rich Context"" Video](https://youtu.be/PIReIlsTI8U)

### Acknowledgments
United States Department of Agriculture
United States Department of Commerce
United States Geological Survey
National Oceanic and Atmospheric Administration
National Science Foundation
National Institutes of Health
CHORUS
Westat
Alfred P. Sloan Foundation
Schmidt Futures
Overdeck Family Foundation

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/overview/code-requirements) for details.**","The objective of the competition is to identify the mention of datasets within scientific publications. Your predictions will be short excerpts from the publications that appear to note a dataset.

Submissions are evaluated on a [Jaccard-based](https://en.wikipedia.org/wiki/Jaccard_index) [FBeta](https://en.wikipedia.org/wiki/F-score) score between predicted texts and ground truth texts, with `Beta = 0.5` (a `micro F0.5` score). Multiple predictions are delineated with a pipe (`|`) character in the submission file.

The following is Python code for calculating the Jaccard score for a single prediction string against a single ground truth string. Note that the overall score for a sample uses Jaccard to compare multiple ground truth and prediction strings that are pipe-delimited - this code does not handle that process or the final `micro F-beta` calculation.
    
    def jaccard(str1, str2): 
        a = set(str1.lower().split()) 
        b = set(str2.lower().split())
        c = a.intersection(b)
        return float(len(c)) / (len(a) + len(b) - len(c))

Note that ALL ground truth texts have been cleaned for matching purposes using the following code:
    
    def clean_text(txt):
        return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())

For each publication's set of predictions, a token-based Jaccard score is calculated for each potential prediction / ground truth pair. The prediction with the highest score for a given ground truth is **matched** with that ground truth.
- Predicted strings for each publication are **sorted alphabetically** and processed in that order. Any scoring ties are resolved on the basis of that sort.
- Any matched predictions where the Jaccard score meets or exceeds the threshold of `0.5` are counted as true positives (`TP`), the remainder as false positives (`FP`).
- Any unmatched predictions are counted as false positives (`FP`).
- Any ground truths with no nearest predictions are counted as false negatives (`FN`).

All `TP`, `FP` and `FN` across all samples are used to calculate a final `micro F0.5` score. (Note that a `micro` F score does precisely this, creating one pool of `TP`, `FP` and `FN` that is used to calculate a score for the entire set of predictions.) 

## Submission File
For each publication Id in the test set, you must predict excerpts (multiple excerpts divided by a pipe character) for `PredictionString` variable. The file should contain a header and have the following format:

    Id,PredictionString
    000e04d6-d6ef-442f-b070-4309493221ba,space objects dataset|small objects data
    0176e38e-2286-4ea2-914f-0583808a98aa,small objects dataset
    01860fa5-2c39-4ea2-9124-74458ae4a4b4,large objects
    01e4e08c-ffea-45a7-adde-6a0c0ad755fc,space location data|national space objects|national space dataset
    01fea149-a6b8-4b01-8af9-51e02f46f03f,a dataset of large objects
    etc.
"
Plant Pathology 2021 - FGVC8 ,Identify the category of foliar diseases in apple trees,https://www.kaggle.com/competitions/plant-pathology-2021-fgvc8,https://storage.googleapis.com/kaggle-competitions/kaggle/25563/logos/header.png?t=2021-02-28-03-55-21,"Image,Plants",626,857,10483,"## Problem Statement 
Apples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive.     

Although computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc. 

Plant Pathology 2020-FGVC7 challenge competition had a pilot dataset of 3,651 RGB images of foliar disease of apples. For Plant Pathology 2021-FGVC8, we have significantly increased the number of foliar disease images and added additional disease categories. This year’s dataset contains approximately 23,000 high-quality RGB images of apple foliar diseases, including a large expert-annotated disease dataset. This dataset reflects real field scenarios by representing non-homogeneous backgrounds of leaf images taken at different maturity stages and at different times of day under different focal camera settings.

## Specific Objectives
The main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image. <p></p>

## Resources

Details and background information on the dataset and Kaggle competition ‘Plant Pathology 2020 Challenge’ were published as a peer-reviewed research article. If you use the dataset for your project, please cite the following

[Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020.](https://bsapubs.onlinelibrary.wiley.com/doi/10.1002/aps3.11390)

### Acknowledgements
We acknowledge sponsorship from Cornell Initiative for Digital Agriculture (CIDA).


<br>
&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/plant-pathology-2021-fgvc8/overview/code-requirements) for details.**","
<p>The evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/MeanFScore""> Mean F1-Score</a>. 
</p><h2>Submission Format</h2>
<p><strong>For every author in the dataset</strong>, submission files should contain two columns: image and labels. labels should be a space-delimited list. </p>
<p>The file should contain a header and have the following format:</p>
<pre>image, labels<br>85f8cb619c66b863.jpg,healthy<br>ad8770db05586b59.jpg,healthy<br>c7b03e718489f3ca.jpg,healthy<br></pre>"
Hotel-ID to Combat Human Trafficking 2021 - FGVC8,Recognizing hotels to aid Human trafficking investigations,https://www.kaggle.com/competitions/hotel-id-2021-fgvc8,https://storage.googleapis.com/kaggle-competitions/kaggle/25980/logos/header.png?t=2021-03-09-19-11-13,"Image,Public Safety",92,129,1083,"<h2>Hotel Recognition to Combat Human Trafficking</h2>
<p>Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.</p>

![Example investigative images.](https://cs.slu.edu/~astylianou/images/example_victim_images.png)

<p>Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about <a href=""https://techcrunch.com/2016/06/25/traffickcam/"">TraffickCam on TechCrunch.</a></p>

<p>Example images from one hotel in the TraffickCam dataset are shown below:</p>

![Example TraffickCam images.](https://cs.slu.edu/~astylianou/images/example_traffickcam_images.png)

<p>In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs.</p>

<p>Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system.</p>

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/hotel-id-2021-fgvc8/overview/code-requirements) for details.**","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):

$$MAP@5 = \frac{1}{U} \sum\_{u=1}^{U}  \sum\_{k=1}^{min(n,5)} P(k) \times rel(k)$$

where \\( U \\)  is the number of images, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number of predictions per image, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant correct label, zero otherwise.

Once a correct label has been scored for *an observation*, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.

    A B C D E
    A A A A A
    A B A C A


## Submission File

For each image in the test set, you must predict a space-delimited list of hotel IDs that could match that image. The list should be sorted such that the first ID is considered the most relevant one and the last the least relevant one. The file should contain a header and have the following format:

```
image,hotel_id 
99e91ad5f2870678.jpg,36363 53586 18807 64314 60181
b5cc62ab665591a9.jpg,36363 53586 18807 64314 60181
d5664a972d5a644b.jpg,36363 53586 18807 64314 60181
```"
iWildcam 2021 - FGVC8,Count the number of animals of each species present in a sequence of images,https://www.kaggle.com/competitions/iwildcam2021-fgvc8,https://storage.googleapis.com/kaggle-competitions/kaggle/24911/logos/header.png?t=2021-03-02-03-13-50,"Image,Animals",42,65,579,"##Description

Camera traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance and density of species in camera trap data, ecologists need to know not just which species were seen, but also **how many** of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over or undercounting. For example, if you get 3 images taken at one frame per second and in the first image you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species as it requires reasoning and tracking of individuals across sparse temporal samples.


![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F115173%2Ffc6541055bb381b99aac808449bebcab%2Ftrain_examples_smallest.gif?generation=1615337929634349&alt=media)


We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap but are not identical. The challenge is to categorize species and count the number of individuals across image bursts. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from [Landsat 8][9]) for each of the camera trap locations. On the competition [GitHub page][6] we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections.

This is an FGVCx competition as part of the [FGVC8][4] workshop at [CVPR 2021][5] and is sponsored by [Microsoft AI for Earth][3] and [Wildlife Insights][8]. There is a GitHub page for the competition [here][6]. Please open an issue if you have questions or problems with the dataset.   
  


You can find the iWildCam 2018 Competition [here](https://github.com/visipedia/iwildcam_comp/blob/master/2018/readme.md), the iWildCam 2019 Competition [here](https://github.com/visipedia/iwildcam_comp/blob/master/2019/readme.md), and the 
iWildCam 2020 Competition [here](https://github.com/visipedia/iwildcam_comp/blob/master/2020/readme.md).

##Acknowledgements
We would like to acknowledge [WCS](https://www.wcs.org/) for providing the camera trap data, [Centaur Labs](https://www.centaurlabs.com/) for generously providing count annotations on the test data, and [Microsoft AI4Earth](https://www.microsoft.com/en-us/ai/ai-for-earth) for hosting our external datasets on Azure. 

*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.* 

  [1]: http://lila.science/datasets/wcscameratraps
  [2]: https://github.com/visipedia/inat_comp
  [3]: https://www.microsoft.com/en-us/ai/ai-for-earth
  [4]: https://sites.google.com/view/fgvc8/home
  [5]: http://cvpr2021.thecvf.com/
  [6]: https://github.com/visipedia/iwildcam_comp
  [7]: https://github.com/microsoft/CameraTraps/blob/master/megadetector.md
  [8]: https://www.wildlifeinsights.org/
  [9]: https://www.usgs.gov/land-resources/nli/landsat/landsat-8 
","##Evaluation

Submissions will be evaluated using Mean Columnwise Root Mean Squared Error (MCRMSE) where each column `j` represents a species, each row `i` represents a sequence, `x_ij` is the predicted count for that species in that sequence, and `y_ij` is the ground truth count.

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F115173%2F12771753f3e7f31aeacd35631826a2fa%2FMCRMSE.png?generation=1615343157324930&alt=media)


We selected this metric out of the options provided by Kaggle in order to capture both species identification mistakes and count mistakes and to ensure false predictions on empty sequences would contribute to the error. Because many sequences are empty in camera trap data due to false triggers and because many species are rare, the error from this normalized metric looks quite small, while the actual errors in counts are still large. To convert the metric to something more interpretable from an ecological standpoint, you can un-normalize the metric from MCRMSE to the Summed Columnwise Root Summed Squared Error (SCRSSE) by multiplying by the number of categories and the square root of the number of test sequences (i.e. scale the error on the leaderboard by m sqrt(n)).

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F115173%2F426bee6baa91622f526b43635877405e%2Fscrsse.png?generation=1615343169987674&alt=media)


## Submission Format

Submission for the competition is a csv file with the following format:

    Id,Predicted2,Predicted3,[...],Predicted571
    58857ccf-23d2-11e8-a6a3-ec086b02610b,0,5,[...],0
    591e4006-23d2-11e8-a6a3-ec086b02610b,1,0,[...],3


The `Id` column corresponds to the test sequence id. The `Predicted2` holds an integer value that indicates the number of individuals of species 2 in that test sequence. If you predict there are no animals in the sequence, the entire row after the sequence ID should be populated with `0` values.
"
Herbarium 2021 - Half-Earth Challenge - FGVC8,"Identify plant species of the Americas, Oceania and the Pacific from herbarium specimens",https://www.kaggle.com/competitions/herbarium-2021-fgvc8,https://storage.googleapis.com/kaggle-competitions/kaggle/25558/logos/header.png?t=2021-03-05-17-29-30,"Image,Plants",80,108,573,"*The Herbarium 2021: Half-Earth Challenge* is to identify vascular plant specimens provided by the [New York Botanical Garden](https://www.nybg.org/) (NY), [Bishop Museum](https://www.bishopmuseum.org/) (BPBM), [Naturalis Biodiversity Center](https://www.naturalis.nl/en) (NL), [Queensland Herbarium](https://www.qld.gov.au/environment/plants-animals/plants/herbarium) (BRI), and [Auckland War Memorial Museum](https://www.aucklandmuseum.com/) (AK).

*The Herbarium 2021: Half-Earth Challenge* dataset includes more than **2.5M images** representing nearly **65,000 species** from the Americas and Oceania that have been aligned to a standardized plant list ([LCVP v1.0.2](https://www.nature.com/articles/s41597-020-00702-z)). 

This dataset has a long tail; there are a minimum of 3 images per species. However, some species can be represented by more than 100 images. This dataset only includes vascular land plants which include lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide almost all of our crops, vegetables, and fruits.

The teams with the most accurate models will be contacted with the intention of using them on the unnamed plant collections in the NYBG herbarium and then be assessed by the NYBG plant specialists for accuracy.

[![Herbarium2021.png](https://i.postimg.cc/htpxH99f/Herbarium2021.png)](https://postimg.cc/BjPXFP70)

# Background

There are approximately 3,000 herbaria world-wide, and they are massive repositories of plant diversity data. These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time. The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist. All specimens not only maintain their morphological features but also include collection dates and locations, their reproductive state, and the name of the person who collected the specimen. This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time. The models developed during this competition are an integral first step to speed the pace of species discovery and save the plants of the world.

There are approximately 400,000 known vascular plant species with an estimated 80,000 still to be discovered. Herbaria contain an overwhelming amount of unnamed and new specimens, and with the threats of climate change, we need new tools to quicken the pace of species discovery. This is more pressing today as a United Nations report indicates that more than one million species are at risk of extinction, and amid this dire prediction is a recent estimate that suggests plants are disappearing more quickly than animals. This year, we have expanded our curated herbarium dataset to vascular plant diversity in the Americas and Oceania. 

The most accurate models will be used on the unidentified plant specimens in our herbarium and assessed by our taxonomists thereby producing a tool to quicken the pace of species discovery.

# About

This is an FGVC competition hosted as part of the [FGVC8](https://sites.google.com/view/fgvc8) workshop at [CVPR 2021](http://cvpr2021.thecvf.com/) and sponsored by [NYBG](https://www.nybg.org/).

Details of this competition are mirrored on the [github](https://github.com/visipedia/herbarium_comp) page. Please post in the forum or open an issue if you have any questions or problems with the dataset.

# Acknowledgements

The images are provided by the [New York Botanical Garden](https://www.nybg.org/), [Bishop Museum](https://www.bishopmuseum.org/), [Naturalis Biodiversity Center](https://www.naturalis.nl/en), [Queensland Herbarium](https://www.qld.gov.au/environment/plants-animals/plants/herbarium), and [Auckland War Memorial Museum](https://www.aucklandmuseum.com/).

[![Logos.png](https://i.postimg.cc/fbSLBX54/Logos.png)](https://postimg.cc/xkYndXWg)","Submissions are evaluated using the [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).

The F1 score is given by

$$
F_1 = 2\frac{precision \cdot recall}{precision+recall}
$$

where:

$$
precision = \frac{TP}{TP+FP},
$$

$$
recall = \frac{TP}{TP+FN}.
$$

In ""macro"" F1 a separate F1 score is calculated for each `species` value and then averaged.
#Submission Format
For each image `Id`, you should predict the corresponding image label (`category_id`) in the `Predicted` column. The submission file should have the following format:
<pre>Id,Predicted<br>0,1<br>1,27<br>2,42<br>...</pre>"
Shopee - Price Match Guarantee,Determine if two products are the same by their images,https://www.kaggle.com/competitions/shopee-product-matching,https://storage.googleapis.com/kaggle-competitions/kaggle/24286/logos/header.png?t=2021-01-07-16-57-37,"Image,Text,Retail and Shopping",2426,3032,51077,"Do you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.

Two different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.

Shopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products. 

In this competition, you’ll apply your machine learning skills to build a model that predicts which items are the same products. 

The applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals.
","Submissions will be evaluated based on their mean [F1 score](https://en.wikipedia.org/wiki/F1_score). The mean is calculated in a sample-wise fashion, meaning that an F1 score is calculated for every predicted row, then averaged.

## Submission File
You must create a space-delimited list of all `posting_id`s that match the posting in the `posting_id` column. _Posts always self-match_. Group sizes were capped at 50, so there is no benefit to predict more than 50 matches.

The file should have a header, be named `submission.csv`, and look like the following:
```
posting_id,matches
test_123,test_123
test_456,test_456 test_789
```
You should predict matches for every `posting_id`. For example, if you believe A matches B and C, `A,A B C`, you would also include `B,B A C` and `C,C A B`."
Bristol-Myers Squibb – Molecular Translation,Can you translate chemical images to text?,https://www.kaggle.com/competitions/bms-molecular-translation,https://storage.googleapis.com/kaggle-competitions/kaggle/22422/logos/header.png?t=2021-02-03-02-05-31,"Chemistry,Image",874,1171,10237,"In a technology-forward world, sometimes the best and easiest tools are still pen and paper. Organic chemists frequently draw out molecular work with the Skeletal formula, a structural notation used for centuries. Recent publications are also annotated with machine-readable chemical descriptions (InChI), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions. Automated recognition of optical chemical structures, with the help of machine learning, could speed up research and development efforts.

Unfortunately, most public data sets are too small to support modern machine learning models. Existing tools produce 90% accuracy but only under optimal conditions. Historical sources often have some level of image corruption, which reduces performance to near zero. In these cases, time-consuming, manual work is required to reliably convert scanned chemical structure images into a machine-readable format.

Bristol-Myers Squibb is a global biopharmaceutical company working to transform patients' lives through science. Their mission is to discover, develop, and deliver innovative medicines that help patients prevail over serious diseases.

In this competition, you’ll interpret old chemical images. With access to a large set of synthetic image data generated by Bristol-Myers Squibb, you'll convert images back to the underlying chemical structure annotated as InChI text.

Tools to curate chemistry literature would be a significant benefit to researchers. If successful, you'll help chemists expand access to collective chemical research. In turn, this would speed up research and development efforts in many key fields by avoiding repetition of previously published chemistries and identifying novel trends via mining large data sets.

Photo by Terry Vlisidis on Unsplash

","Submissions are evaluated on the mean [Levenshtein distance](http://en.wikipedia.org/wiki/Levenshtein_distance) between the InChi strings you submit and the ground truth InChi values.

## Submission File
For each `image_id` in the test set, you must predict the InChi string of the molecule in the corresponding image. The file should contain a header and have the following format:

    image_id,InChI
    00000d2a601c,InChI=1S/H2O/h1H2
    00001f7fc849,InChI=1S/H2O/h1H2
    000037687605,InChI=1S/H2O/h1H2
    etc.
"
Tabular Playground Series - Mar 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-mar-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/25226/logos/header.png?t=2021-01-27-17-34-31,"Tabular,Logistic Regression",1495,1568,12945,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset.  These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

### Getting Started

Check out this [Starter Notebook](https://www.kaggle.com/inversion/get-started-mar-tabular-playground-competition) which walks you through how to make your very first submission!

For more ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each row in the test set, you must predict the probability of a binary target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:

    id,target
    5,0.5
    6,0.1
    8,0.9
    etc."
March Machine Learning Mania 2021 - NCAAM - Spread,Predict the margin of victory in the 2021 men's tournament,https://www.kaggle.com/competitions/ncaam-march-mania-2021-spread,https://storage.googleapis.com/kaggle-competitions/kaggle/26082/logos/header.png?t=2021-02-24-16-35-44,"Basketball,Sports",96,111,163,"In addition to the predictive modeling competitions we typically host (<a href=""https://www.kaggle.com/c/ncaaw-march-mania-2021/overview"">NCAA Women's</a> and <a href=""https://www.kaggle.com/c/ncaam-march-mania-2021/overview"">Men’s</a>), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls?

<p>This competition (and the parallel competition for the women's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt="""" width=""600"" height=""282""></p>
<p>In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.</p>

*Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year!*

<h3>Acknowledgments</h3>
Markus Spiske on Unsplash and The Noun Project
","<p>Submissions are evaluated on <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" target=""_blank"">Root-Mean-Squared-Error (RMSE)</a> between the predicted point spread and the observed spread.

<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2021 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2 &nbsp;= 2,016 matchups.</p>

<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2015_3106_3107"" indicates team 3106 played team 3107 in the year 2015. You must predict the difference in score between both teams, relative to the team with the lower id. For example, if you believe team 3106 will beat 3107 by 7 points, your prediction would be 7. if you believe team 3106 will lose to 3107 by 3 points, your prediction will be -3.</p>

<p>The resulting submission format looks like the following, where `Pred` represents the difference in score between the lower id team and the higher id team:</p>
<pre>ID,Pred<br>2015_3106_3107,7<br>2015_3106_3110,-1<br>2015_3106_3113,-9<br>...</pre>"
March Machine Learning Mania 2021 - NCAAW - Spread,Predict the margin of victory in the 2021 women's tournament,https://www.kaggle.com/competitions/ncaaw-march-mania-2021-spread,https://storage.googleapis.com/kaggle-competitions/kaggle/26082/logos/header.png?t=2021-02-24-16-35-44,"Basketball,Sports",70,77,116,"In addition to the predictive modeling competitions we typically host (<a href=""https://www.kaggle.com/c/ncaaw-march-mania-2021/overview"">NCAA Women's</a> and <a href=""https://www.kaggle.com/c/ncaam-march-mania-2021/overview"">Men’s</a>), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls?

<p>This competition (and the parallel competition for the men's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt="""" width=""600"" height=""282""></p>
<p>In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.</p>

*Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year!*

<h3>Acknowledgments</h3>
Markus Spiske on Unsplash and The Noun Project
","<p>Submissions are evaluated on <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" target=""_blank"">Root-Mean-Squared-Error (RMSE)</a> between the predicted point spread and the observed spread.

<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2021 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2 &nbsp;= 2,016 matchups.</p>

<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2015_3106_3107"" indicates team 3106 played team 3107 in the year 2015. You must predict the difference in score between both teams, relative to the team with the lower id. For example, if you believe team 3106 will beat 3107 by 7 points, your prediction would be 7. if you believe team 3106 will lose to 3107 by 3 points, your prediction will be -3.</p>

The resulting submission format looks like the following, where `Pred` represents the difference in score between the lower id team and the higher id team:
<pre>ID,Pred<br>2015_3106_3107,7<br>2015_3106_3110,-1<br>2015_3106_3113,-9<br>...</pre>"
March Machine Learning Mania 2021 - NCAAW,Predict the 2021 NCAAW Basketball Tournament,https://www.kaggle.com/competitions/ncaaw-march-mania-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/26080/logos/header.png?t=2021-02-24-01-37-37,"Basketball,Sports",451,508,836,"<p>Back again after a pandemic-year hiatus, Kaggle's March Machine Learning Mania challenges data scientists to predict winners and losers of the women's 2021 NCAA basketball tournament. You're provided data  of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt="""" width=""600"" height=""282""></p>
<p>In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.</p>

*Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for its 8th year!*

<h3>Acknowledgments</h3>
Banner image by Ben Hershey on Unsplash

","Submissions are scored on the log loss:

$$
\textrm{LogLoss} = - \frac{1}{n} \sum\_{i=1}^n \left[ y\_i \log(\hat{y}\_i) + (1 - y\_i) \log(1 - \hat{y}\_i) \right],
$$
where

 - \\( n \\) is the number of games played
 - \\( \hat{y}\_i \\) is the predicted probability of team 1 beating team 2
 - \\( y\_i  \\) is 1 if team 1 wins, 0 if team 2 wins
 - \\( log \\) is the natural logarithm

The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.

## Submission File

The file you submit will depend on whether the competition is in stage 1--historical model building--or stage 2--the 2021 tournament. Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict \\( (64*63)/2 = 2,016\\) matchups.

Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2015\_3106\_3107"" indicates team 3106 played team 3107 in the year 2015. You must predict the probability that the team with the lower id beats the team with the higher id.

The resulting submission format looks like the following, where `Pred` represents the predicted probability that the first team will win:

    ID,Pred
    2015_3106_3107,0.5
    2015_3106_3110,0.5
    2015_3106_3113,0.5
    ..."
March Machine Learning Mania 2021 - NCAAM,Predict the 2021 NCAAM Basketball Tournament,https://www.kaggle.com/competitions/ncaam-march-mania-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/26080/logos/header.png?t=2021-02-24-01-37-37,"Basketball,Sports",707,791,1242,"<p>Back again after a pandemic-year hiatus, Kaggle's March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2021 NCAA basketball tournament. You're provided data  of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt="""" width=""600"" height=""282""></p>
<p>In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.</p>

*Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for its 8th year!*

<h3>Acknowledgments</h3>
Banner image by Ben Hershey on Unsplash

","Submissions are scored on the log loss:

$$
\textrm{LogLoss} = - \frac{1}{n} \sum\_{i=1}^n \left[ y\_i \log(\hat{y}\_i) + (1 - y\_i) \log(1 - \hat{y}\_i) \right],
$$
where

 - \\( n \\) is the number of games played
 - \\( \hat{y}\_i \\) is the predicted probability of team 1 beating team 2
 - \\( y\_i  \\) is 1 if team 1 wins, 0 if team 2 wins
 - \\( log \\) is the natural logarithm

The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.

## Submission File

The file you submit will depend on whether the competition is in stage 1--historical model building--or stage 2--the 2021 tournament. Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams, you will predict \\( (68*67)/2  = 2,278\\) matchups.

Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2016\_1107\_1110"" indicates team 1107 potentially played team 1110 in the year 2016. You must predict the probability that the team with the lower id beats the team with the higher id.

The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:

    ID,Pred
    2015_1107_1110,0.5
    2015_1107_1112,0.5
    2015_1107_1113,0.5
"
Hash Code 2021 - Traffic Signaling,Optimize city traffic in this extension of the 2021 Hash Code qualifier,https://www.kaggle.com/competitions/hashcode-2021-oqr-extension,https://storage.googleapis.com/kaggle-competitions/kaggle/25951/logos/header.png?t=2021-02-18-20-26-18,"Optimization,Automobiles and Vehicles",179,189,825,"This is an extended version of the [Hash Code](https://codingcompetitions.withgoogle.com/hashcode/) Online Qualifications 2021 problem. After Hash Code's official Online Qualifications, which lasts only 4 hours, you can improve your submissions here on Kaggle, optimize them in more detail, and discuss your approaches with other teams. We created a new data set just for this Kaggle competition that was not used in Hash Code before. Please find the full problem statement as a PDF file under the data tab.

*Note that this is not Hash Code's official Online Qualifications and that there are no prizes for this competition. You can't qualify for Hash Code's Finals on Kaggle.*

**Problem statement** 

The world's first traffic light dates back to 1868. It was installed in London to control traffic for... *horse-drawn vehicles*! Today, traffic lights can be found at street intersections in almost every city in the world, making it safer for vehicles to go through them.

Traffic lights have at least two states- and use one color (usually red) to signal ""stop""- and another (usually green) to signal that cars can proceed through. The very first traffic lights were manually controlled. Nowadays they are automatic, meaning that they have to be carefully designed and timed in order to optimize the overall travel time for all the participants in traffic.

Given the description of a city plan and planned paths for all cars in that city, you will be optimizing the schedule of traffic lights to minimize the total amount of time spent in traffic, and help as many cars as possible reach their destination before a given deadline. 

Photo by Eliobed Suarez on Unsplash","To make a submission, you will need to attach your answer file to a Kaggle notebook (or generate it within one) and export the file to the notebook's working directory. You can then submit from the committed notebook's output section. The answer file needs to be labeled submission.csv as a convention for our backend, though it is most definitely not actually a csv file.

---

For each car that finishes its route before the end of the simulation in the sample solution, the solution scores the bonus points (F) + additional points for being early: if the car finishes its route at time T it scores additional D-T points. In other words: if a car finishes at time T it scores
- F + (D - T) points if T ≤ D
- or 0 points otherwise.

The score for the solution is the sum of scores for all cars.

**Example**
For instance, in the example from the problem statement, *the first car*:
- T = 0s: crosses immediately intersection 0, as the traffic light there is always green
- T = 1s: one second later, it has gone through b-street. However, the traffic light is red (as for intersection 1, the submission has set the duration for c-street's light to be green for 2 seconds).
- T = 2s: the car now crosses the intersection and continues to e-street
- T = 5s: the car has reached the end of e-street, finds a green light at intersection 2, crosses it and continues to d-street.
This first car would have reached the end of d-street at T = 7s, but this is past the deadline of the  run (defined in the input data set), meaning that 0 points are assigned to this car.

*The second car*:
- T = 0s: finds a green light at intersection 1 and continues to e-street.
- T = 3s: reaches the end of e-street, finds a green light at intersection 2 and no cars waiting, so it immediately crosses the intersection and heads towards a-street.
- T= 4s: the car reaches the end of a-street, which is its destination.
So the second car finishes before the deadline, and gets a score of 1000 + (6 - 4) = 1002 points.

The final score for this submission is 1002.
"
Tabular Playground Series - Feb 2021,Practice your ML skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-feb-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/25225/logos/header.png?t=2021-01-27-17-34-26,"Regression,Tabular",1433,1474,12553,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.

Good luck and have fun!

### Getting Started

Check out this [Starter Notebook](https://www.kaggle.com/inversion/get-started-feb-tabular-playground-competition) which walks you through how to make your very first submission!

For more ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are scored on the **root mean squared error**. RMSE is defined as:

$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2} $$

where \\( \hat{y} \\) is the predicted value, \\( y \\) is the original value, and \\( n \\) is the number of rows in the test data.

## Submission File
For each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:

    id,target
    0,0.5
    5,10.2
    15,2.2
    etc."
Indoor Location & Navigation,Identify the position of a smartphone in a shopping mall,https://www.kaggle.com/competitions/indoor-location-navigation,https://storage.googleapis.com/kaggle-competitions/kaggle/22559/logos/header.png?t=2020-09-30-17-40-59,"Signal Processing,Text Mining",1170,1446,28009,"Your smartphone goes everywhere with you—whether driving to the grocery store or shopping for holiday gifts. With your permission, apps can use your location to provide contextual information. You might get driving directions, find a store, or receive alerts for nearby promotions. These handy features are enabled by GPS, which requires outdoor exposure for the best accuracy. Yet, there are many times when you’re inside large structures, such as a shopping mall or event center. Accurate indoor positioning, based on public sensors and user permission, allows for a great location-based experience even when you aren’t outside.

Current positioning solutions have poor accuracy, particularly in multi-level buildings, or generalize poorly to small datasets. Additionally, GPS was built for a time before smartphones. Today’s use cases often require more granularity than is typically available indoors.

In this competition, your task is to predict the indoor position of smartphones based on real-time sensor data, provided by indoor positioning technology company XYZ10 in partnership with Microsoft Research. You'll locate devices using “active” localization data, which is made available with the cooperation of the user. Unlike passive localization methods (e.g. radar, camera), the data provided for this competition requires explicit user permission. You'll work with a dataset of nearly 30,000 traces from over 200 buildings.

If successful, you’ll contribute to research with broad-reaching possibilities, including industries like manufacturing, retail, and autonomous devices. With more accurate positioning, existing location-based apps could even be improved. Perhaps you’ll even see the benefits yourself the next time you hit the mall.

### Acknowledgments

XYZ10 is a rising indoor positioning technology company in China. Since 2017, XYZ10 has been accumulating a privacy-sensitive indoor location dataset of WiFi, geomagnetic, and Bluetooth signatures with ground truths from nearly 1,000 buildings.

Microsoft Research is the research subsidiary of Microsoft. Its goal is to advance state-of-the-art computing and solve difficult world research-motivated competition problems through technological innovation in collaboration with academic, government, and industry researchers.
","Submissions are evaluated on the `mean position error` as defined as:

$$
\text{mean position error} = \frac{1}{N} \sum_{i=1}^{N} 
                                                \left( \sqrt{\( \hat{x}\_i - x\_i \)^{2} + \( \hat{y}\_i - y\_i \)^{2}}
                                                + p \cdot | \hat{f}\_{i} - f\_i | \right)
$$

where:
 - \\( N \\) is the number of rows in the test set
 - \\( \hat{x}\_i, \hat{y}\_i \\) are the predicted locations for a given test row
 - \\( x\_i, y\_i \\) are the ground truth locations for a given test row
 - \\( p \\) is the floor penalty, set at 15
 - \\( \hat{f}\_{i}, f\_{i} \\) are the predicted and ground truth integer floor level for a given test row

**IMPORTANT:** The integer `floor` used in the submission must be mapped from the char/int floors used in the dataset. The mapping is as follows:
- F1, 1F \\( \rightarrow \\) 0
- F2, 2F \\( \rightarrow \\) 1
- etc.
- B1, 1B \\( \rightarrow \\) -1
- B2, 2B \\( \rightarrow \\) -2

There are other floor names in the training data, e.g., LG2, LM, etc., which you may decide to use for training, but none of these non-standard floors are found in the test set.

## Submission File
For each `site_path_timestamp` row in the test set, you must predict the floor converted to an integer as per above and the `x` and `y` of the waypoint. The file should contain a header and have the following format:

    site_path_timestamp,floor,x,y
    5a0546857ecc773753327266_046cfa46be49fc10834815c6_1578474564146,0,15.0,55.0
    5a0546857ecc773753327266_046cfa46be49fc10834815c6_1578474573154,0,25.0,65.0
    5a0546857ecc773753327266_046cfa46be49fc10834815c6_1578474579463,0,35.0,75.0
    etc.
"
Human Protein Atlas - Single Cell Classification,Find individual human cell differences in microscope images,https://www.kaggle.com/competitions/hpa-single-cell-image-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/23823/logos/header.png?t=2020-11-24-14-18-10,"Image,Multiclass Classification,Multilabel Classification",757,991,19058,"There are billions of humans on this earth, and each of us is made up of trillions of cells. Just like every individual is unique, even genetically identical twins, scientists observe differences between the genetically identical cells in our bodies.

Differences in the location of proteins can give rise to such cellular heterogeneity. Proteins play essential roles in virtually all cellular processes. Often, many different proteins come together at a specific location to perform a task, and the exact outcome of this task depends on which proteins are present. As you can imagine, different subcellular distributions of one protein can give rise to great functional heterogeneity between cells. Finding such differences, and figuring out how and why they occur, is important for understanding how cells function, how diseases develop, and ultimately how to develop better treatments for those diseases.

To see more, start with less. That may seem counterintuitive, but the study of a single cell enables the discovery of mechanisms too difficult to see with multi-cell research. The importance of studying single cells is reflected in the ongoing revolution in biology centered around technologies for single cell analysis. Microscopy offers an opportunity to study differences in protein localizations within a population of cells. Current machine learning models for classifying protein localization patterns in microscope images gives a summary of the entire population of cells. However, the single-cell revolution in biology demands models that can precisely classify patterns in each individual cell in the image.

The Human Protein Atlas is an initiative based in Sweden that is aimed at mapping proteins in all human cells, tissues, and organs. The data in the [Human Protein Atlas database](https://www.proteinatlas.org/) is freely accessible to scientists all around the world that allows them to explore the cellular makeup of the human body. Solving the single-cell image classification challenge will help us characterize single-cell heterogeneity in our large collection of images by generating more accurate annotations of the subcellular localizations for thousands of human proteins in individual cells. Thanks to you, we will be able to more accurately model the spatial organization of the human cell and provide new open-access cellular data to the scientific community, which may accelerate our growing understanding of how human cells functions and how diseases develop.

This is a weakly supervised multi-label classification problem and a code competition. Given images of cells from our microscopes and labels of protein location assigned together for all cells in the image, Kagglers will develop models capable of segmenting and classifying each individual cell with precise labels. If successful, you'll contribute to the revolution of single-cell biology! 

The scientific journal [Nature Methods] (https://www.nature.com/nmeth/) is interested in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team, led by Professor Emma Lundberg, would like to invite top performing teams to join as co-authors in writing this paper. Please follow the discussion forum for more details on how you can help.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/hpa-single-cell-image-classification/overview/code-requirements) for details.**

","Submissions are evaluated by computing [mAP], with the mean taken over the [19 segmentable classes of the challenge](https://www.kaggle.com/c/hpa-single-cell-image-classification/data). It is identical to the [OpenImages Instance Segmentation Challenge evaluation](https://www.kaggle.com/c/open-images-2019-instance-segmentation/overview/evaluation). The OpenImages version of the metric is [described in detail here](https://storage.googleapis.com/openimages/web/evaluation.html#instance_segmentation_eval). See also [this tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md#instance-segmentation-track) on running the evaluation in Python.


Segmentation is calculated using IoU with a threshold of `0.6`.

## Submission File

For each image in the test set, you must predict a list of instance segmentation masks and their associated detection score (`Confidence`). The submission csv file uses the following format:

<pre><code>ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2 ...
ImageBID,ImageBWidth,ImageBHeight,LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 …
</code></pre>

Note that a mask MAY have more than one class. If that is the case, predict separate detections for each class using the same mask.

<pre><code>ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA1 ...
</code></pre>

A sample with real values would be:
<pre><code>ID,ImageWidth,ImageHeight,PredictionString
721568e01a744247,1118,1600,0 0.637833 eNqLi8xJM7BOTjS08DT2NfI38DfyM/Q3NMAJgJJ+RkBs7JecF5tnAADw+Q9I
7b018c5e3a20daba,1600,1066,16 0.85117 eNqLiYrLN7DNCjDMMIj0N/Iz9DcwBEIDfyN/QyA2AAsBRfxMPcKTA1MMADVADIo=
</code></pre>


The binary segmentation masks are [run-length encoded](https://en.wikipedia.org/wiki/Run-length_encoding) (RLE), [zlib](https://en.wikipedia.org/wiki/Zlib) compressed, and [base64](https://en.wikipedia.org/wiki/Base64) encoded to be used in text format as `EncodedMask`. Specifically, we use the Coco masks RLE encoding/decoding (see the `encode` method of [COCO’s mask API](http://cocodataset.org/#download)), the zlib compression/decompression ([RFC1950](https://www.ietf.org/rfc/rfc1950.txt)), and vanilla base64 encoding.
  
An example python function to encode an instance segmentation mask would be:

<pre><code>import base64
import numpy as np
from pycocotools import _mask as coco_mask
import typing as t
import zlib


def encode_binary_mask(mask: np.ndarray) -> t.Text:
  """"""Converts a binary mask into OID challenge encoding ascii text.""""""
  
  # check input mask --
  if mask.dtype != np.bool:
    raise ValueError(
        ""encode_binary_mask expects a binary mask, received dtype == %s"" %
        mask.dtype)
  
  mask = np.squeeze(mask)
  if len(mask.shape) != 2:
    raise ValueError(
        ""encode_binary_mask expects a 2d mask, received shape == %s"" %
        mask.shape)
   
  # convert input mask to expected COCO API input --
  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
  mask_to_encode = mask_to_encode.astype(np.uint8)
  mask_to_encode = np.asfortranarray(mask_to_encode)
  
  # RLE encode mask --
  encoded_mask = coco_mask.encode(mask_to_encode)[0][""counts""]
  
  # compress and base64 encoding --
  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
  base64_str = base64.b64encode(binary_str)
  return base64_str
</code></pre>

(This code is [available as a gist here](https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c).)"
Tabular Playground Series - Jan 2021,Practice your ML regression skills on this approachable dataset!,https://www.kaggle.com/competitions/tabular-playground-series-jan-2021,https://storage.googleapis.com/kaggle-competitions/kaggle/24673/logos/header.png?t=2021-01-02-00-34-25,"Tabular,Regression",1728,1728,16104,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions, and thus more beginner-friendly. 

In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching a month-long tabular Playground competition on the 1st of every month, and continue the experiment as long as there's sufficient interest and participation.

The goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.

For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. 

Good luck and have fun!

### Getting Started

Check out this [Starter Notebook](https://www.kaggle.com/inversion/get-started-jan-tabular-playground-competition) which walks you through how to make your very first submission!

For more ideas on how to improve your score, check out the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) courses on Kaggle Learn.","Submissions are scored on the **root mean squared error**. RMSE is defined as:

$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2} $$

where \\( \hat{y} \\) is the predicted value, \\( y \\) is the original value, and \\( n \\) is the number of rows in the test data.

## Submission File
For each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:

    id,target
    0,0.5
    2,10.2
    6,2.2
    etc."
VinBigData Chest X-ray Abnormalities Detection,Automatically localize and classify thoracic abnormalities from chest radiographs,https://www.kaggle.com/competitions/vinbigdata-chest-xray-abnormalities-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/24800/logos/header.png?t=2020-12-17-19-26-15,"Image,Healthcare",1275,1697,29249,"When you have a broken arm, radiologists help save the day—and the bone. These doctors diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.

Existing methods of interpreting chest X-ray images classify them into a list of findings. There is currently no specification of their locations on the image which sometimes leads to inexplicable results. A solution for localizing findings on chest X-ray images is needed for providing doctors with more meaningful diagnostic assistance.

Established in August 2018 and funded by the Vingroup JSC, the Vingroup Big Data Institute (VinBigData) aims to promote fundamental research and investigate novel and highly-applicable technologies. The Institute focuses on key fields of data science and artificial intelligence: computational biomedicine, natural language processing, computer vision, and medical image processing. The medical imaging team at VinBigData conducts research in collecting, processing, analyzing, and understanding medical data. They're working to build large-scale and high-precision medical imaging solutions based on the latest advancements in artificial intelligence to facilitate effective clinical workflows.

In this competition, you’ll automatically localize and classify 14 types of thoracic abnormalities from chest radiographs. You'll work with a dataset consisting of 18,000 scans that have been annotated by experienced radiologists. You can train your model with 15,000 independently-labeled images and will be evaluated on a test set of 3,000 images. These annotations were collected via VinBigData's web-based platform, [VinLab](https://vindr.ai/vinlab). Details on building the dataset can be found in our recent paper [“VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations”](https://arxiv.org/pdf/2012.15029.pdf).

If successful, you'll help build what could be a valuable second opinion for radiologists. An automated system that could accurately identify and localize findings on chest radiographs would relieve the stress of busy doctors while also providing patients with a more accurate diagnosis.

###Acknowledgments

**Challenge Organizing Team**
* Ha Q. Nguyen, PhD - Vingroup Big Data Institute 
* Hieu H. Pham, PhD - Vingroup Big Data Institute
* Nhan T. Nguyen, MSc - Vingroup Big Data Institute
* Dung B. Nguyen, BSc - Vingroup Big Data Institute
* Minh Dao, PhD - Vingroup Big Data Institute
* Van Vu, PhD - Vingroup Big Data Institute
* Khanh Lam, MD, PhD - Hospital 108
* Linh T. Le, MD, PhD - Hanoi Medical University Hospital

**Data Contributors**
The dataset used in this competition was created by assembling de-identified Chest X-ray studies provided by two hospitals in Vietnam: the Hospital 108 and the Hanoi Medical University Hospital. 
","The challenge uses the standard PASCAL VOC 2010 [mean Average Precision (mAP) ](http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf)at IoU > 0.4.

## Submission File
Images in the test set may contain more than one object. For each object in a given test image, you must predict a class ID, `confidence` score, and bounding box in format `xmin ymin xmax ymax`. If you predict that there are NO objects in a given image, you should predict `14 1.0 0 0 1 1`, where `14` is the class ID for ""No finding"", 1.0 is the confidence, and `0 0 1 1` is a one-pixel bounding box.

The submission file should contain a header and have the following format:

    ID,TARGET
    004f33259ee4aef671c2b95d54e4be68,14 1 0 0 1 1
    004f33259ee4aef671c2b95d54e4be69,11 0.5 100 100 200 200 13 0.7 10 10 20 20
    etc.
"
RANZCR CLiP - Catheter and Line Position Challenge,Classify the presence and correct placement of tubes on chest x-rays to save lives,https://www.kaggle.com/competitions/ranzcr-clip-catheter-line-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/23870/logos/header.png?t=2020-12-01-04-28-05,"Image,Multilabel Classification",1547,1854,28112,"Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity.     

Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5].  The likelihood of complication is directly related to both the experience level and specialty of the proceduralist.  Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines.

The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications.

The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught.

In this competition, you’ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed.

The dataset has been labelled with a set of definitions to ensure consistency with labelling. The **normal** category includes lines that were appropriately positioned and did not require repositioning. The **borderline** category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position.  The **abnormal** category included lines that required immediate repositioning.

If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines  is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/overview/code-requirements) for details.**

1. Koopmann MC, Kudsk KA, Szotkowski MJ, Rees SM. A Team-Based Protocol and Electromagnetic Technology Eliminate Feeding Tube Placement Complications [Internet]. Vol. 253, Annals of Surgery. 2011. p. 297–302. Available from: http://dx.doi.org/10.1097/sla.0b013e318208f550
2. Sorokin R, Gottlieb JE. Enhancing patient safety during feeding-tube insertion: a review of more than 2,000 insertions. JPEN J Parenter Enteral Nutr. 2006 Sep;30(5):440–5.
3. Marderstein EL, Simmons RL, Ochoa JB. Patient safety: effect of institutional protocols on adverse events related to feeding tube placement in the critically ill. J Am Coll Surg. 2004 Jul;199(1):39–47; discussion 47–50.
4. Jemmett ME. Unrecognized Misplacement of Endotracheal Tubes in a Mixed Urban to Rural Emergency Medical Services Setting [Internet]. Vol. 10, Academic Emergency Medicine. 2003. p. 961–5. Available from: http://dx.doi.org/10.1197/s1069-6563(03)00315-4
5. Lotano R, Gerber D, Aseron C, Santarelli R, Pratter M. Utility of postintubation chest radiographs in the intensive care unit. Crit Care. 2000 Jan 24;4(1):50–3.
","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

To calculate the final score, AUC is calculated for each of the 11 labels, then averaged. The score is then the average of the individual AUCs of each predicted column.


## Submission File
For each ID in the test set, you must predict a probability for all target variables. The file should contain a header and have the following format:

> StudyInstanceUID,ETT - Abnormal,ETT - Borderline,ETT - Normal,NGT - Abnormal,NGT - Borderline,NGT - Incompletely Imaged,NGT - Normal,CVC - Abnormal,CVC - Borderline,CVC - Normal,Swan Ganz Catheter Present
1.2.826.0.1.3680043.8.498.62451881164053375557257228990443168843,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.83721761279899623084220697845011427274,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.12732270010839808189235995393981377825,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.11769539755086084996287023095028033598,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.87838627504097587943394933987052577153,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.53211840524738036417560823327351887819,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.93555795394184819372299157360228027866,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.52241894131170494723503100795076463919,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.36500167484503936720548852591033878284,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.86199852603457900780565655267977637728,0,0,0,0,0,0,0,0,0,0,0
"
Cassava Leaf Disease Classification,Identify the type of disease present on a Cassava Leaf image,https://www.kaggle.com/competitions/cassava-leaf-disease-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/13836/logos/header.png?t=2020-10-01-17-22-54,"Image,Multiclass Classification,Plants",3900,4792,81524,"As the second-largest provider of carbohydrates in Africa, cassava is a key food security crop grown by smallholder farmers because it can withstand harsh conditions. At least 80% of household farms in Sub-Saharan Africa grow this starchy root, but viral diseases are major sources of poor yields. With the help of data science, it may be possible to identify common diseases so they can be treated.

Existing methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants. This suffers from being labor-intensive, low-supply and costly. As an added challenge, effective solutions for farmers must perform well under significant constraints, since African farmers may only have access to mobile-quality cameras with low-bandwidth.

In this competition, we introduce a dataset of 21,367 labeled images collected during a regular survey in Uganda. Most images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala. This is in a format that most realistically represents what farmers would need to diagnose in real life.

Your task is to classify each cassava image into four disease categories or a fifth category indicating a healthy leaf. With your help, farmers may be able to quickly identify diseased plants, potentially saving their crops before they inflict irreparable damage.

<div class=""note"">
<strong>Recommended Tutorial</strong><br>
We highly recommend <a href=https://www.kaggle.com/jessemostipak/getting-started-tpus-cassava-leaf-disease>Jesse Mostipak’s Getting Started Tutorial</a> that walks you through making your very first submission step by step.
</div>

## Acknowledgements

The **Makerere Artificial Intelligence (AI) Lab** is an AI and Data Science research group based at Makerere University in Uganda. The lab specializes in the application of artificial intelligence and data science - including for example, methods from machine learning, computer vision and predictive analytics to problems in the developing world. Their mission is: “To advance Artificial Intelligence research to solve real-world challenges.""

We thank the different experts and collaborators from **National Crops Resources Research Institute (NaCRRI)** for assisting in preparing this dataset.

<br>
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/cassava-leaf-disease-classification/overview/code-requirements) for details.**","##Evaluation

Submissions will be evaluated based on their [categorization accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy).

## Submission Format

The submission format for the competition is a csv file with the following format:

    image_id,label
    1000471002.jpg,4
    1000840542.jpg,4
    etc."
Rainforest Connection Species Audio Detection,Automate the detection of bird and frog species in a tropical soundscape,https://www.kaggle.com/competitions/rfcx-species-audio-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/21669/logos/header.png?t=2020-10-28-04-28-01,"Audio,TensorFlow,Animals",1143,1385,33339,"<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2F0fea48cf0c653ac827ccfb498a4c9e9c%2Frfcx-frog.jpg?generation=1603859020476496&alt=media"" style=""float: right; width: 200px"">

Who doesn't enjoy the morning chirp of a bird or a frog’s evening croak? Animals bring more than sweet songs and natural ambience to the world. The presence of rainforest species is a good indicator of the impact of climate change and habitat loss. As it's easier to hear these species than see them, it’s important to use acoustic technologies that can work on a global scale. Real-time information, such as provided through machine learning techniques, could enable early-stage detection of human impacts on the environment. This result could drive more effective conservation management decisions.

Traditional methods of assessing the diversity and abundance of species are costly and limited in space and time. And while automatic acoustic identification via deep learning has been successful, models require a large number of training samples per species. This limits applicability to rarer species, which are central to conservation efforts. Thus, methods to automate high-accuracy species detection in noisy soundscapes with limited training data are the solution.

Rainforest Connection (RFCx) created the world’s first scalable, real-time monitoring system for protecting and studying remote ecosystems. Unlike visual-based tracking systems like drones or satellites, RFCx relies on acoustic sensors that monitor the ecosystem soundscape at selected locations year round. RFCx technology has advanced to support a comprehensive biodiversity monitoring program that allows local partners to measure progress of wildlife restoration and recovery through principles of adaptive management. The RFCx monitoring platform also has the capacity to create convolutional neural network (CNN) models for analysis.

In this competition, you’ll automate the detection of bird and frog species in tropical soundscape recordings. You'll create your models with limited, acoustically complex training data. Rich in more than bird and frog noises, expect to hear an insect or two, which your model will need to filter out.

If successful, you'll have a hand in a rapidly expanding field of science: the development of automated eco-acoustic monitoring systems. The resulting real-time information could enable earlier detection of human environmental impacts, making environmental conservation more swift and effective.
","The task consists of predicting the species present in each test audio file. Some test audio files contain a single species while others contain multiple. The predictions are to be done at the audio file level, i.e., no start/end timestamps are required. 

The competition metric is the **label-weighted** [**label-ranking average precision**](https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision), which is a generalization of the mean reciprocal rank measure for the case where there can be multiple true labels per test item.

The ""label-weighted"" part means that the overall score is the average over all the *labels* in the test set, where each label receives equal weight (by contrast, plain *lrap* gives each *test observation* equal weight, thereby discounting the contribution of individual labels when when an observation has multiple labels). In other words, each test observation is weighted by the number of ground truth labels found in the observation.

## Submission File
For each `recording_id` in the test set, you must predict the probability of *each* species label being found in the audio sample. The file should contain a header (each species number with an `s` prefix) and have the following format:

    recording_id,s0,...,s23
    000316da7,0.1,....,0.3
    003bc2cb2,0.0,...,0.8
    etc."
HuBMAP - Hacking the Kidney,Identify glomeruli in human kidney tissue images,https://www.kaggle.com/competitions/hubmap-kidney-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/22990/logos/header.png?t=2020-11-05-21-54-19,"Image,Health,Biology",1200,1625,34699,"Our best estimates show there are over 7 billion people on the planet and 300 billion stars in the Milky Way galaxy. By comparison, the adult human body contains 37 *trillion* cells. To determine the function and relationship among these cells is a monumental undertaking. Many areas of human health would be impacted if we better understand cellular activity. A problem with this much data is a great match for the Kaggle community.

Just as the Human Genome Project mapped the entirety of human DNA, the [Human BioMolecular Atlas Program](https://hubmapconsortium.org/) (HuBMAP) is a major endeavor. Sponsored by the National Institutes of Health (NIH), HuBMAP is working to catalyze the development of a framework for mapping the human body at a level of glomeruli functional tissue units for the first time in history. Hoping to become one of the world’s largest collaborative biological projects, HuBMAP aims to be an open map of the human body at the cellular level. 

This competition, “Hacking the Kidney,"" starts by mapping the human kidney at single cell resolution.

Your challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a “three-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block” ([de Bono, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24103658)). The goal of this competition is the implementation of a successful and robust glomeruli FTU detector.

You will also have the opportunity to present your findings to a panel of judges for additional consideration. Successful submissions will construct the tools, resources, and cell atlases needed to determine how the relationships between cells can affect the health of an individual.

Advancements in HuBMAP will accelerate the world’s understanding of the relationships between cell and tissue organization and function and human health. These datasets and insights can be used by researchers in cell and tissue anatomy, pharmaceutical companies to develop therapies, or even parents to show their children the magnitude of  the human body.

&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/hubmap-kidney-segmentation/overview/code-requirements) for details.**
",
NFL 1st and Future - Impact Detection,Detect helmet impacts in videos of NFL plays,https://www.kaggle.com/competitions/nfl-impact-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/12125/logos/header.png?t=2018-11-30-18-08-32,"Football,Video Data,Sports,Computer Vision",459,573,7795,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/nfl-helmet-impacts/impacts.gif"" style=""display:block; margin: 20px auto;"">

The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to develop the “Digital Athlete,” a virtual representation of a composite NFL player that the NFL can use to model game scenarios to try to better predict and prevent player injury. The NFL is actively addressing the need for a computer vision system to detect on-field helmet impacts as part of the “Digital Athlete” platform, and the league is calling on Kagglers to help.

In this competition, you’ll develop a computer vision model that automatically detects helmet impacts that occur on the field. Kick off with a dataset of more than one thousand definitive head impacts from thousands of game images, labeled video from the sidelines and end zones, and player tracking data. This information is sourced from the NFL’s Next Gen Stats (NGS) system, which documents the position, speed, acceleration, and orientation for every player on the field during NFL games.

This competition is part of the NFL’s annual 1st and Future competition, which is designed to spur innovation in athlete safety and performance. For the first time this year, 1st and Future will be broadcast in primetime during Super Bowl LV week on NFL Network, and winning Kagglers may have the opportunity to present their computer vision systems as part of this exciting event. 

If successful, you could support the NFL’s research programs in a big way: improving athletes' safety. Backed by this research, the NFL may implement rule changes and helmet design improvements to try to better protect the athletes who play the game millions watch each week.

The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit NFL.com/PlayerHealthandSafety. 

&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/nfl-impact-detection/overview/code-requirements) for details.**
","In this task, you will segment helmet collisions in videos of football plays using bounding boxes. This competition is evaluated using a micro F1 score at an Intersection over Union (IoU) threshold of 0.35.

There are a few important differences from other bounding box metrics:
  - _The main departure from a traditional metric is that some imprecision on the timing of the impact is acceptable._ For a given ground truth impact, a prediction within +/- 4 frames (9 frames total) within the same play can be accepted as valid without necessarily degrading the score. Assuming the player is moving over the course of those frames, the exact bounding box predicted to achieve an IoU of 1.0 would also vary depending on the frame.
  - As one helmet may partially obscure another from the camera's perspective, both predicted and ground truth bounding boxes may overlap. However, at most one prediction will ever be assigned to a given ground truth box.
  - The two criteria described above mean that one or more predictions could theoretically be assigned to more than one ground truth boxes. If this happens, our metric will optimize for the assignments between your prediction(s) and the ground truth boxes that lead to the highest total number of True Positives (thereby maximizing the F1 score). At most one prediction will be assigned to any ground truth box and vice versa.


The IoU of a proposed bounding box and a ground truth bounding box is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$

The metric treats any IoU of at least 0.35 as a true positive.

F1 is calculated as follows:

$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$


## Submission File

Due to the custom metric, this competition relies on an evaluation pipeline which is slightly different than a typical code competition. **Your notebook must import and submit via the custom `nflimpact` python module available in Kaggle notebooks.** To submit, simply add these three lines at the end of your code:

```
import nflimpact
env = nflimpact.make_env()
env.predict(df) # df is a pandas dataframe of your entire submission file
```

The dataframe should be in the following format. Each row in your submission represents a single predicted bounding box for the given frame. Note that it is _not_ required to include labels of which players had an impact, only a bounding box where it occurred.

    gameKey,playID,view,video,frame,left,width,top,height
    57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1
    57590,3607,Sideline,57590_003607_Sideline.mp4,1,1,1,1,1
    57595,1252,Endzone,57595_001252_Endzone.mp4,1,1,1,1,1
    57595,1252,Sideline,57595_001252_Sideline.mp4,1,1,1,1,1
    etc.
"
INGV - Volcanic Eruption Prediction,Discover hidden precursors in geophysical data to help emergency response,https://www.kaggle.com/competitions/predict-volcanic-eruptions-ingv-oe,https://storage.googleapis.com/kaggle-competitions/kaggle/19059/logos/header.png?t=2020-09-20-19-43-36,"Signal Processing,Geology,Physics",620,760,9202,"What if scientists could anticipate volcanic eruptions as they predict the weather? While determining rain or shine days in advance is more difficult, weather reports become more accurate on shorter time scales. A similar approach with volcanoes could make a big impact. Just one unforeseen eruption can result in tens of thousands of lives lost. If scientists could reliably predict when a volcano will next erupt, evacuations could be more timely and the damage mitigated.

Currently, scientists often identify “time to eruption” by surveying volcanic tremors from seismic signals. In some volcanoes, this intensifies as volcanoes awaken and prepare to erupt. Unfortunately, patterns of seismicity are difficult to interpret. In very active volcanoes, current approaches predict eruptions some minutes in advance, but they usually fail at longer-term predictions.

Enter Italy's Istituto Nazionale di Geofisica e Vulcanologia (INGV), with its focus on geophysics and volcanology. The INGV's main objective is to contribute to the understanding of the Earth's system while mitigating the associated risks. Tasked with the 24-hour monitoring of seismicity and active volcano activity across the country, the INGV seeks to find the earliest detectable precursors that provide information about the timing of future volcanic eruptions.

In this competition, using your data science skills, you’ll predict when a volcano's next eruption will occur. You'll analyze a large geophysical dataset collected by sensors deployed on active volcanoes. If successful, your algorithms will identify signatures in seismic waveforms that characterize the development of an eruption. 

With enough notice, areas around a volcano can be safely evacuated prior to their destruction. Seismic activity is a good indicator of an impending eruption, but earlier precursors must be identified to improve longer-term predictability. The impact of your participation could be felt worldwide with tens of thousands of lives saved by more predictable volcanic ruptures and earlier evacuations.
","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/MeanAbsoluteError"">mean absolute error (MAE)</a>&nbsp;between the predicted loss and the actual loss.</p>
<h2>Submission File</h2>
<p>For every id in the test set, you should predict the time until the next eruption. The file should contain a header and have the following format:</p>
<pre>segment_id,time_to_eruption<br>1,1<br>2,2<br>3,3<br>etc.</pre>"
Riiid Answer Correctness Prediction,Track knowledge states of 1M+ students in the wild,https://www.kaggle.com/competitions/riiid-test-answer-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/21651/logos/header.png?t=2020-09-09-03-03-31,"Education,Tabular",3395,4387,64378,"<h2>Riiid AIEd Challenge 2020</h2>

[<h3>Challenge Website</h3>](https://www.ednetchallenge.ai/)
>Thank you for all those who attended the [AAAI-2021](https://aaai.org/Conferences/AAAI-21/) workshop on AI Education! Prize-winning teams presented their models at the AAAI-2021 Workshop on AI Education - [Imagining Post-COVID Education with AI](https://sites.google.com/view/tipce-2021/home?authuser=1) - on February 9, 2021. You can find the model write-ups on the workshop website.

Think back to your favorite teacher. They motivated and inspired you to learn. And they knew your strengths and weaknesses. The lessons they taught were based on your ability. For example, teachers would make sure you understood algebra before advancing to calculus. Yet, many students don’t have access to personalized learning. In a world full of information, data scientists like you can help. Machine learning can offer a path to success for young people around the world, and you are invited to be part of this mission.

<img title=”Riiid” src=""https://storage.googleapis.com/kaggle-media/competitions/Riiid/Graphic%20or%20image%20within%20description%20(min%20size%20350x350).png"" style=""float: right; width: 250px"">

In 2018, 260 million children weren't attending school. At the same time, more than half of these young students didn't meet minimum reading and math standards. Education was already in a tough place when COVID-19 forced most countries to temporarily close schools. This further delayed learning opportunities and intellectual development. The equity gaps in every country could grow  wider. We need to re-think the current education system in terms of attendance, engagement, and individualized attention.

Riiid Labs, an AI solutions provider delivering creative disruption to the education market, empowers global education players to rethink traditional ways of learning leveraging AI. With a strong belief in equal opportunity in education, Riiid launched an AI tutor based on deep-learning algorithms in 2017 that attracted more than one million South Korean students. This year, the company released EdNet, the world’s largest open database for AI education containing more than 100 million student interactions.

In this competition, your challenge is to create algorithms for ""Knowledge Tracing,"" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions. You will pair your machine learning skills using Riiid’s EdNet data. 

Your innovative algorithms will help tackle global challenges in education. If successful, it’s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. With your participation, we can build a better and more equitable model for education in a post-COVID-19 world.

<h2>Acknowledgements</h2>
<h3>Academic Advisors</h3>
[Paul Kim](https://gse-it.stanford.edu/about/team/paul-kim), Stanford Graduate School of Education
[Neil Heffernan](https://www.neilheffernan.net/), WPI & ASSISTments

<h3>Partners</h3>
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5180416%2F1316be20b53c09a1569e3bb4af77762a%2FScreen%20Shot%202020-10-12%20at%2011.24.59%20AM.png?generation=1602469542965510&alt=media)","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
You must make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

The kernels environment automatically formats and creates your submission files in this competition. There is no need to manually create your submissions."
Google Research Football with Manchester City F.C.,Train agents to master the world's most popular sport,https://www.kaggle.com/competitions/google-football,https://storage.googleapis.com/kaggle-competitions/kaggle/21723/logos/header.png?t=2020-07-23-19-26-16,"Simulations,Reinforcement Learning",1138,1288,14015,"Manchester City F.C. and Google Research are proud to present AI football competition using the [Google Research Football Environment](https://github.com/google-research/football).

### A word from Manchester City F.C.

<img title=”MCFC” src=""https://storage.googleapis.com/kaggle-media/competitions/Research%20Football/Manchester%20City%20(1).png"" style=""float: right; width: 200px"">

Brian Prestidge, Director of Data Insights & Decision Technology at City Football Group, the owners of Manchester City F.C., sets out the challenge. “Football is a tough environment to perform in and an even tougher environment to learn in. Learning is all about harnessing failure, but failure in football is seldom accepted. Working with Google Research’s physics based football environment provides us with a new place to learn through simulation and offers us the capabilities to test tactical concepts and refine principles so that they are strong enough for a coach to stake their career on.”

“We are therefore very pleased to be working with Google’s research team in creating this competition and are looking forward to the opportunity to support some of the most creative and successful competitors through funding and exclusive prizes. We hope to establish ongoing collaboration with the winners beyond this competition, and that it will provide us all with the platform to explore and establish fundamental principles of football tactics, thus improving our ability to perform and be successful on the pitch.”

Greg Swimer, Chief Technology Officer at City Football Group added ""Technologies such as Machine Learning and Artificial Intelligence have huge future potential to enhance the understanding and enjoyment of football for players, coaches and fans.  We are delighted to be collaborating with Google's research team to help broaden the knowledge, talent, and innovation working in this exciting and transformational area"".
 
### The Google Research football environment competition

The world gets a kick out of football (soccer in the United States). As the most popular sport on the planet, millions of fans enjoy watching Sergio Agüero, Raheem Sterling, and Kevin de Bruyne on the field. Football video games are less lively, but still immensely popular, and we wonder if AI agents would be able to play those properly.

Researchers want to explore AI agents' ability to play in complex settings like football. The sport requires a balance of short-term control, learned concepts such as passing, and high-level strategy, which can be difficult to teach agents. A current environment exists to train and test agents, but other solutions may offer better results.

The teams at Google Research aspire to make discoveries that impact everyone. Essential to their approach is sharing research and tools to fuel progress in the field. Together with Manchester City F.C., Google Research has put forth this competition to get help in reaching their *goal*.

![GRF](https://1.bp.blogspot.com/-tSPIa1HlNrg/XPqRavoz7lI/AAAAAAAAEMU/oGB2mmwSl_4TFVKN1NNCQD-qlDNZQr2VQCLcBGAs/s640/Screenshot%2B2019-06-05%2Bat%2B1.38.14%2BPM.png)

In this competition, you’ll create AI agents that can play football. Teams compete in “steps,” where agents react to a game state. Each agent in an 11 vs 11 game controls a single active player and takes actions to improve their team’s situation. As with a typical football game, you want your team to score more than the other side. You can optionally see your efforts rendered in a physics-based 3D football simulation.

If controlling 11 football players with code sounds difficult, don't be discouraged! You only need to control one player at a time (the one with the ball on offense, or the one closest to the ball on defense) and your code gets to pick from 1 of 19 possible actions. We have prepared a [getting started](https://www.kaggle.com/c/google-football/overview/getting-started) example to show you how simple a basic strategy can be. Before implementing your own strategy, however, you might want to learn more about [the Google Research football environment](https://github.com/google-research/football/), especially [observations provided to you by the environment and available actions](https://github.com/google-research/football/blob/master/gfootball/doc/observation.md). You can also [play the game yourself](https://github.com/google-research/football/#playing-the-game) on your computer locally to get better understanding of the environment's dynamics and explore different scenarios.

If successful, you'll help researchers explore the ability of AI agents to play in complex settings. This could offer new insights into the strategies of the world's most-watched sport. Additionally, this research could pave the way for a new generation of AI agents that can be trained to learn complex skills.","Each day, your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time, skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.

Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.

When you upload a Submission, we first play a Validation Episode where that Submission plays against a copy of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.

We repeatedly run Episodes from the pool of All Submissions and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission with an additional slight rate increase for the newest-submitted Episodes to give you feedback faster.

After an Episode finishes, we'll update the Rating estimate of both agents in that Episode. If one agent won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.

At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run games. At the conclusion of this week, the leaderboard is final."
OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction,Urgent need to bring the COVID-19 vaccine to mass production,https://www.kaggle.com/competitions/stanford-covid-vaccine,https://storage.googleapis.com/kaggle-competitions/kaggle/22111/logos/header.png?t=2020-09-10-20-12-59,"Biology,Biotechnology,Coronavirus,Public Health",1636,1839,35806,"Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress. 

mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19,  but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines. 

Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.

  <img src=""https://storage.googleapis.com/kaggle-media/competitions/Stanford/banner%20(2).png"">

The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford’s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world’s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.

In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a  second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models! 

Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve.  Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.    

<a href=""http://eternagame.org/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/Stanford/logo_eterna_reverse.png"" style=""width: 300px"">&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""https://challenges.eternagame.org/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/Stanford/OpenVaccine-logo-black-transp-bg%20(1).png"" style=""width: 150px""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a><a href=""http://daslab.stanford.edu/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/Stanford/logo_das.png"" style=""width: 100px"">
","Submissions are scored using MCRMSE, mean columnwise root mean squared error:

$$
\textrm{MCRMSE} = \frac{1}{N\_{t}}\sum\_{j=1}^{N\_{t}}\sqrt{\frac{1}{n} \sum\_{i=1}^{n} (y\_{ij} - \hat{y}\_{ij})^2}
$$
where \\(N\_t\\) is the number of scored ground truth target columns, and \\(y\\) and \\(\hat{y}\\) are the actual and predicted values, respectively.

From the `Data` page: There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg\_Mg\_pH10, and deg\_Mg\_50C.

## Submission File
For each sample `id` in the test set, you must predict targets for _each_ sequence position (`seqpos`), one per row. If the length of the `sequence` of an `id` is, e.g., 107, then you should make 107 predictions. Positions greater than the `seq_scored` value of a sample are not scored, but still need a value in the solution file.

    id_seqpos,reactivity,deg_Mg_pH10,deg_pH10,deg_Mg_50C,deg_50C	
    id_00073f8be_0,0.1,0.3,0.2,0.5,0.4
    id_00073f8be_1,0.3,0.2,0.5,0.4,0.2
    id_00073f8be_2,0.5,0.4,0.2,0.1,0.2
    etc."
RSNA STR Pulmonary Embolism Detection,Classify Pulmonary Embolism cases in chest CT scans,https://www.kaggle.com/competitions/rsna-str-pulmonary-embolism-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/22307/logos/header.png?t=2020-09-02-17-44-41,"Image,Health",784,981,11408,"<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2F9a3aac7e7ac865f134201cc2a5cd52f3%2Fkaggle_header3.png?generation=1599585319459400&amp;alt=media"" style=""float: right; width: 300px"">

If every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients.

Currently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists’ time may contribute to delayed diagnosis.

The Radiological Society of North America (RSNA®) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE.

In this competition, you’ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment.

With 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes.

&gt; **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection/overview/code-requirements) for details.**

###Acknowledgments

The Radiological Society of North America (RSNA®) is an international society of radiologists, medical physicists, and other medical professionals with more than 53,400 members worldwide.  RSNA hosts the world’s premier radiology forum and publishes two top peer-reviewed journals: Radiology, the highest-impact scientific journal in the field, and RadioGraphics, the only journal dedicated to continuing education in radiology. 

The Society of Thoracic Radiology (STR) was founded in 1982.  The STR is dedicated to advancing cardiothoracic imaging in clinical application, education, and research in radiology and allied disciplines.  Continuing professional development opportunities provided by the STR include educational and scientific meetings, mentorship programs, grant support and award opportunities, our society journal, Journal of Thoracic Imaging, and global collaboration activities.

[A full set of acknowledgments can be found on this page](https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection/overview/acknowledgments).","Every study / exam has a row for each label that is scored (detailed in the Data page). It is uniquely indicated by the `StudyInstanceUID`. Every image, further, has a row for the `PE Present on Image` label and is uniquely indicated by the `SOPInstanceUID`. Your prediction file should have a number of rows equal to: (number of images) + (number of studies * number of scored labels).

## Metric

The metric used in this competition is weighted log loss. It is weighted to account for the relative importance of some labels. There are 9 study-level labels and one image-level label, detailed further on the Data page.

## Exam-level weighted log loss

Let y\_ij = 1 if label j was annotated to exam i and y\_ij = 0, otherwise. Let p_ij be the predicted probability that y\_ij = 1: 
i = 1, 2, …, N for N exams in the test set
j = 1, 2, …, 9 labels

Let w_j signify the weight for label j.

The weights are as follows:
| Label | Weight |
| --- | --- |
| Negative for PE | 0.0736196319 |
| Indeterminate | 0.09202453988 |
| Chronic | 0.1042944785 |
| Acute & Chronic | 0.1042944785 |
| Central PE | 0.1877300613 |
| Left PE | 0.06257668712 |
| Right PE | 0.06257668712 |
| RV/LV Ratio >= 1 | 0.2346625767 |
| RV/LV Ratio < 1 | 0.0782208589 |

Kaggle uses a binary log loss equation for each label and then takes the mean of the log loss over all labels.

The binary weighted log loss function for label j on exam i is specified as:
$$ 
L\_{ij} = - w\_j \* [ y_{ij}\*log(p\_{ij}) + (1-y\_{ij})\*log(1-p\_{ij}) ]
$$

## Image-level weighted log loss

Let y\_ik = 1 if image k in exam i was annotated as ‘PE Present on Image’; otherwise, y\_ik = 0. 
Let p\_ik be the predicted probability that y\_ik = 1.
w = `0.07361963`
i = 1, 2, …, N exams
k =  1, 2, …, n\_i, where n\_i is the number of images in exam i

Then, let m\_i = sum\_(k = 1 to n\_i) y\_ik be the number of positive images in exam i such that 
q\_i = m\_i/n\_i is the proportion of positive images in exam i

At the image level, we have a binary classification where the image is classified as `PE Present on Image` or not (image is negative for PE).

The image-level log loss is written as:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F115173%2F190abb127749ab99082a1b0454205b48%2Fql_c2a79726b7d3b29ce07fc58ae568c6e8_l3.png?generation=1599680376511361&alt=media)



The total loss is the average of all image- and exam-level loss, divided by the average of all row (both image- and exam-level) weights. To get the average of all row weights, sum the weights of all images (`q_i*w` for each image) and all exam-level labels (`w_j` for each label `j` in the test set) and divide by the number of rows.

## Submission Format

> id,label
df06fad17bc3\_negative\_exam\_for\_pe,0.5
df06fad17bc3\_rv\_lv\_ratio\_gte\_1,0.5
df06fad17bc3\_rv\_lv\_ratio\_lt\_1,0.5
df06fad17bc3\_leftsided\_pe,0.5
df06fad17bc3\_chronic\_pe,0.5
df06fad17bc3\_rightsided\_pe,0.5
df06fad17bc3\_acute\_and\_chronic\_pe,0.5
df06fad17bc3\_central\_pe,0.5
df06fad17bc3\_indeterminate,0.5
eb3cbf4180b5,0.5
57b93aeb1b16,0.5
ca48991fcad3,0.5
c72c1f5763d4,0.5
26c67856a1e9,0.5
3c64e5645222,0.5
d3e59334bba4,0.5
be315623c913,0.5
74941ba7b035,0.5
70589c8529fb,0.5
etc."
Hash Code Archive - Drone Delivery,Can you help coordinate the drone delivery supply chain?,https://www.kaggle.com/competitions/hashcode-drone-delivery,https://storage.googleapis.com/kaggle-competitions/kaggle/22040/logos/header.png?t=2020-08-19-16-25-21,"Optimization,Internet",130,130,1291,"This is a synthetic code challenge to sharpen your programming skills. This problem was first released during the 2016 qualification round of Google's annual coding competition, [Hash Code](https://codingcompetitions.withgoogle.com/hashcode/). We’ve re-released it as a Playground Code Competition to help you sharpen your skills. Along with the [Photo Slideshow Optimization competition](https://www.kaggle.com/c/hashcode-photo-slideshow), open for late submissions, you can use it as practice in advance of Hash Code 2021.

The Internet has profoundly changed the way we buy things, but the online shopping of today is likely not the end of that change; the expectations for purchase delivery has gone from a week, to two days, to one day, to same day. What about in just a few hours? With drones, this may be possible, and they’ll bring a whole new fleet of problems to solve with data science.

Drones are­ autonomous, electric vehicles often used to deliver online purchases. Current experiments use flying drones, so they’re never stuck in traffic. As drone technology improves every year, there remains a major issue: how would we manage and coordinate all those drones?

In this competition, you are given a hypothetical fleet of drones, a list of customer orders, and availability of the individual products in warehouses. Can you schedule the drone operations so that the orders are completed as soon as possible?

When flying delivery drones become the norm, scheduling is one of the many problems to be solved. Get a head start—and improve your data science skills at the same time.

<blockquote><b>This is a Code Competition. Refer to <a href=""https://www.kaggle.com/c/hashcode-drone-delivery#Code-Requirements"" target = ""blank"">Code Requirements</a> for details.</b></blockquote>

<span>Photo by <a href=""https://unsplash.com/@iusher?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"">Ian Usher</a> on <a href=""https://unsplash.com/s/photos/drone?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"">Unsplash</a></span>","Each completed order will earn between 1 and 100 points, depending on the turn in which it is completed.
The order is completed in the first turn at the end of which all items in the order are delivered.
For an order completed in turn _t_ and a simulation taking _T_ turns in total, the score for the order is calculated as (_T_ − _t_) / _T_ × 100 , rounded up to the next integer.

For example, if the simulation takes 160 turns (_T_ = 160), and an order consists of three items, delivered at turns 5, 15 and 15, then the order is considered completed at _t_ = 15, and the score is calculated as (160 − 15)/160 = 0.90625 , multiplied by 100 and rounded up to 91 points.

For more examples, see the instructions pdf on [the data tab](https://kaggle.com/c/hashcode-drone-delivery/data)."
Mechanisms of Action (MoA) Prediction,Can you improve the algorithm that classifies drugs based on their biological activity?,https://www.kaggle.com/competitions/lish-moa,https://storage.googleapis.com/kaggle-competitions/kaggle/19988/logos/header.png?t=2020-06-16-17-24-17,"Biology,Genetics,Drugs and Medications,Tabular",4373,5323,88732,"The [Connectivity Map](https://clue.io), a project within the Broad Institute of MIT and Harvard,  the [Laboratory for Innovation Science at Harvard (LISH)](http://lish.harvard.edu), and the [NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS)](http://lincsproject.org), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.

**What is the Mechanism of Action (MoA) of a drug? And why is it important?**   

In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.

**How do we determine the MoAs of a new drug?**   

One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.

In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells’ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.

As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.    

**How to evaluate the accuracy of a solution?**    

Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the [logarithmic  loss function](https://www.kaggle.com/c/lish-moa/overview/evaluation) applied to each drug-MoA annotation pair.

If successful, you’ll help to develop an algorithm to predict a compound’s MoA given its cellular signature, thus helping scientists advance the drug discovery process.    

> **This is a Code Competition. Refer to [Code Requirements](/c/lish-moa/overview/code-requirements) for details.**

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4688294%2Fa7c39a710116cc60ab0e0707020df4f5%2FUnknown-31?generation=1601643378654178&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4688294%2F7b66ae0c294d9ca67272209d4756e0e9%2Flogo_largetext_preview-4.png?generation=1601643409931253&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4688294%2F18004b33573a510867eac50ae6c68ec2%2FUnknown-32?generation=1601643476632984&alt=media)
","For every `sig_id` you will be predicting the probability that the sample had a positive response for each `<MoA>` target. For \\(N\\) `sig_id` rows and \\(M\\) `<MoA>` targets, you will be making \\(N \times M\\) predictions. Submissions are scored by the log loss:

$$ \text{score} = - \frac{1}{M}\sum\_{m=1}^{M} \frac{1}{N} \sum\_{i=1}^{N} \left[ y\_{i,m} \log(\hat{y}\_{i,m}) + (1 - y\_{i,m}) \log(1 - \hat{y}\_{i,m})\right] $$

where:

 - \\(N\\) is the number of `sig_id` observations in the test data (\\(i=1,...,N\\))
 - \\(M\\) is the number of scored MoA targets (\\(m=1,...,M\\))
 - \\( \hat{y}_{i,m} \\) is the predicted probability of a positive `MoA` response for a `sig_id`
 - \\( y_{i,m} \\) is the ground truth, 1 for a positive response, 0 otherwise
 - \\( log() \\) is the natural (base e) logarithm

Note: the actual submitted predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\). A smaller log loss is better.

## Submission File

You must predict a probability of a positive target for each `sig_id-<MoA>` pair. The `id` used for the submission is created by concatenating the `sig_id` with the `MoA` target for which you are predicting. The file should have a header and be in the following format:

    sig_id,11-beta-hsd1_inhibitor,ace_inhibitor,...,wnt_inhibitor
    id_000644bb2,0.32,0.01,...,0.57
    id_000a6266a,0.88,0.27,...,0.42
    etc...
"
Conway's Reverse Game of Life 2020,Reverse the arrow of time in the Game of Life,https://www.kaggle.com/competitions/conways-reverse-game-of-life-2020,https://storage.googleapis.com/kaggle-competitions/kaggle/22122/logos/header.png?t=2020-08-31-19-59-27,"Simulations,Board Games",188,259,1377,"This is a relaunch of a previous competition, [Conway's Reverse Game of Life](https://www.kaggle.com/c/conway-s-reverse-game-of-life/overview), with the following changes:
1. The grid size is larger (25 vs. 25) and the grid wraps around from top to bottom and left to right
2. Submissions are solved forward by the appropriate number of steps, so that _any_ correct starting solution will achieve a maximum score. This [article](http://jakevdp.github.io/blog/2013/08/07/conways-game-of-life/) contains the stepping function that is used for this competition.

**Obligatory Disclaimer:** A lot has changed since the original competition was launched 6 years ago. With the change from ""exact starting point"" to  ""_any_ correct starting point"", it is possible to get a perfect score. We just don't know how difficult that will be. Use it as a fun learning experience, _**and don't spoil it for others by posting perfect solutions!**_

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:

- **Overpopulation:** if a living cell is surrounded by more than three living cells, it dies.
- **Stasis:** if a living cell is surrounded by two or three living cells, it survives.
- **Underpopulation:** if a living cell is surrounded by fewer than two living cells, it dies.
- **Reproduction:** if a dead cell is surrounded by exactly three cells, it becomes a live cell.

These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics. As [Wikipedia](http://en.wikipedia.org/wiki/Conway's_Game_of_Life) states</p>

```
Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.
```

The emergence of order from simple rules begs an interesting question&#8212;_what happens if we set time backwards?_

This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends? We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/conways-reverse-game-of-life-2020/overview/code-requirements) for details.**","You are evaluated on the mean absolute error of your predictions, stepped forward by the specified, and compared to the provided ending solution.

In this case, this is equivalent to \\(1 - \text{classification accuracy}\\) across all of the cells. You may only predict 0 (dead) or 1 (alive) for each cell.

##Submission File
For every game in the test set, your submission file should list the predicted starting board on a single row. **Values are listed in a _row_-wise order.** That is, if you want to predict a matrix,

    1 2
    3 4

the predicted row would be (1,2,3,4). The submission file should contain a header and have the following format:

    id,start_0,start_1,start_2,...,start_624
    50000,0,0,0,0,0,0,...,0
    50001,0,0,0,0,0,0,...,0
    etc."
I’m Something of a Painter Myself,Use GANs to create art - will you be the next Monet?,https://www.kaggle.com/competitions/gan-getting-started,https://storage.googleapis.com/kaggle-competitions/kaggle/21755/logos/header.png?t=2020-08-26-18-44-58,"Image,GAN",98,108,424,">*“Every artist dips his brush in his own soul, and paints his own nature into his pictures.”*
-Henry Ward Beecher
 
We recognize the works of artists through their unique style, such as color choices or brush strokes. The “je ne sais quoi” of artists like Claude Monet can now be imitated with algorithms thanks to generative adversarial networks (GANs). In this getting started competition, you will bring that style to your photos or recreate the style from scratch!
 
Computer vision has advanced tremendously in recent years and GANs are now capable of mimicking objects in a very convincing way.  But creating museum-worthy masterpieces is thought of to be, well, more art than science. So can (data) science, in the form of GANs, trick classifiers into believing you’ve created a true Monet? That’s the challenge you’ll take on!


## The Challenge:
 
A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.
 
The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.
 
Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images. 


## Getting Started:
 
Details on the dataset can be found [here](https://www.kaggle.com/c/gan-getting-started/data) and an overview of the evaluation process can be found [here](https://www.kaggle.com/c/gan-getting-started/overview/evaluation).
 
To learn how to submit and answers to other FAQs, review the [Frequently Asked Questions](https://www.kaggle.com/c/gan-getting-started/overview/frequently-asked-questions).
 
<div class=""note"">
<strong>Recommended Tutorial</strong><br>
We highly recommend <a href=""https://www.kaggle.com/amyjang/monet-cyclegan-tutorial"">Amy Jang's notebook </a> that goes over the basics of loading data from TFRecords, using TPUs, and building a CycleGAN.
</div>

Although the competition dataset only includes Monet images, check out this [dataset](https://github.com/junyanz/CycleGAN) for Cezanne, Ukiyo-e, and Van Gogh paintings to run your GAN on.","## MiFID

Submissions are evaluated on MiFID (Memorization-informed Fréchet Inception Distance), which is a modification from [Fréchet Inception Distance (FID)](https://arxiv.org/abs/1706.08500). 

The smaller MiFID is, the better your generated images are.  



## What is FID?
Originally published [here](https://arxiv.org/abs/1706.08500) ([github](https://github.com/bioinf-jku/TTUR)), FID, along with [Inception Score (IS)](https://arxiv.org/abs/1606.03498), are both commonly used in recent publications as the standard for evaluation methods of GANs. 

In FID, we use the Inception network to extract features from an intermediate layer. Then we model the data distribution for these features using a multivariate Gaussian distribution with mean µ and covariance Σ. The FID between the real images \\(r\\) and generated images \\(g\\) is computed as:

\\[
\text{FID} = ||\mu\_r - \mu\_g||^2 + \text{Tr} (\Sigma\_r + \Sigma\_g - 2 (\Sigma\_r \Sigma\_g)^{1/2})
\\]

where \\(Tr\\) sums up all the diagonal elements. FID is calculated by computing the Fréchet distance between two Gaussians fitted to feature representations of the Inception network.



## What is MiFID (Memorization-informed FID)?
In addition to FID, Kaggle takes training sample memorization into account. 

The memorization distance is defined as the minimum cosine distance of all training samples in the feature space, averaged across all user generated image samples. This distance is thresholded, and it's assigned to 1.0 if the distance exceeds a pre-defined epsilon. 

In mathematical form:

\\[d\_{ij} = 1 - cos(f\_{gi}, f\_{rj}) = 1 - \frac{f\_{gi} \cdot f\_{rj}}{|f\_{gi}| |f\_{rj}|}\\] 

where \\(f\_g\\) and \\(f\_r\\) represent the generated/real images in feature space (defined in pre-trained networks); and \\(f\_{gi}\\) and \\(f\_{rj}\\) represent the \\(i^{th}\\) and \\(j^{th}\\) vectors of \\(f\_g\\) and \\(f\_r\\), respectively. 


\\[d = \frac{1}{N} \sum\_{i} \min\_j d\_{ij} \\]

defines the minimum distance of a certain generated image (\\(i\\)) across all real images ((\\(j\\)), then averaged across all the generated images. 

![enter image description here][1]
defines the threshold of the weight only applies when the (\\(d\\)) is below a certain empirically determined threshold. 

Finally, this memorization term is applied to the FID:
\\[ MiFID = FID \cdot \frac{1}{d\_{thr}}\\]

## Kaggle's workflow calculating MiFID for public and private scores
Kaggle calculates public MiFID scores with the pre-train neural network [Inception](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz), and the public images used for evaluation are the rest of the TFDS Monet paintings. **Note that as a Getting Started competition there is no private leaderboard.**

A demo of our MiFID evaluation code can be seen [here](https://www.kaggle.com/wendykan/demo-mifid-metric-for-dog-image-generation-comp).


## Submission File
You are going to generate 7,000-10,000 Monet-style images that are in `jpg` format. Their sizes should be 256x256x3 (RGB). Then you need to zip those images and your output from your Kernel should only have **ONE** output file named `images.zip`. 

Please note that Kaggle Kernels has a number of output files capped at 500. We highly encourage you to either directly write to a `zip` file as you generate images, or create a folder at `../tmp` as your temporary directory. 

> **Honor Code**: This competition has a unique format with images expected as submission, rather than predictions. Competitors should be using **generative methods** to create their submission images and not directly submit images of Monet paintings or altered versions of such images. This defeats purpose of a getting started competition aimed at learning.


  [1]: https://storage.googleapis.com/kaggle-media/competitions/GAN/latex-img-150.png
  [2]: https://storage.googleapis.com/kaggle-media/competitions/GAN/Kaggle%20GAN%20Diagram%20(3).png"
Lyft Motion Prediction for Autonomous Vehicles,Build motion prediction models for self-driving vehicles ,https://www.kaggle.com/competitions/lyft-motion-prediction-autonomous-vehicles,https://storage.googleapis.com/kaggle-competitions/kaggle/19990/logos/header.png?t=2020-08-17-20-10-07,"Automobiles and Vehicles,Tabular,Image,Transportation",935,1254,14900,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/Lyft-Kaggle/Motion%20Prediction/BP9I1484%20(1).jpg"" style=""float: right; width: 200px"">


Autonomous vehicles (AVs) are expected to dramatically redefine the future of transportation. However, there are still significant engineering challenges to be solved before one can fully realize the benefits of self-driving cars. One such challenge is building models that reliably predict the movement of traffic agents around the AV, such as cars, cyclists, and pedestrians.

  

The ridesharing company Lyft started [Level 5](https://level5.lyft.com/) to take on the self-driving challenge and build a full self-driving system ([they’re hiring!](https://www.lyft.com/careers?category=autonomous%2520vehicles)). Their [previous competition](http://kaggle.com/c/3d-object-detection-for-autonomous-vehicles/) tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents.

  

In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You'll have access to the largest [Prediction Dataset](https://self-driving.lyft.com/level5/prediction/) ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV's environment.

  

Lyft’s mission is to improve people’s lives with the world’s best transportation. They believe in a future where self-driving cars make transportation safer, environment-friendly and more accessible for everyone. Their goal is to accelerate development across the industry by sharing data with researchers. As a result of your participation, you can have a hand in propelling the industry forward and helping people around the world benefit from self-driving cars sooner.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/overview/code-requirements) for details.**","The goal of this competition is to predict the trajectories of other traffic participants. You can employ uni-modal models yielding a single prediction per sample, or multi-modal ones generating multiple hypotheses (up to 3) - further described by a confidence vector.

Due to the high amount of multi-modality and ambiguity in traffic scenes, the used evaluation metric to score this competition is tailored to account for multiple predictions.

**Note:** The following is a brief excerpt of our [metrics page in the L5Kit repository] (https://github.com/lyft/l5kit/blob/master/competition.md)


We calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. Let us take a closer look at this. Assume, ground truth positions of a sample trajectory are

<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20x_1%2C%20%5Cldots%2C%20x_T%2C%20y_1%2C%20%5Cldots%2C%20y_T"" style=""float: left"">
<br/>

and we predict K hypotheses, represented by means

<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20%5Cbar%7Bx%7D_1%5Ek%2C%20%5Cldots%2C%20%5Cbar%7Bx%7D_T%5Ek%2C%20%5Cbar%7By%7D_1%5Ek%2C%20%5Cldots%2C%20%5Cbar%7By%7D_T%5Ek"" style=""float: left"">
<br/>

In addition, we predict confidences c of these K hypotheses. We assume the ground truth positions to be modeled by a mixture of multi-dimensional independent Normal distributions over time, yielding the likelihood

<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20p%28x_%7B1%2C%20%5Cldots%2C%20T%7D%2C%20y_%7B1%2C%20%5Cldots%2C%20T%7D%7Cc%5E%7B1%2C%20%5Cldots%2C%20K%7D%2C%20%5Cbar%7Bx%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7B1%2C%20%5Cldots%2C%20K%7D%2C%20%5Cbar%7By%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7B1%2C%20%5Cldots%2C%20K%7D%29"" style=""float: left"">
<br/>
<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20%3D%20%5Csum_k%20c%5Ek%20%5Cmathcal%7BN%7D%28x_%7B1%2C%20%5Cldots%2C%20T%7D%7C%5Cbar%7Bx%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7Bk%7D%2C%20%5CSigma%3D1%29%20%5Cmathcal%7BN%7D%28y_%7B1%2C%20%5Cldots%2C%20T%7D%7C%5Cbar%7By%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7Bk%7D%2C%20%5CSigma%3D1%29"" style=""float: left"">
<br/>
<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20%3D%20%5Csum_k%20c%5Ek%20%5Cprod_t%20%5Cmathcal%7BN%7D%28x_t%7C%5Cbar%7Bx%7D_t%5Ek%2C%20%5Csigma%3D1%29%20%5Cmathcal%7BN%7D%28y_t%7C%5Cbar%7By%7D_t%5Ek%2C%20%5Csigma%3D1%29"" style=""float: left"">
<br/>
which results in the loss
<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20L%20%3D%20-%20%5Clog%20p%28x_%7B1%2C%20%5Cldots%2C%20T%7D%2C%20y_%7B1%2C%20%5Cldots%2C%20T%7D%7Cc%5E%7B1%2C%20%5Cldots%2C%20K%7D%2C%20%5Cbar%7Bx%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7B1%2C%20%5Cldots%2C%20K%7D%2C%20%5Cbar%7By%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7B1%2C%20%5Cldots%2C%20K%7D%29"" style=""float: left"">
<br/>
<img src=""https://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Clarge%20%3D%20-%20%5Clog%20%5Csum_k%20e%5E%7B%5Clog%28c%5Ek%29%20-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_t%20%28%5Cbar%7Bx%7D_t%5Ek%20-%20x_t%29%5E2%20&plus;%20%28%5Cbar%7By%7D_t%5Ek%20-%20y_t%29%5E2%7D"" style=""float: left"">
<br/>


## Submission File
**Note:** if you're using [L5Kit](https://github.com/lyft/l5kit), we provide a [function](https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/csv_utils.py#L140) to directly convert your predictions (single and multi-modal) into a valid CSV.


Every agent is identified by its `track_id` and its `timestamp`.  Each trajectory holds 50 2D `(X,Y)` predictions. 
You can predict up to 3 trajectories for each agent in the test set. Because the format is a CSV file, all 3 trajectories fields must have a value, even if your prediction is single-modal. However, each one of the three trajectory has its own confidence, and you can set it 0 to completely ignore one or more trajectories during evaluation. The 3 confidences must sum to 1.

An example of a valid CSV header:
```
timestamp, track_id, conf_0, conf_1, conf_2, coord_x00, coord_y_00,...,coord_x049, coord_y_049, coord_x10, coord_y_10,...,coord_x149, coord_y_149, coord_x20, coord_y_20,...,coord_x249, coord_y_249
```"
"Contradictory, My Dear Watson",Detecting contradiction and entailment in multilingual text using TPUs,https://www.kaggle.com/competitions/contradictory-my-dear-watson,https://storage.googleapis.com/kaggle-competitions/kaggle/21733/logos/header.png?t=2020-07-29-20-44-13,"Text,Multiclass Classification,NLP",71,86,202,"> *""...when you have eliminated the impossible,
whatever remains, however improbable, must be the truth""*
-Sir Arthur Conan Doyle

Our brains process the meaning of a sentence like this rather quickly. 

We're able to surmise:
- Some things to be true: ""You can find the right answer through the process of elimination.”
- Others that may have truth: ""Ideas that are improbable are not impossible!""
- And some claims are clearly contradictory: ""Things that you have ruled out as impossible are where the truth lies.""

Natural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more. 


## The Challenge:

If you have two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated. Natural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related. 

Your task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! You can find more details on the dataset by reviewing the [Data page](https://www.kaggle.com/c/contradictory-my-dear-watson/data).

Today, the most common approaches to NLI problems include using embeddings and transformers like BERT. In this competition, we’re providing [a starter notebook](https://www.kaggle.com/anasofiauzsoy/tutorial-notebook) to try your hand at this problem using the power of Tensor Processing Units (TPUs). TPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. Kaggle provides all users TPU Quota at no cost, which you can use to explore this competition. Check out our [TPU documentation](https://www.kaggle.com/docs/tpu) and [Kaggle’s YouTube playlist](https://www.youtube.com/playlist?list=PLqFaTIg4myu-1c3ygYzakW8-hNzQG59-5) for more information and resources.

<div class=""note"">
<strong>Recommended Tutorial</strong><br>
We highly recommend <a href=""https://www.kaggle.com/anasofiauzsoy/tutorial-notebook"">Ana Sofia Uzsoy’s Tutorial</a> that walks you through creating your very first submission step by step with TPUs and BERT.
</div>

This is a great opportunity to flex your NLP muscles and solve an exciting problem!

*Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.*","### Goal

Your goal is to predict whether a given hypothesis is related to its premise by `contradiction`, `entailment`, or whether neither of those is true (`neutral`).
For each sample in the test set, you must predict a 0, 1, or 2 value for the variable.

Those values map to the logical condition as:
> 0 == entailment
1 == neutral
2 == contradiction

### Metric

Your score is the percentage of relationships you correctly predict. This is known as [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification).

### Submission File Format

You should submit a csv file with exactly 5195 entries **plus** a header row. Your submission will show an error if you have extra columns (beyond `id` and `prediction`) or rows.  

The file should have exactly 2 columns:

*   `id` (sorted in any order)
*   `prediction` (contains your predictions: 0 for entailment, 1 for neutral, 2 for contradiction)

> **id,prediction**  
c6d58c3f69,1
cefcc82292,1
e98005252c,1
58518c10ba,1
c32b0d16df,1
Etc.

You can download an example submission file (`sample_submission.csv`) on the [Data page](https://www.kaggle.com/c/contradictory-my-dear-watson/data).

### Code Submission Requirement

In this code competition, your `submission.csv` file must be generated as an output from a Kaggle notebook. For details on how to submit from a notebook, review the [FAQ on ""How do I make a submission?""](https://www.kaggle.com/c/contradictory-my-dear-watson/overview/frequently-asked-questions)"
Google Landmark Recognition 2020,Label famous (and not-so-famous) landmarks in images,https://www.kaggle.com/competitions/landmark-recognition-2020,https://storage.googleapis.com/kaggle-competitions/kaggle/19231/logos/header.png?t=2020-07-23-22-48-06,"Image,Computer Vision",736,937,18783,"Welcome to the third Landmark Recognition competition! This year, we have worked to set this up as a code competition and collected a new set of test images.

Have you ever gone through your vacation photos and asked yourself: *What was the name of that temple I visited in China?* or *Who created this monument I saw in France?* Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.

Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.

In the previous editions of this challenge ([2018](https://www.kaggle.com/c/landmark-recognition-challenge) and  [2019](https://www.kaggle.com/c/landmark-recognition-2019)), submissions were handled by uploading prediction files to the system. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.

This challenge is organized in conjunction with the [Landmark Retrieval Challenge 2020](https://www.kaggle.com/c/landmark-retrieval-2020), which was launched June 30, 2020. Both challenges are affiliated with the [Instance-Level Recognition workshop](https://ilr-workshop.github.io/ECCVW2020/) in ECCV’20.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/landmark-recognition-2020/overview/code-requirements) for details.**
","<p>Submissions are evaluated using Global Average Precision (GAP) at \\(k\\), where \\(k=1\\). This metric is also known as micro Average Precision (\\(\mu\\)AP), as per [1,2]. It works as follows:</p>
<p>For each test image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions (sorted in descending order by confidence scores), and computes the Average Precision based on this list. </p>
<p>If a submission has \\(N\\) predictions (label/confidence pairs) sorted in descending order by their confidence scores, then the Global Average Precision is computed as:</p>
<p>$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$</p>
<p>where:
</p><ul>
<li> \\(N\\) is the total number of predictions returned by the system, across all queries</li>
<li> \\(M\\) is the total number of queries with at least one landmark from the training set visible in it (note that some queries may not depict landmarks) </li>
<li> \\(P(i)\\) is the precision at rank \\(i\\)</li>
<li> \\(rel(i)\\) denotes the relevance of prediciton \\(i\\): it’s 1 if the \\(i\\)-th prediction is correct, and 0 otherwise</li>
</ul>
<p>[1] F. Perronnin, Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,"" Proc. CVPR'09</p>
<p>[2] T. Weyand, A. Araujo, B. Cao and J. Sim, ""Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,"" Proc. CVPR'20</p>

<h2>Submission File</h2>
<p>For each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some images contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. The submission file should contain a header and have the following format (larger scores denote more confident matches):</p>
<pre>id,landmarks
000088da12d664db,8815 0.03
0001623c6d808702,
0001bbb682d45002,5328 0.5
etc.</pre>"
OSIC Pulmonary Fibrosis Progression,Predict lung function decline,https://www.kaggle.com/competitions/osic-pulmonary-fibrosis-progression,https://storage.googleapis.com/kaggle-competitions/kaggle/20604/logos/header.png?t=2020-05-06-19-02-45,"Image,Healthcare",2097,2530,44505,"<p>Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That’s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren’t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2F4f30bdd2ffddda01f8ec5d857171ca93%2FKaggleCompetition_Description_700x700_option1-orig.png?generation=1588791895230128&alt=media"" style=""float: right; width: 275px"">

<p>Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety—in addition to fibrosis-related symptoms—from the disease’s opaque path of progression.

<p><a href=""https://www.osicild.org/"" target=""_new"">Open Source Imaging Consortium (OSIC)</a> is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.

<p>In this competition, you’ll predict a patient’s severity of decline in lung function based on a CT scan of their lungs. You’ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.

<p>If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.

<br>
<blockquote><b>This is a Code Competition. Refer to <a href=""https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression#Code-Requirements"" target = ""blank"">Code Requirements</a> for details.</b></blockquote>","This competition is evaluated on a modified version of the Laplace Log Likelihood. In medical applications, it is useful to evaluate a model's confidence in its decisions. Accordingly, the metric is designed to reflect both the accuracy and certainty of each prediction.

For each true FVC measurement, you will predict both an FVC and a confidence measure (standard deviation \\( \sigma \\)). The metric is computed as:

$$ \sigma\_{clipped} = max(\sigma, 70), $$

$$ \Delta = min \( |FVC\_{true} - FVC\_{predicted}|, 1000 \), $$

$$ metric = -   \frac{\sqrt{2} \Delta}{\sigma\_{clipped}} - \ln \( \sqrt{2} \sigma\_{clipped} \).  $$

The error is thresholded at 1000 ml to avoid large errors adversely penalizing results, while the confidence values are clipped at 70 ml to reflect the approximate measurement uncertainty in FVC. The final score is calculated by averaging the metric across all test set `Patient_Week`s (three per patient). Note that metric values will be negative and higher is better.

## Submission File

For each `Patient_Week`, you must predict the `FVC`  and a confidence. To avoid potential leakage in the timing of follow up visits, you are asked to predict every patient's `FVC` measurement for every possible week. Those weeks which are not in the final three visits are ignored in scoring. 

The file should contain a header and have the following format:

    Patient_Week,FVC,Confidence
    ID00002637202176704235138_1,2000,100
    ID00002637202176704235138_2,2000,100
    ID00002637202176704235138_3,2000,100
    etc.
"
Google Landmark Retrieval 2020,"Given an image, can you find all of the same landmarks in a dataset?",https://www.kaggle.com/competitions/landmark-retrieval-2020,https://storage.googleapis.com/kaggle-competitions/kaggle/19233/logos/header.png?t=2020-06-24-19-50-33,"Image,Computer Vision",541,665,4983,"Welcome to the third Landmark Retrieval competition! This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets. 

Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.

In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the [Landmark Recognition Challenge 2020](https://kaggle.com/c/landmark-recognition-2020). Both challenges will be discussed at the [Instance-Level Recognition workshop](https://ilr-workshop.github.io/ECCVW2020/) in ECCV’20.

In the previous editions of this challenge ([2018](https://www.kaggle.com/c/landmark-retrieval-challenge) and  [2019](https://www.kaggle.com/c/landmark-retrieval-2019)), submissions were handled by uploading prediction files to the system. This year's competition is structured in a representation learning format: rather than creating a submission file with retrieved images, you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality with mean average precision.


> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/landmark-retrieval-2020/overview/code-requirements) for details.**
","Submissions are evaluated according to mean Average Precision @ 100 (\\(mAP@100\\)):

$$mAP@100 = \frac{1}{Q} \sum\_{q=1}^{Q} \frac{1}{min(m\_q, 100)} \sum\_{k=1}^{min(n\_q,100)} P\_q(k) rel_q(k)$$

where:

- \\(Q\\) is the number of query images
- \\(m\_q\\) is the number of index images containing a landmark in common with the query image \\(q\\) (note that \\(m_q \gt 0\\))
- \\(n_q\\) is the number of predictions made by the system for query \\(q\\)
- \\(P_q(k)\\) is the precision at rank \\(k\\) for the \\(q\\)-th query
- \\(rel_q(k)\\) denotes the relevance of prediciton \\(k\\) for the \\(q\\)-th query: it’s 1 if the \\(k\\)-th prediction is correct, and 0 otherwise

## Submission file

Unlike a traditional Kaggle code competition where notebooks are rerun top-to-bottom on a private test set, in this competition you will be submitting a model file. Please refer to the [data page](https://www.kaggle.com/c/landmark-retrieval-2020/data) for complete details. In most cases, scoring is expected to take a few hours to complete.
"
Petals to the Metal - Flower Classification on TPU,Getting Started with TPUs on Kaggle!,https://www.kaggle.com/competitions/tpu-getting-started,https://storage.googleapis.com/kaggle-competitions/kaggle/21154/logos/header.png?t=2020-06-04-00-33-35,"Image,Multiclass Classification",144,156,556,"<h3>Learn how to use Tensor Processing Units (TPUs) on Kaggle</i></h3>
<p>TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.</p>
<p>TPU quotas are available on Kaggle at no cost to users.</p>
<p>Watch the video below to see how to get started!  You can follow along with <a href=""https://www.kaggle.com/philculliton/a-simple-petals-tf-2-2-notebook"" target=""_blank"">this notebook</a>.</p>
<a href=""https://www.youtube.com/watch?v=1pdwRQ1DQfY&amp;feature=youtu.be"" target=""_blank"">
  <img width=""699"" align=""center"" src=""https://i.imgur.com/ANlTHUw.png"">
</a>

<h3>The Challenge</h3>
<p>It’s difficult to fathom just how vast and diverse our natural world is.</p>
<p>There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over <b>400,000</b> different types of flowers.</p>
<p>In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).</p>
<div class=""note"">
<strong>Recommended Tutorial</strong><br>
We highly recommend <a href=""https://www.kaggle.com/ryanholbrook/create-your-first-submission"">Ryan Holbrook’s Tutorial</a> that walks you through making your very first submission step by step.
</div>
<h3>Have Questions?</h3>
<p>Kaggle Data Scientists will be actively monitoring the competition forum - your fellow data scientists and TPU users will be there too! If you have a question or need help troubleshooting, that’s the best place to find help.</p>
<p></p>
<h3>Learn More</h3>
<p>Check out <a href=""https://www.youtube.com/playlist?list=PLqFaTIg4myu-1c3ygYzakW8-hNzQG59-5""> Kaggle’s Youtube playlist</a> for more videos introducing TPUs.</p>
<p>Read the <a href=""https://www.kaggle.com/docs/tpu"">TPU documentation</a> for more information and resources.</p>
<p><i>Many thanks to <a href=""https://twitter.com/martin_gorner?lang=en"" target=""_blank"">Martin Görner</a>, Google Developer Advocate and author of <a href=""https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd"" target=""_blank"">Tensorflow without a PhD</a> for his tireless work on the dataset, the notebooks, and the original competition that this Getting Started competition draws from.</i></p>","Submissions are evaluated on [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).

F1 is calculated as follows:

$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$


In ""macro"" F1 a separate F1 score is calculated for each class / label and then averaged.


## Submission File
For each `id` in the test set, you must predict a type of flower (or `label`). The file should contain a header and have the following format:

```
id,label
a762df180,0
24c5cf439,0
7581e896d,0
eb4b03b29,0
etc.
```"
Cornell Birdcall Identification,Build tools for bird population monitoring,https://www.kaggle.com/competitions/birdsong-recognition,https://storage.googleapis.com/kaggle-competitions/kaggle/19596/logos/header.png?t=2020-05-29-00-25-08,Audio,1390,1630,17938,"Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. With proper sound detection and classification, researchers could automatically intuit factors about an area’s quality of life based on a changing bird population.

<img title=”Bewick's Wren © Derek Hameister / Macaulay Library at the Cornell Lab of Ornithology ML214764391” src=""https://storage.googleapis.com/kaggle-media/competitions/Birdsong/Bewick's%20Wren%20%C2%A9%20Derek%20Hameister%20_%20Macaulay%20Library%20at%20the%20Cornell%20Lab%20of%20Ornithology%20ML214764391.png"" style=""float: right; width: 250px"">

There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar. 

To unlock the full potential of these extensive and information-rich sound archives, researchers need good machine listeners to reliably extract as much information as possible to aid data-driven conservation.

The [Cornell Lab of Ornithology’s Center for Conservation Bioacoustics](https://www.birds.cornell.edu/ccb/) (CCB)’s mission is to collect and interpret sounds in nature. The CCB develops innovative conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By partnering with the data science community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses.

In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings!

If successful, your work will help researchers better understand changes in habitat quality, levels of pollution, and the effectiveness of restoration efforts. Reliable machine listeners would also allow conservationists to deploy more recording units worldwide and would enable data-driven conservation at a scale not yet possible. The eventual conservation outcomes could greatly improve the quality of life for many living organisms—birds and human beings included.

","Submissions will be evaluated based on their row-wise micro averaged [F1 score](https://en.wikipedia.org/wiki/F1_score).

For each `row_id`/time window, you need to provide a space separated list of the set of unique `birds` that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code `nocall`.

There are three sites in the test set. Sites 1 and 2 are labeled in 5 second increments, while site 3 was labeled per audio file due to the time consuming nature of the labeling process.

The submission file must have a header and should look like the following:

## Submission File
```
row_id,birds
site_1_0a997dff022e3ad9744d4e7bbf923288_5,amecro
site_1_0a997dff022e3ad9744d4e7bbf923288_10,amecro amerob
site_1_0a997dff022e3ad9744d4e7bbf923288_15,nocall
```
"
Halite by Two Sigma,Collect the most halite during your match in space,https://www.kaggle.com/competitions/halite,https://storage.googleapis.com/kaggle-competitions/kaggle/18011/logos/header.png?t=2019-12-20-19-33-21,"Simulations,Video Games",1139,1291,15037,"Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory?

Halite by [Two Sigma](https://www.twosigma.com/) (""Halite"") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3258%2F73a73a0b4a807a7a9e674a40c55f7396%2Fhalite.gif?generation=1594994852379393&alt=media"" style = ""display: block;margin: 50px auto;"">

Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment.

Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players!

So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!  ","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.

Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.

When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.

We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission, with an additional slight rate increase for the newest-submitted Episodes to give you feedback faster.

After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.

Since Halite is a four-player game the resolution is a little more complex. We treat a four-agent episode as 6 two-agent episodes (agent 1 vs agent 2, agent 1 vs agent 3, agent 2 vs agent 3, etc) and calculate the standard two-player score update for each pair, then we average those updates to calculate each agent's final ranking updates. 

At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run games. At the conclusion of this week, the leaderboard is final."
SIIM-ISIC Melanoma Classification,Identify melanoma in lesion images,https://www.kaggle.com/competitions/siim-isic-melanoma-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/20270/logos/header.png?t=2020-05-06-18-21-24,Image,3308,4110,101845,"Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective.

Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account “contextual” images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work.

As the leading healthcare organization for informatics in medical imaging, the [Society for Imaging Informatics in Medicine (SIIM)](https://siim.org/)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the [International Skin Imaging Collaboration (ISIC)](https://www.isic-archive.com/), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions.

In this competition, you’ll identify melanoma in images of skin lesions. In particular, you’ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.

Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `image_name` in the test set, you must predict the probability (`target`) that the sample is **malignant**. The file should contain a header and have the following format:

    image_name,target
    ISIC_0052060,0.7
    ISIC_0052349,0.9
    ISIC_0058510,0.8
    ISIC_0073313,0.5
    ISIC_0073502,0.5
    etc.
"
TREC-COVID Information Retrieval,Build a pandemic document retrieval system,https://www.kaggle.com/competitions/trec-covid-information-retrieval,https://storage.googleapis.com/kaggle-competitions/kaggle/20829/logos/header.png?t=2020-05-19-03-00-05,"NLP,Coronavirus,Text,Text Mining",19,19,66,">  **LAUNCHED**
This competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the [Data page](https://www.kaggle.com/c/trec-covid-information-retrieval/data) for more details.

Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems.

Kaggle first teamed up with the Allen Institute for AI in the launch of the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis. 

This is the **3rd Round of the TREC-COVID Challenge**. Prior runs were hosted directly on the [TREC-COVID Site](https://ir.nist.gov/covidSubmit/data.html). For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (""runs"") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document).  The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020.

Following the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment.

With your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments.

### Acknowledgments

The [Text REtrieval Conference (TREC)](http://trec.nist.gov/) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.

The TREC-COVID Challenge is being organized by the [Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/), the [National Institute of Standards and Technology (NIST)](http://nist.gov/), the [National Library of Medicine (NLM)](https://www.nlm.nih.gov/), [Oregon Health and Science University (OHSU)](http://ohsu.edu/informatics/), and the [University of Texas Health Science Center at Houston (UTHealth)](https://sbmi.uth.edu/).

See the [NIST press release](https://www.nist.gov/news-events/news/2020/04/nist-and-ostp-launch-effort-improve-search-engines-covid-19-research) for more information.

","## Metric

Submissions will be evaluated using [NDCG](http://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) (Normalized Discounted Cumulative Gain).

## Relevance Judgements

The relevance judgments will be made by human annotators that have biomedical expertise. Annotators will use a three-way scale:
 - **Relevant:** the article is fully responsive to the information need as expressed by the topic, i.e. answers the Question in the topic. The article need not contain all information on the topic, but must, on its own, provide an answer to the question.
 - **Partially Relevant:** the article answers part of the question but would need to be combined with other information to get a complete answer.
 - **Not Relevant:** everything else.

## Submission File
The submission should be a list of `topicid`-`docid` ranked from top to bottom according to their relevance (i.e., the top `docid` is the most relevant document for a the specified topic). Each line should be a single `topicid`-`docid`.

The columns are as follows:

 - `topicid` - is the topic number (1..40)
 - `docid` - the `cord_uid` of the document retrieved in this position. It must be a valid `cord_id` in the May 19 release of CORD-19. If it has already been judged for this topic, it will be removed.

### Example
    topicid,docid
    1,000ajevz
    1,000q5l5n
    ...
    1,000tfenb"
Open Images Instance Segmentation RVC 2020 edition,Outline segmentation masks of objects in images,https://www.kaggle.com/competitions/open-images-instance-segmentation-rvc-2020,https://storage.googleapis.com/kaggle-competitions/kaggle/20009/logos/header.png?t=2020-05-04-14-48-29,Image,18,26,27,"#Introduction

Computer vision has advanced considerably but is still challenged in matching the precision of human perception.

[Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.


This year the Open Images  Instance Segmentation competition is a part of the larger [Robust Vision Challenge 2020](http://www.robustvision.net). This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the [RVC 2020 page](http://www.robustvision.net) and the [Open Images Challenge page](https://storage.googleapis.com/openimages/web/challenge_overview.html) for more details.
![http://www.robustvision.net](http://www.robustvision.net/images/banner_rvc2020_loop.gif)

Participants are also welcome to submit to this playground competition beyond the context of RVC.

#Instance Segmentation Track
In this track of the Challenge, you are asked **to provide segmentation masks of objects**.

This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art [interactive segmentation process](https://arxiv.org/pdf/1903.10830.pdf), where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.

<img alt=""wuxipark"" src=""https://storage.googleapis.com/kaggle-media/competitions/open-images/wuxipark.png"" width=""45%"" > <img alt=""catcafe"" src=""https://storage.googleapis.com/kaggle-media/competitions/open-images/catcafe.png"" width=""45%"" >

*Example train set annotations. Left: [Wuxi science park, 1995](https://www.flickr.com/photos/garysoup/3777131020) by [Gary Stevens](https://www.flickr.com/people/garysoup/). Right: [Cat Cafe Shinjuku calico](https://www.flickr.com/photos/picsoflife/6776736950) by [Ari Helminen](https://www.flickr.com/people/picsoflife/). Both images used under CC BY 2.0 license.*

<p/>
The training data, format, and submission modalities are identical to the [2019 Open Images Challenge](https://www.kaggle.com/c/open-images-2019-instance-segmentation/overview/).


","
Submissions are evaluated by computing mean [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Average_precision), with the mean taken over the [300 segmentable classes of the challenge](https://storage.googleapis.com/openimages/challenge_2019/challenge-2019-classes-description-segmentable.csv). It follows the same spirit as the [Object Detection evaluation](https://www.kaggle.com/c/open-images-2019-object-detection/overview/evaluation), but takes into account mask-to-mask matching. The metric is [described in detail here](https://storage.googleapis.com/openimages/web/evaluation.html#instance_segmentation_eval). See also [this tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md#instance-segmentation-track) on running the evaluation in Python.

<p/>
Note that the [RVC Challenge ranks participants in a separate leaderboard](http://www.robustvision.net/leaderboard.php?benchmark=instance) which integrates results across multiple benchmarks, including this competition.

## Submission File

For each image in the test set, you must predict a list of instance segmentation masks and their associated detection score (`Confidence`). The submission csv file uses the following format:

<pre><code>ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2 ...
ImageBID,ImageBWidth,ImageBHeight,LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 …
</code></pre>

A sample with real values would be:
<pre><code>ImageID,ImageWidth,ImageHeight,PredictionString
721568e01a744247,1118,1600,/m/018xm 0.637833 eNqLi8xJM7BOTjS08DT2NfI38DfyM/Q3NMAJgJJ+RkBs7JecF5tnAADw+Q9I
7b018c5e3a20daba,1600,1066,/m/01g317 0.85117 eNqLiYrLN7DNCjDMMIj0N/Iz9DcwBEIDfyN/QyA2AAsBRfxMPcKTA1MMADVADIo=
</code></pre>


The binary segmentation masks are [run-length encoded](https://en.wikipedia.org/wiki/Run-length_encoding) (RLE), [zlib](https://en.wikipedia.org/wiki/Zlib) compressed, and [base64](https://en.wikipedia.org/wiki/Base64) encoded to be used in text format as `EncodedMask`. Specifically, we use the Coco masks RLE encoding/decoding (see the `encode` method of [COCO’s mask API](http://cocodataset.org/#download)), the zlib compression/decompression ([RFC1950](https://www.ietf.org/rfc/rfc1950.txt)), and vanilla base64 encoding.
  
An example python function to encode an instance segmentation mask would be:

<pre><code>import base64
import numpy as np
from pycocotools import _mask as coco_mask
import typing as t
import zlib


def encode_binary_mask(mask: np.ndarray) -> t.Text:
  """"""Converts a binary mask into OID challenge encoding ascii text.""""""
  
  # check input mask --
  if mask.dtype != np.bool:
    raise ValueError(
        ""encode_binary_mask expects a binary mask, received dtype == %s"" %
        mask.dtype)
  
  mask = np.squeeze(mask)
  if len(mask.shape) != 2:
    raise ValueError(
        ""encode_binary_mask expects a 2d mask, received shape == %s"" %
        mask.shape)
   
  # convert input mask to expected COCO API input --
  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
  mask_to_encode = mask_to_encode.astype(np.uint8)
  mask_to_encode = np.asfortranarray(mask_to_encode)
  
  # RLE encode mask --
  encoded_mask = coco_mask.encode(mask_to_encode)[0][""counts""]
  
  # compress and base64 encoding --
  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
  base64_str = base64.b64encode(binary_str)
  return base64_str
</code></pre>

(This code is [available as a gist here](https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c).)

There is a 5Gb file size limit on the submission csv file. This implicitly limits the number of detections to about 50~100 per image (on average).
"
Open Images Object Detection RVC 2020 edition,Detect objects in varied and complex images,https://www.kaggle.com/competitions/open-images-object-detection-rvc-2020,https://storage.googleapis.com/kaggle-competitions/kaggle/19205/logos/header.png?t=2020-05-04-14-48-59,Image,89,107,224,"#Introduction

Computer vision has advanced considerably but is still challenged in matching the precision of human perception.

[Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.

This year the Open Images Object Detection competition is a part of the larger [Robust Vision Challenge 2020](http://www.robustvision.net). This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the [RVC 2020 page](http://www.robustvision.net) and the [Open Images Challenge page](https://storage.googleapis.com/openimages/web/challenge_overview.html) for more details.
![http://www.robustvision.net](http://www.robustvision.net/images/banner_rvc2020_loop.gif)

Participants are also welcome to submit to this playground competition beyond the context of RVC.

#Object Detection Track
In this track, you are asked **to predict a tight bounding box around object instances**.

The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).

![guitar](https://storage.googleapis.com/kaggle-media/competitions/open-images/guitarist.png) ![house](https://storage.googleapis.com/kaggle-media/competitions/open-images/table.png)

*Example annotations. Left: [Mark Paul Gosselaar plays the guitar](https://www.flickr.com/photos/rhysasplundh/5738556102) by [Rhys A](https://www.flickr.com/people/rhysasplundh/). Right: [the house](https://www.flickr.com/photos/krakluski/2950388100) by [anita kluska](https://www.flickr.com/photos/krakluski/). Both images used under CC BY 2.0 license.*

The training data, format, and submission modalities are identical to the [2019 Open Images Challenge](https://www.kaggle.com/c/open-images-2019-object-detection).","Submissions on Open Images Object Detection competition are evaluated by computing mean [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision) (mAP), modified to take into account the annotation process of [Open Images dataset](https://storage.googleapis.com/openimages/web/index.html) (mean is taken over per-class APs). The metric is described on the [Open Images Challenge website](https://storage.googleapis.com/openimages/web/evaluation.html#object_detection_eval), and is  implemented as a part of [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). See [this Tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md) on running the evaluation in Python.

The final mAP is computed as the average AP over the 500 classes. The participants  leaderboard will be ranked on this final metric. 

Note that the [RVC Challenge ranks participants in a separate leaderboard](http://www.robustvision.net/leaderboard.php?benchmark=object) which integrates results across multiple benchmarks, including this competition.
 

## Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as 

    ImageID,PredictionString
    ImageID,{Label Confidence XMin YMin XMax YMax} {...}
"
COVID19 Global Forecasting (Week 5),Forecast daily COVID-19 spread in regions around world,https://www.kaggle.com/competitions/covid19-global-forecasting-week-5,https://storage.googleapis.com/kaggle-competitions/kaggle/20475/logos/header.png?t=2020-04-30-15-50-51,"Coronavirus,Tabular",173,93,688,"*This is week 5 of Kaggle's COVID-19 forecasting series, following the [Week 4 competition](https://www.kaggle.com/c/covid19-global-forecasting-week-4). This competition has some changes from prior weeks - be sure to check the [Evaluation](https://www.kaggle.com/c/covid19-global-forecasting-week-5/overview/evaluation) and [Data](https://www.kaggle.com/c/covid19-global-forecasting-week-5/data) pages for more details. All of the prior discussion forums have been migrated to this competition for continuity.*

### Background

The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1).

### The Challenge

Kaggle is launching a companion COVID-19 forecasting challenges to help answer a [subset of the NASEM/WHO questions](https://www.kaggle.com/c/covid19-global-forecasting-week-4/overview/open-scientific-questions). While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal **isn't only to produce accurate forecasts**. It’s also to identify factors that appear to impact the transmission rate of COVID-19.

You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. 

As the data becomes available, we will update the leaderboard with live results based on [data made available](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).

We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. 

### Companies and Organizations

**There is also a call to action for companies and other organizations:** If you have datasets that might be useful, please upload them to [Kaggle’s dataset platform](https://www.kaggle.com/datasets) and reference them in [this forum thread](https://www.kaggle.com/c/covid19-global-forecasting-week-4/discussion/137078). That will make them accessible to those participating in this challenge and a resource to the wider scientific community.

### Acknowledgements

JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/covid19-global-forecasting-week-4/overview) for details.**
","##Public and Private Leaderboard

To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-04-27 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.

- **Public Leaderboard Period**: 2020-04-27 - 2020-05-11
- **Private Leaderboard Period**: 2020-05-13 - 2020-06-10

##Evaluation

Submissions are scored using the **Weighted Pinball Loss**.

$$
\text{score} =  \frac{1}{N\_{f}} \sum\_{f} w\_{f} \frac{1}{N\_{\tau}} \sum\_{\tau} L\_{\tau}(y\_i,\hat{y}_\{i})
$$

where:
$$
\begin{eqnarray}
 L_{\tau}(y,\hat{y}) &amp; = &amp; (y - \hat{y}) \tau &amp; \textrm{ if } y \geq \hat{y} \\\
&amp; = &amp; (\hat{y} - y) (1 - \tau) &amp; \textrm{ if } \hat{y} &gt; y
\end{eqnarray}
$$

and:
 - \\(y \\) is the ground truth value
 - \\(\hat{y} \\) is the predicted value
 - \\(\tau \\) is the quantile to be predicted, e.g., one of `[0.05, 0.50, 0.95]`
 - \\(N\_{f}\\) is the total number of forecast (\\(f\\)) `day x target` combinations
 - \\(N\_{\tau} \\) is the total number of quantiles to predict
 - \\( w\\) is a weighting factor

Weights are calculated as follows:
 - `ConfirmedCases`:  \\(\log(\text{population}+1)^{-1}\\)
 - `Fatalities`:  \\(10  \cdot \log(\text{population}+1)^{-1}\\)
 
## Submission File

For each `ForecastId` in the test set, you'll predict the 0.05, 0.50, and 0.95 quantiles for *daily* COVID-19 cases and fatalities to date. The file should contain a header and have the following format:

    ForecastId_Quantile,TargetValue
    1_0.05,1
    1_0.50,1
    1_0.95,1
    2_0.05,1
    etc.

You will get the `ForecastId_Quantile` for the corresponding date and location from the `test.csv` file.
"
Global Wheat Detection ,Can you help identify wheat heads using image analysis?,https://www.kaggle.com/competitions/global-wheat-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/19989/logos/header.png?t=2020-04-20-18-13-31,"Image,Plants",2245,2269,8187,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/UofS-Wheat/descriptionimage.png"" alt=""Wheat Head"" width=""300"" style=""float: right""></p>

Open up your pantry and you’re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of ""wheat heads""—spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields.

However, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains.

The [Global Wheat Head Dataset](http://www.global-wheat.com/2020-challenge/) is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l’agriculture, l’alimentation et l’environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen.


In this competition, you’ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China.

Wheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.


> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/global-wheat-classification#Code-Requirements) for details.**","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.75 with a step size of 0.05. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object.  A false negative indicates a ground truth object had no associated predicted object.

**Important note:** if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.

The average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$

In your submission, you are also asked to provide a `confidence` level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.

Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.

## Intersection over Union (IoU)

Intersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects).  It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.

It can be visualized as the following:

![Image of Intersection over Union][1]

The two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together.  IoU would be low - and would likely not count as a ""hit"" at higher IoU thresholds.

## Submission File

The submission format requires a space delimited set of bounding boxes. For example:

`ce4833752,0.5 0 0 100 100`

indicates that image `ce4833752` has a bounding box with a `confidence` of 0.5, at `x` == 0 and `y` == 0, with a `width` and `height` of 100.

The file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.

    image_id,PredictionString
    ce4833752,1.0 0 0 50 50
    adcfa13da,1.0 0 0 50 50
    6ca7b2650,
    1da9078c1,0.3 0 0 50 50 0.5 10 10 30 30
    7640b4963,0.5 0 0 50 50
    etc...


  [1]: https://storage.googleapis.com/kaggle-media/competitions/rsna/IoU.jpg
"
Hash Code Archive - Photo Slideshow Optimization,Optimizing a photo album from Hash Code 2019,https://www.kaggle.com/competitions/hashcode-photo-slideshow,https://storage.googleapis.com/kaggle-competitions/kaggle/18685/logos/header.png?t=2020-04-24-16-15-10,"Optimization,Internet",89,110,405,"Note: Put your heads together to solve programming challenges. Google's coding competition, [Hash Code](https://codingcompetitions.withgoogle.com/hashcode), has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions!

As the saying goes, ""a picture is worth a thousand words.""  We agree – photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story they’re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece.

Approximately 2.5 billion people around the world carry a camera – in the form of a smartphone – in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection.

Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the [evaluation](https://www.kaggle.com/c/hashcode-photo-slideshow/overview/evaluation) section explains what we mean by “interesting”)

Will your slideshow tell a good story or be a major snoozefest?
","### Scoring

The slideshow is scored based on how interesting the transitions between each pair of subsequent (neighboring) slides are. We want the transitions to have something in common to preserve continuity (the two slides should not be totally different), but we also want them to be different enough to keep the audience interested. The similarity of two vertical photos on a single slide is not taken into account for the scoring function. This means that two photos can, but don't have to, have tags in common.

For two subsequent slides Si and Si+1, the interest factor is the minimum of:
- the number of common tags between Si and Si+1
- the number of tags in Si but not in Si+1
- the number of tags in Si+1 but not in Si.


### Submission Format
You must make your submission via a Kaggle notebook, and that notebook must output a submission file named **submission.txt**. Once the notebook has been saved and committed, you can file your submission from the output section of the results.

The output file must start with a single integer S (1 ≤ S ≤N)— the number of slides in the slideshow. This must be followed by S lines describing the individual slides. Each line should contain either:
- A single integer – ID of the single horizontal photo in the slide.
- Two integers separated by a single space – IDs of the two vertical photos in the slide in any order.
Each photo can be used only one time or not at all.

##Example
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2F8fd43b3cad0f25e9ecd5927c544dfb5c%2FScreen%20Shot%202020-04-24%20at%2011.43.06%20AM.png?generation=1587752487843161&alt=media)

To submit an album with three slides in total, including slides 0, 3, and the pair of 1 and 2:
```
3
0
3
1 2
```"
TReNDS Neuroimaging,"Multiscanner normative age and assessments prediction with brain function, structure, and connectivity",https://www.kaggle.com/competitions/trends-assessment-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/16245/logos/header.png?t=2019-10-28-01-13-44,"Computer Vision,Neuroscience",1047,1214,14309,"Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.
<img style = ""float:right"" width = ""310"" src = ""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1537731%2Fa5fdbe17ca91e6713d2880887232c81a%2FScreen%20Shot%202019-12-09%20at%2011.25.31%20AM.png?generation=1575920121028151&alt=media""/>

In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward.

The Tri-Institutional Georgia State University/Georgia Institute of Technology/Emory University [Center for Translational Research in Neuroimaging and Data Science (TReNDS)](http://trendscenter.org/) leverages advanced brain imaging to promote research into brain health. The organization is focused on developing, applying and sharing advanced analytic approaches and neuroinformatics tools. Among its [software projects](http://trendscenter.org/software/) are the GIFT and FIT neuroimaging toolboxes, the COINS data management system, and the COINSTAC toolkit for federated learning, all aimed at supporting data scientists and other neuroimaging researchers.

Making the leap from research to clinical application is particularly difficult in brain health. In order to translate to clinical settings, research findings have to be reproduced consistently and validated in out-of-sample instances. The problem is particularly well-suited for data science, but current approaches typically do not generalize well. With this large dataset and competition, your efforts could directly address an important area of brain research.

**Acknowledgments**
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2Faa4d99443bbf16acc44c5b66bd362c2a%2FOfHBM.png?generation=1587603203346450&alt=media)
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2F08d4f3ea426afd2fc2a370a294eaffe4%2FIEEE.png?generation=1587603262596610&alt=media)
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2F47c74960e6540f11287e1e271438e029%2FTReNDS.png?generation=1587603283379241&alt=media)","Submissions are scored using feature-weighted, normalized absolute errors.

$$
\text{score} = \sum\_f w\_f \left( \frac{\sum\_i \lvert y\_{f,i} - \hat{y}\_{f,i} \rvert}{\sum\_i \hat{y}\_{f,i}} \right)
$$

where \\(y\_{f,i}\\) is the \\(i^{th}\\) observation of feature \\(f\\), \\(\hat{y}\_{f,i}\\) is the corresponding ground truth for that observation, and \\(w\_f\\) is a weighting given to each feature. The weights are `[.3, .175, .175, .175, .175]` corresponding to features `[age, domain1_var1, domain1_var2, domain2_var1, domain2_var2]`.

A small percentage of values are missing from the ground truth. These are skipped in the calculation. You should, though, make predictions for every row in the submission file.

## Submission File
For each `Id` in the test set, you must make a corresponding prediction for each feature. The `Id` contains both the sample id as well as feature. The file should contain a header and have the following format:

    Id,Predicted
    10003_age,0
    10003_domain1_var1,50.0
    10003_domain1_var2,50.0
    10003_domain2_var1,50.0
    10003_domain2_var2,50.0
    etc.
"
Prostate cANcer graDe Assessment (PANDA) Challenge,Prostate cancer diagnosis using the Gleason grading system,https://www.kaggle.com/competitions/prostate-cancer-grade-assessment,https://storage.googleapis.com/kaggle-competitions/kaggle/18647/logos/header.png?t=2020-04-10-17-14-59,"Image,Medicine",1010,1290,19723,"With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually. The key to decreasing mortality is developing more precise diagnostics. Diagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, you will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available.

The grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis. 

Automated deep learning systems have shown some promise in accurately grading PCa. Recent research, including two studies independently conducted by the groups hosting this challenge, have shown that these systems can achieve pathologist-level performance. However, these systems/results were not tested with multi-center datasets at scale.    

Your work here will improve on these efforts using the most extensive multi-center dataset on Gleason grading yet. The training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, roughly 8 times the size of the [CAMELYON17](https://camelyon17.grand-challenge.org/) challenge, one of the largest digital pathology datasets and best known challenges in the field. Furthermore, in contrast to previous challenges, we are making full diagnostic biopsy images available. Using a sizable multi-center test set, graded by expert uro-pathologists, we will evaluate challenge submissions on their applicability to improve this critical diagnostic function. 

  <img src=""https://storage.googleapis.com/kaggle-media/competitions/PANDA/Screen%20Shot%202020-04-08%20at%202.03.53%20PM.png"">

**Figure 1**: An illustration of the Gleason grading process for an example biopsy containing prostate cancer. The most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge.

**[Radboud University Medical Center](https://www.radboudumc.nl/en/research)** and **[Karolinska Institute](https://ki.se/en/meb)** have teamed up to organize this competition in collaboration with colleagues from Tampere University. The [Computational Pathology Group (CPG)](https://www.computationalpathologygroup.eu/) of the Radboud University Medical Center is a research group that develops computer algorithms to aid clinicians. Karolinska Institute’s Department of Medical Epidemiology and Biostatistics (MEB) includes an interdisciplinary research group to improve the diagnostics and treatment of prostate cancer. Together, they hope to further their existing research to make a significant impact on the healthcare of prostate cancer patients.

Challenge organizer team: Wouter Bulten, Geert Litjens, Hans Pinckaers, Peter Ström, Martin Eklund, Lars Egevad, Henrik Grönberg, Kimmo Kartasalo, Pekka Ruusuvuori, Tomi Häkkinen, Sohier Dane, Maggie Demkin.
   

<a href=""https://www.radboudumc.nl/english/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/PANDA/radboudumclogo.png"" style=""width: 250px""></a><a href=""https://ki.se/en""><img src=""https://storage.googleapis.com/kaggle-media/competitions/PANDA/ki_logo_rgb.png"" style=""width: 250px""></a>

###Sponsors   


The PANDA workshop at MICCAI 2020 is sponsored by ContextVision, Ibex and Google.   


<img src=""https://storage.googleapis.com/kaggle-media/competitions/PANDA/ContextVision-Logo-Small-RGB.jpg"" style=""width: 100px""></a><img src=""https://storage.googleapis.com/kaggle-media/competitions/PANDA/ibex_logo300x300.jpg"" style=""width: 100px""></a><img src=""https://storage.googleapis.com/kaggle-media/competitions/PANDA/Google%20Logo.png""  style=""width: 100px""></a>

###Published results
The paper on the PANDA challenge has been published as Open Access in Nature Medicine. In the paper, we took a deep dive into the solutions, tested the methods to see if they generalize well to unseen data, and performed a comparison with pathologists. You can read the full paper and all results here:

https://www.nature.com/articles/s41591-021-01620-2

    Bulten, W., Kartasalo, K., Chen, PH.C. et al. Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge. Nat Med (2022). https://doi.org/10.1038/s41591-021-01620-2

###Using the data outside of the competition
With the paper's publication, the embargo on the data is now lifted (see [forum post](https://www.kaggle.com/competitions/prostate-cancer-grade-assessment/discussion/300840)). If you want, you can now use the dataset for further scientific work and publish your results on the dataset. If you do so, please take the license (CC BY-SA-NC 4.0) into account (non-commercial) and make sure you cite the PANDA paper. The test sets will not be made public at this time, to allow further late submissions to be used for benchmarking algorithms. We are looking forward to seeing new scientific projects coming out of this dataset! ","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.

The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix <em>O</em> is constructed, such that <em>O<sub>i,j</sub></em> 
corresponds to the number of `isup_grade`s <em>i</em> (actual) that received a predicted value <em>j</em>. An <em>N-by-N </em>matrix of weights, <em>w</em>, 
is calculated based on the difference between actual and predicted values:

$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$

An <em>N-by-N</em> histogram matrix of expected outcomes, <em>E</em>, is calculated assuming that there is no correlation between values.  
This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that <em>E</em> and <em>O</em> have the same sum.

From these three matrices, the quadratic weighted kappa is calculated as: 

$$\kappa=1-\frac{\sum\_{i,j}w\_{i,j}O\_{i,j}}{\sum\_{i,j}w\_{i,j}E\_{i,j}}.$$

## Submission File
You must predict the `isup_grade` for each `image_id`. 
The submission file must have a header and should look like the following:

    image_id,isup_grade
    test_1,0
    test_2,1
    etc.
"
COVID19 Global Forecasting (Week 4),Forecast daily COVID-19 spread in regions around world,https://www.kaggle.com/competitions/covid19-global-forecasting-week-4,https://storage.googleapis.com/kaggle-competitions/kaggle/20010/logos/header.png?t=2020-04-08-20-08-54,"Tabular,Coronavirus",472,388,1925,"*This is week 4 of Kaggle's COVID-19 forecasting series, following the [Week 3 competition](https://www.kaggle.com/c/covid19-global-forecasting-week-3). This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity.*

### Background

The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1).

### The Challenge

Kaggle is launching a companion COVID-19 forecasting challenges to help answer a [subset of the NASEM/WHO questions](https://www.kaggle.com/c/covid19-global-forecasting-week-4/overview/open-scientific-questions). While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal **isn't only to produce accurate forecasts**. It’s also to identify factors that appear to impact the transmission rate of COVID-19.

You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. 

As the data becomes available, we will update the leaderboard with live results based on [data made available](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).

We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. 

### Companies and Organizations

**There is also a call to action for companies and other organizations:** If you have datasets that might be useful, please upload them to [Kaggle’s dataset platform](https://www.kaggle.com/datasets) and reference them in [this forum thread](https://www.kaggle.com/c/covid19-global-forecasting-week-4/discussion/137078). That will make them accessible to those participating in this challenge and a resource to the wider scientific community.

### Acknowledgements

JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/covid19-global-forecasting-week-4/overview) for details.**
","##Public and Private Leaderboard

To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-04-1 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.

- **Public Leaderboard Period** - 2020-04-01 - 2020-04-15
- **Private Leaderboard Period** - 2020-04-16 - 2020-05-14

##Evaluation

Submissions are evaluated using the column-wise <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">root mean squared  logarithmic error</a>.
<p>The RMSLE for a single column calculated as</p>
<p>$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },<br />$$</p>
<p>where:</p>
<p>\\(n\\) is the total number of observations <br />\\(p_i\\) is your prediction<br />\\(a_i\\) is the actual value <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<p>The final score is the mean of the RMSLE over all columns (in this case, 2).</p>

## Submission File

*We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold*.

For each `ForecastId` in the test set, you'll predict the *cumulative* COVID-19 cases and fatalities to date. The file should contain a header and have the following format:

    ForecastId,ConfirmedCases,Fatalities
    1,10,0
    2,10,0
    3,10,0
    etc.

You will get the `ForecastId` for the corresponding date and location from the `test.csv` file."
COVID19 Global Forecasting (Week 3),Forecast daily COVID-19 spread in regions around world,https://www.kaggle.com/competitions/covid19-global-forecasting-week-3,https://storage.googleapis.com/kaggle-competitions/kaggle/19853/logos/header.png?t=2020-04-01-21-45-38,"Tabular,Coronavirus",452,290,1408,"**[This week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version, and make a submission there.](https://www.kaggle.com/c/covid19-global-forecasting-week-4)**

*This is week 3 of Kaggle's COVID19 forecasting series, following the [Week 2 competition](https://www.kaggle.com/c/covid19-global-forecasting-week-2). This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity.*

### Background

The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1).

### The Challenge

Kaggle is launching a companion COVID-19 forecasting challenges to help answer a [subset of the NASEM/WHO questions](https://www.kaggle.com/c/covid19-global-forecasting-week-3/overview/open-scientific-questions). While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal **isn't only to produce accurate forecasts**. It’s also to identify factors that appear to impact the transmission rate of COVID-19.

You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. 

As the data becomes available, we will update the leaderboard with live results based on [data made available](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).

We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. 

### Companies and Organizations

**There is also a call to action for companies and other organizations:** If you have datasets that might be useful, please upload them to [Kaggle’s dataset platform](https://www.kaggle.com/datasets) and reference them in [this forum thread](https://www.kaggle.com/c/covid19-global-forecasting-week-3/discussion/137078). That will make them accessible to those participating in this challenge and a resource to the wider scientific community.

### Acknowledgements

JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/covid19-global-forecasting-week-3/overview) for details.**
","##Public and Private Leaderboard

To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-03-26 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.

- **Public Leaderboard Period** - 2020-03-26 - 2020-04-08
- **Private Leaderboard Period** - 2020-04-09 - 2020-05-07

##Evaluation

Submissions are evaluated using the column-wise <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">root mean squared  logarithmic error</a>.
<p>The RMSLE for a single column calculated as</p>
<p>$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },<br />$$</p>
<p>where:</p>
<p>\\(n\\) is the total number of observations <br />\\(p_i\\) is your prediction<br />\\(a_i\\) is the actual value <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<p>The final score is the mean of the RMSLE over all columns (in this case, 2).</p>

## Submission File

*We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold*.

For each `ForecastId` in the test set, you'll predict the *cumulative* COVID-19 cases and fatalities to date. The file should contain a header and have the following format:

    ForecastId,ConfirmedCases,Fatalities
    1,10,0
    2,10,0
    3,10,0
    etc.

You will get the `ForecastId` for the corresponding date and location from the `test.csv` file."
COVID19 Global Forecasting (Week 2),Forecast daily COVID-19 spread in regions around world,https://www.kaggle.com/competitions/covid19-global-forecasting-week-2,https://storage.googleapis.com/kaggle-competitions/kaggle/19702/logos/header.png?t=2020-03-26-06-02-18,"Coronavirus,Tabular",215,263,1368,"**[This week 2 forecasting task is now closed for submissions. Click here to visit the week 3 version, and make a submission there.](https://www.kaggle.com/c/covid19-global-forecasting-week-3)**

*This is week 2 of Kaggle's COVID19 forecasting series, following the [Week 1 competition](https://www.kaggle.com/c/covid19-global-forecasting-week-1). This is the 2nd of at least 4 competitions we plan to launch in this series.*

### Background

The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1).

### The Challenge

Kaggle is launching a companion COVID-19 forecasting challenges to help answer a [subset of the NASEM/WHO questions](https://www.kaggle.com/c/covid19-global-forecasting-week-2/overview/open-scientific-questions). While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal **isn't only to produce accurate forecasts**. It’s also to identify factors that appear to impact the transmission rate of COVID-19.

You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. 

As the data becomes available, we will update the leaderboard with live results based on [data made available](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).

We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. 

### Companies and Organizations

**There is also a call to action for companies and other organizations:** If you have datasets that might be useful, please upload them to [Kaggle’s dataset platform](https://www.kaggle.com/datasets) and reference them in [this forum thread](https://www.kaggle.com/c/covid19-global-forecasting-week-2/discussion/137078). That will make them accessible to those participating in this challenge and a resource to the wider scientific community.

### Acknowledgements

JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/covid19-global-forecasting-week-2/overview) for details.**
","##Public and Private Leaderboard

To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-03-19 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.

- **Public Leaderboard Period** - 2020-03-19 - 2020-04-01
- **Private Leaderboard Period** - 2020-04-02 - 2020-04-30

##Evaluation

Submissions are evaluated using the column-wise <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">root mean squared  logarithmic error</a>.
<p>The RMSLE for a single column calculated as</p>
<p>$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },<br />$$</p>
<p>where:</p>
<p>\\(n\\) is the total number of observations <br />\\(p_i\\) is your prediction<br />\\(a_i\\) is the actual value <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<p>The final score is the mean of the RMSLE over all columns (in this case, 2).</p>

## Submission File

*We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold*.

For each `ForecastId` in the test set, you'll predict the *cumulative* COVID-19 cases and fatalities to date. The file should contain a header and have the following format:

    ForecastId,ConfirmedCases,Fatalities
    1,10,0
    2,10,0
    3,10,0
    etc.

You will get the `ForecastId` for the corresponding date and location from the `test.csv` file."
Tweet Sentiment Extraction,Extract support phrases for sentiment labels,https://www.kaggle.com/competitions/tweet-sentiment-extraction,https://storage.googleapis.com/kaggle-competitions/kaggle/16295/logos/header.png?t=2019-12-12-19-46-30,"Text,Internet",2225,2817,37917,"*""My ridiculous dog is amazing.""* [sentiment: positive]

With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds.  But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.

 Help build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?

In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://www.figure-eight.com/data-for-everyone/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.

Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.

","The metric in this competition is the [word-level Jaccard score](https://en.wikipedia.org/wiki/Jaccard_index). A good description of Jaccard similarity for strings is [here](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50).

A Python implementation based on the links above, and matched with the output of the C# implementation on the back end, is provided below.

```
def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
```
The formula for the overall metric, then, is:
$$
score = \frac{1}{n} \sum_{i=1}^n jaccard( gt\_i, dt\_i )
$$
where:
$$
n = \textrm{number of documents}
$$
$$
jaccard = \textrm{the function provided above}
$$
$$
gt\_i = \textrm{the ith ground truth}
$$
$$
dt\_i = \textrm{the ith prediction}
$$

## Submission File
For each ID in the test set, you must predict the string that best supports the sentiment for the tweet in question. Note that the selected text _needs_ to be **quoted** and **complete** (include punctuation, etc. - the above code splits ONLY on whitespace) to work correctly. The file should contain a header and have the following format:

    textID,selected_text
    2,""very good""
    5,""I don't care""
    6,""bad""
    8,""it was, yes""
    etc.
"
Jigsaw Multilingual Toxic Comment Classification,Use TPUs to identify toxicity comments across multiple languages,https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/19018/logos/header.png?t=2020-02-20-19-20-37,"Text,Languages,TPU",1621,1922,36072,"It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by [Jigsaw](https://jigsaw.google.com/) and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything *rude, disrespectful or otherwise likely to make someone leave a discussion*. If these toxic contributions can be identified, we could have a safer, more collaborative internet.

In the previous 2018 [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge), Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the [Unintended Bias in Toxicity Classification Challenge](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification), you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of [Kaggle's new TPU support](https://www.kaggle.com/docs/tpu) and challenging you to build multilingual models with English-only training data.

Jigsaw's API, [Perspective](http://perspectiveapi.com/), serves toxicity models and others in a growing set of languages (see our [documentation](https://github.com/conversationai/perspectiveapi/blob/master/2-api/models.md#all-attribute-types) for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results ""translate"" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages. 

As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential.

*Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.*

<div class=""note"">
<strong>To get started with TPUs:</strong><br>
<ol>
<li>Read the <a href=""https://www.kaggle.com/docs/tpu"" target=""_blank"">TPU documentation one-pager</a></li>
<li>Then jump right into the <a href=""https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/overview/getting-started"" target=""_blank"">Getting Started Notebooks</a> for this competition</li>
</ol>
<p><i>Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.</i></p>
</div>
","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

## Submission File
For each ID in the test set, you must predict a probability for the `toxic` variable. The file should contain a header and have the following format:

> id,toxic
0,0.5
1,1
2,0
3,0.5
4,0.75
5,0.5
6,1
7,0.5
8,0
"
COVID19 Global Forecasting (Week 1),Forecast daily COVID-19 spread in regions around world,https://www.kaggle.com/competitions/covid19-global-forecasting-week-1,https://storage.googleapis.com/kaggle-competitions/kaggle/19539/logos/header.png?t=2020-03-18-01-05-54,"Coronavirus,Tabular",545,640,2913,"**[This week 1 forecasting task is now closed for submissions. Click here to visit the week 2 version, and make a submission there.](https://www.kaggle.com/c/covid19-global-forecasting-week-2)**

*This is one of the two complementary forecasting tasks to predict COVID-19 spread. This task is based on various regions across the world. To start on a single state-level subcomponent, please see the companion forecasting task for [California, USA](https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1/overview/description)*.

### Background

The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1).

### The Challenge

Kaggle is launching two companion COVID-19 forecasting challenges to help answer a [subset of the NASEM/WHO questions](https://www.kaggle.com/c/covid19-global-forecasting-week-1/overview/open-scientific-questions). While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 by region, the primary goal **isn't to produce accurate forecasts**. It’s to identify factors that appear to impact the transmission rate of COVID-19.

You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. 

As the data becomes available, we will update the leaderboard with live results based on [data made available](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).

We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. 

### Companies and Organizations

**There is also a call to action for companies and other organizations:** If you have datasets that might be useful, please upload them to [Kaggle’s dataset platform](https://www.kaggle.com/datasets) and reference them in [this forum thread](https://www.kaggle.com/c/covid19-global-forecasting-week-1/discussion/137078). That will make them accessible to those participating in this challenge and a resource to the wider scientific community.

### Acknowledgements

JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/covid19-global-forecasting-week-1/overview) for details.**
","##Public and Private Leaderboard

To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data on or prior to 2020-03-11 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.

- **Public Leaderboard Period** - 2020-03-12 - 2020-03-25
- **Private Leaderboard Period** - 2020-03-26 - 2020-04-23

##Evaluation

Submissions are evaluated using the column-wise <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">root mean squared  logarithmic error</a>.
<p>The RMSLE for a single column calculated as</p>
<p>$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },<br />$$</p>
<p>where:</p>
<p>\\(n\\) is the total number of observations <br />\\(p_i\\) is your prediction<br />\\(a_i\\) is the actual value <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<p>The final score is the mean of the RMSLE over all columns (in this case, 2).</p>

## Submission File

*We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold*.

For each `ForecastId` in the test set, you'll predict the *cumulative* COVID-19 cases and fatalities to date. The file should contain a header and have the following format:

    ForecastId,ConfirmedCases,Fatalities
    1,10,0
    2,10,0
    3,10,0
    etc.

You will get the `ForecastId` for the corresponding date and location from the `test.csv` file."
COVID19 Local US-CA Forecasting (Week 1),"Forecast daily COVID-19 spread in California, USA",https://www.kaggle.com/competitions/covid19-local-us-ca-forecasting-week-1,https://storage.googleapis.com/kaggle-competitions/kaggle/19548/logos/header.png?t=2020-03-18-01-06-45,"Coronavirus,Tabular",190,216,896,"*This is one of the two complementary forecasting tasks to predict COVID-19 spread. This one is based on a single state-level subcomponent in California, USA. Our intent in having this region-specific version is to offer a more manageable starting point for the global forecasting task. To start on the global version, please see [the companion forecasting task](https://www.kaggle.com/c/covid19-global-forecasting-week-1).*

### Background

The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle)  to prepare the [COVID-19 Open Research Dataset (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to attempt to address [key open scientific questions on COVID-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks). Those questions are drawn from [National Academies of Sciences, Engineering, and Medicine’s (NASEM)](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the [World Health Organization (WHO)](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1).

### The Challenge

Kaggle is launching two companion COVID-19 forecasting challenges to help answer a [subset of the NASEM/WHO questions](https://www.kaggle.com/c/covid19-global-forecasting-week-1/overview/open-scientific-questions). While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 in California, the primary goal **isn't to produce accurate forecasts**. It’s to identify factors that appear to impact the transmission rate of COVID-19.

You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. 

As the data becomes available, we will update the leaderboard with live results based on [data made available](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).

We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. 

### Companies and Organizations

**There is also a call to action for companies and other organizations:** If you have datasets that might be useful, please upload them to [Kaggle’s dataset platform](https://www.kaggle.com/datasets) and reference them in [this forum thread](https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1/discussion/137079). That will make them accessible to those participating in this challenge and a resource to the wider scientific community.

### Acknowledgements

JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.

> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/covid19-global-forecasting-week-1/overview) for details.**
","##Public and Private Leaderboard

To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data on or prior to 2020-03-11 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.

- **Public Leaderboard Period** - 2020-03-12 - 2020-03-25
- **Private Leaderboard Period** - 2020-03-26 - 2020-04-23

##Evaluation

Submissions are evaluated using the column-wise <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">root mean squared  logarithmic error</a>.
<p>The RMSLE for a single column calculated as</p>
<p>$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },<br />$$</p>
<p>where:</p>
<p>\\(n\\) is the total number of observations <br />\\(p_i\\) is your prediction<br />\\(a_i\\) is the actual value <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<p>The final score is the mean of the RMSLE over all columns (in this case, 2).</p>

## Submission File

*We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold*.

For each `ForecastId` in the test set, you'll predict the *cumulative* COVID-19 cases and fatalities to date. The file should contain a header and have the following format:

    ForecastId,ConfirmedCases,Fatalities
    1,10,0
    2,10,0
    3,10,0
    etc.

You will get the `ForecastId` for the corresponding date and location from the `test.csv` file."
Plant Pathology 2020 - FGVC7,Identify the category of foliar diseases in apple trees,https://www.kaggle.com/competitions/plant-pathology-2020-fgvc7,https://storage.googleapis.com/kaggle-competitions/kaggle/18648/logos/header.png?t=2020-02-20-17-30-35,"Image,Agriculture",1317,1510,21784,"## Problem Statement
Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection. 

## Specific Objectives
Objectives of ‘Plant Pathology Challenge’ are to train a model using images of training dataset to 1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf; 2) Accurately distinguish between many diseases, sometimes more than one on a single leaf; 3) Deal with rare classes and novel symptoms; 4) Address depth perception—angle, light, shade, physiological age of the leaf; and 5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning. 

## Resources

Details and background information on the dataset and Kaggle competition ‘Plant Pathology 2020 Challenge’ were published. If you use the dataset for your project, please cite the following peer-reviewed research article

[Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020.](https://bsapubs.onlinelibrary.wiley.com/doi/10.1002/aps3.11390)

## Acknowledgments

We acknowledge financial support from [Cornell Initiative for Digital Agriculture (CIDA)](https://www.digitalagriculture.cornell.edu/) and special thanks to Zach Guillian for help with data collection.   

*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.* 

","Submissions are evaluated on mean column-wise ROC AUC. In other words, the score is the average of the individual AUCs of each predicted column. 

## Submission File
For each image_id in the test set, you must predict a probability for each target variable. The file should contain a header and have the following format:

    image_id,
    test_0,0.25,0.25,0.25,0.25
    test_1,0.25,0.25,0.25,0.25
    test_2,0.25,0.25,0.25,0.25
    etc.
"
iWildCam 2020 - FGVC7,Categorize animals in the wild,https://www.kaggle.com/competitions/iwildcam-2020-fgvc7,https://storage.googleapis.com/kaggle-competitions/kaggle/18294/logos/header.png?t=2020-01-12-18-11-26,"Multiclass Classification,Biology",121,152,1449,"Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the scope of these models we are faced with an interesting problem: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data?   


In order to tackle this problem, we have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to classify species in the test cameras correctly. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from [Landsat 8][9]) for each of the camera trap locations. On the competition [GitHub page][6] we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections.

If you use this dataset in publication, please cite:
```
@article{beery2020iwildcam,
    title={The iWildCam 2020 Competition Dataset},
    author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},
    journal={arXiv preprint arXiv:2004.10340},
    year={2020}
}
```

This is an FGVCx competition as part of the [FGVC7][4] workshop at [CVPR 2020][5], and is sponsored by [Microsoft AI for Earth][3] and [Wildlife Insights][8]. There is a GitHub page for the competition [here][6]. Please open an issue if you have questions or problems with the dataset.   
  


You can find the iWildCam 2018 Competition [here](https://github.com/visipedia/iwildcam_comp/blob/master/2018/readme.md), and the iWildCam 2019 Competition [here](https://github.com/visipedia/iwildcam_comp/blob/master/2019/readme.md).




*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.* 

  [1]: http://lila.science/datasets/wcscameratraps
  [2]: https://github.com/visipedia/inat_comp
  [3]: https://www.microsoft.com/en-us/ai/ai-for-earth
  [4]: https://sites.google.com/view/fgvc7/home
  [5]: http://cvpr2020.thecvf.com/
  [6]: https://github.com/visipedia/iwildcam_comp
  [7]: https://github.com/microsoft/CameraTraps/blob/master/megadetector.md
  [8]: https://www.wildlifeinsights.org/
  [9]: https://www.usgs.gov/land-resources/nli/landsat/landsat-8","##Evaluation

Submissions will be evaluated based on their categorization accuracy 

## Submission Format

The submission format for the competition is a csv file with the following format:

    Id,Predicted
    58857ccf-23d2-11e8-a6a3-ec086b02610b,1
    591e4006-23d2-11e8-a6a3-ec086b02610b,5


The `Id` column corresponds to the test image id. The `Category` is an integer value that indicates the class of the animal, or `0` to represent the absence of an animal.
"
Herbarium 2020 - FGVC7,Identify plant species from herbarium specimens. Data from New York Botanical Garden.,https://www.kaggle.com/competitions/herbarium-2020-fgvc7,https://storage.googleapis.com/kaggle-competitions/kaggle/18627/logos/header.png?t=2020-03-03-16-04-31,"Image,Plants",153,192,1386,"The Herbarium 2020 FGVC7 Challenge is to identify vascular plant species from a large, long-tailed collection herbarium specimens provided by the [New York Botanical Garden](https://www.nybg.org/plant-research-and-conservation/) (NYBG).

The Herbarium 2020 dataset contains over 1M images representing over 32,000 plant species. This is a dataset with a long tail; there are a minimum of 3 specimens per species. However, some species are represented by more than a hundred specimens. This dataset only contains vascular land plants which includes lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide all of our crops, vegetables, and fruits.

<p>
<img src=""https://raw.githubusercontent.com/visipedia/herbarium_comp/master/2020/assets/specimen1.jpg"" width=""125"">
<img src=""https://raw.githubusercontent.com/visipedia/herbarium_comp/master/2020/assets/specimen2.jpg"" width=""125"">
<img src=""https://raw.githubusercontent.com/visipedia/herbarium_comp/master/2020/assets/specimen3.jpg"" width=""125"">
<img src=""https://raw.githubusercontent.com/visipedia/herbarium_comp/master/2020/assets/specimen4.jpg"" width=""125"">
<img src=""https://raw.githubusercontent.com/visipedia/herbarium_comp/master/2020/assets/specimen5.jpg"" width=""125"">
</p>

The teams with the most accurate models will be contacted, with the intention of using them on the un-named plant collections in the NYBG herbarium collection, and assessed by the NYBG plant specialists.

## Background
The New York Botanical Garden (NYBG) herbarium contains more than 7.8 million plant and fungal specimens. Herbaria are a massive repository of plant diversity data.  These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time.  The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist.  All specimens not only maintain their morphological features but also include collection dates and locations, and the name of the person who collected the specimen.  This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time.

## About
This is an FGVC competition hosted as part of the [FGVC7](https://sites.google.com/corp/view/fgvc7/home) workshop at [CVPR 2020](http://cvpr2020.thecvf.com/) and sponsored by [NYBG](https://www.nybg.org/plant-research-and-conservation/).

Details of this competition are mirrored on the [github page](https://github.com/visipedia/herbarium_comp). Please post in the forum or open an issue if you have any questions or problems with the dataset.","Submissions are evaluated using the [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).

F1 is calculated as follows:

$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$

In ""macro"" F1 a separate F1 score is calculated for each `species` value and then averaged.

## Submission Format

For each image `Id`, you should predict the corresponding image label (""category_id"") in the `Predicted` column. The submission file should have the following format:

    Id,Predicted
    0,0
    1,27
    2,42
    ..."
M5 Forecasting - Accuracy,Estimate the unit sales of Walmart retail goods,https://www.kaggle.com/competitions/m5-forecasting-accuracy,https://storage.googleapis.com/kaggle-competitions/kaggle/18599/logos/header.png?t=2020-02-21-17-36-55,Time Series Analysis,5558,7022,88741,"*Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its [companion competition](https://www.kaggle.com/c/m5-forecasting-uncertainty)*

How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.

The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.

In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.

If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.

**Acknowledgements**
Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.","This competition uses a **Weighted Root Mean Squared Scaled Error** (RMSSE). Extensive details about the metric, scaling, and weighting can be found in the [M5 Participants Guide](https://mofc.unic.ac.cy/m5-competition/).

## Submission File
Each row contains an `id` that is a concatenation of an `item_id` and a `store_id`, which is either `validation` (corresponding to the Public leaderboard), or `evaluation` (corresponding to the Private leaderboard). You are predicting 28 forecast days (`F1-F28`) of items sold for each row. For the `validation` rows, this corresponds to `d_1914 - d_1941`, and for the `evaluation` rows, this corresponds to `d_1942 - d_1969`. (Note: a month before the competition close, the ground truth for the `validation` rows will be provided.)

The files must have a header and should look like the following:

    id,F1,...F28
    HOBBIES_1_001_CA_1_validation,0,...,2
    HOBBIES_1_002_CA_1_validation,2,...,11
    ...
    HOBBIES_1_001_CA_1_evaluation,3,...,7
    HOBBIES_1_002_CA_1_evaluation,1,...,4"
M5 Forecasting - Uncertainty, Estimate the uncertainty distribution of Walmart unit sales.  ,https://www.kaggle.com/competitions/m5-forecasting-uncertainty,https://storage.googleapis.com/kaggle-competitions/kaggle/18600/logos/header.png?t=2020-02-21-17-32-51,Time Series Analysis,909,1103,10075,"*Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its [companion competition.](https://www.kaggle.com/c/m5-forecasting-accuracy)*

How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.

The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.

In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.

If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.

**Acknowledgements**
Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.","This competition uses a **Weighted Scaled Pinball Loss** (WSPL). Extensive details about the metric, scaling, and weighting can be found in the [M5 Participants Guide](https://mofc.unic.ac.cy/m5-competition/).

## Submission File
Similar to the point forecast competition, each row contains an `id` that is a concatenation of an `item_id`, a `store_id`, a `quartile`, and the prediction interval, which is either `validation` (corresponding to the Public leaderboard), or `evaluation` (corresponding to the Private leaderboard).

In addition, this competition has rows that have been aggregated at different levels. An `X` indicates the absence of a second aggregation level.

You are predicting 28 forecast days (`F1-F28`) of items sold for each row. For the `validation` rows, this corresponds to `d_1914 - d_1941`, and for the `evaluation` rows, this corresponds to `d_1942 - d_1969`. (Note: a month before the competition close, the ground truth for the `validation` rows will be provided.)

The files must have a header and should look like the following:

    id,F1,...F28
    Total_X_0.005_validation,53,...,201
    HOBBIES_1_001_CA_1_0.005_validation,0,...,2
    HOBBIES_1_002_CA_1_0.005_validation,2,...,11
    ...
    HOBBIES_1_001_CA_1_0.995_evaluation,3,...,7
    HOBBIES_1_002_CA_1_0.995_evaluation,1,...,4"
University of Liverpool - Ion Switching,Identify the number of channels open at each time point,https://www.kaggle.com/competitions/liverpool-ion-switching,https://storage.googleapis.com/kaggle-competitions/kaggle/18045/logos/header.png?t=2020-02-21-18-37-17,Biology,2618,3004,53547,"Think you can use your data science skills to make big predictions at a submicroscopic level?

Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/Liverpool/ion%20image.jpg"" style=""float: right; width: 300px"">

When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.

The University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.

Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.

Acknowledgements:
This would not be possible without the help of the [Biotechnology and Biological Sciences Research Council (BBSRC)](https://bbsrc.ukri.org/). ","Submissions are evaluated using the [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).

F1 is calculated as follows:

$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$

In ""macro"" F1 a separate F1 score is calculated for each `open_channels` value and then averaged.


## Submission File
For each `time` value in the test set, you must predict `open_channels`. The files must have a header and should look like the following:

    time,open_channels
    500.0000,0
    500.0001,2
    etc.
"
Abstraction and Reasoning Challenge,Create an AI capable of solving reasoning tasks it has never seen before,https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/18329/logos/header.png?t=2020-02-07-16-38-49,Artificial Intelligence,913,1025,13018,"![ARC Example](https://storage.googleapis.com/kaggle-media/competitions/ARC/arc_example.png)

Can a computer learn complex, abstract tasks from just a few examples?

Current machine learning techniques are data-hungry and brittle&mdash;they can only make sense of patterns they've seen before. Using current methods, an algorithm can gain new skills by exposure to large amounts of data, but cognitive abilities that could broadly generalize to *many* tasks remain elusive. This makes it very challenging to create systems that can handle the variability and unpredictability of the real world, such as domestic robots or self-driving cars.

However, alternative approaches, like inductive programming, offer the potential for more human-like abstraction and reasoning. The Abstraction and Reasoning Corpus (ARC) provides a benchmark to measure AI skill-acquisition on unknown tasks, with the constraint that only a handful of demonstrations are shown to learn a complex task. It provides a glimpse of a future where AI could quickly learn to solve new problems on its own. The Kaggle Abstraction and Reasoning Challenge invites you to try your hand at bringing this future into the present!

This competition is hosted by [François Chollet](https://fchollet.com/), creator of the Keras neural networks library. [Chollet’s paper on measuring intelligence](https://arxiv.org/abs/1911.01547) provides the context and motivation behind the ARC benchmark.

In this competition, you’ll create an AI that can solve reasoning tasks it has never seen before. Each ARC task contains 3-5 pairs of train inputs and outputs, and a test input for which you need to predict the corresponding output with the pattern learned from the train examples.

If successful, you’ll help bring computers closer to human cognition and you'll open the door to completely new AI applications!
","The competition uses top-3 error rate for the evaluation metric. For each task in the test set, you can predict up to 3 outputs for each test input grid. Each task output has one ground truth. For a given task output, if the ground truth is contained in any of the 3 predicted outputs, then the error for that task is 0, otherwise it is 1. The final score is the error averaged across all tasks.

Mathematically, for each task \\(i\\), your algorithm can make up to 3 predictions \\( o\_{ij} \\), where \\(1 \le j \le 3\\). The error for task \\(i\\) with ground truth \\( g\_i\\) is:

$$
e\_i = \min\_j d\(o\_{ij},  g\_i)
$$

where \\( d\(x,  y) \text{ is } 0 \text{ if } x=y, \text{ otherwise } 1 \\). The overall error score is the average over all \\(N\\) task outputs:

$$
\text{score } = \frac{1}{N} \sum\_i e\_i 
$$

## Format of output predictions
The training, evaluation, and as well as test _input_ data all in the same JSON format. The inputs and outputs are stored in 2d python lists. Your _output predictions_, though, must be flattened into strings, with list rows delimited with an `|`.

For example, the output `[[1,2], [3,4]]` should be reformatted to `|12|34|` as a prediction. You can have up to 3 output predictions per task, and they should be space delimited. See the `sample_solution.csv` file as an example.

The following python code converts a 2d list `pred` into the correct format:
```
def flattener(pred):
    str_pred = str([row for row in pred])
    str_pred = str_pred.replace(', ', '')
    str_pred = str_pred.replace('[[', '|')
    str_pred = str_pred.replace('][', '|')
    str_pred = str_pred.replace(']]', '|')
    return str_pred
```

## Submission File
For each task output `output_id` in the test set, you can make up to 3 predictions. The `output_id` is the `id` of the task, with an indicator of which output you are predicting for that task. Most tasks only have a single output (i.e., `0`), although some tasks have two outputs that must be predicted (i.e., `0, 1`). The file should contain a header and have the following format:

    output_id,output
    00576224_0,|32|78| |32|78| |00|00|
    ...
    12997ef3_0,|00000000000|01100000000|11000000000|...
    12997ef3_1,|00000000000|01100000000|11000000000|...
    etc.


"
Flower Classification with TPUs,"Use TPUs to classify 104 types of flowers
",https://www.kaggle.com/competitions/flower-classification-with-tpus,https://storage.googleapis.com/kaggle-competitions/kaggle/18278/logos/header.png?t=2020-01-31-23-42-27,"Image,Plants,TPU",848,939,10387,"<h3>Tensor Processing Units (TPUs) are Now Available on Kaggle</i></h3>
<p>Tensor Processing Unit (TPU) quotas are now available on Kaggle, at no cost to you!</p>
<p>TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.</p>
<p>The latest Tensorflow release (TF 2.1) was focused on TPUs and they’re now supported both through the Keras high-level API and at a lower level, in models using a custom training loop.</i></p>
<p>We can’t wait to see how your solutions are <i>accelerated</i> by TPUs!</p>
<h3>The Challenge</h3>
<p>It’s difficult to fathom just how vast and diverse our natural world is.</p>
<p>There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over <b>400,000</b> different types of flowers.</p>
<p>In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).</p>
<div class=""note"">
<strong>To get started with TPUs:</strong><br>
<ol>
<li>Read the <a href=""https://www.kaggle.com/docs/tpu"" target=""_blank"">TPU documentation one-pager</a></li>
<li>Then jump right into the <a href=""https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu/"" target=""_blank"">Getting Started Notebook</a> for this competition</li>
</ol>
<p><i>Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.</i></p>
</div>
<h3>Have Questions?</h3>
<p><a href=""https://twitter.com/martin_gorner?lang=en"" target=""_blank"">Martin Görner</a>, Google Developer Advocate and author of <a href=""https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd"" target=""_blank"">Tensorflow without a PhD</a> will be actively engaged in the competition forum. If you have a question or need help troubleshooting, that’s the best place to find help.</p>","Submissions are evaluated on [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).

F1 is calculated as follows:

$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$


In ""macro"" F1 a separate F1 score is calculated for each class / label and then averaged.


## Submission File
For each `id` in the test set, you must predict a type of flower (or `label`). The file should contain a header and have the following format:

```
id,label
a762df180,0
24c5cf439,0
7581e896d,0
eb4b03b29,0
etc.
```"
Categorical Feature Encoding Challenge II,"Binary classification, with every feature a categorical (and interactions!)",https://www.kaggle.com/competitions/cat-in-the-dat-ii,https://storage.googleapis.com/kaggle-competitions/kaggle/17000/logos/header.png?t=2019-11-08-19-16-42,Binary Classification,1161,1293,13837,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/playground/cat_in_dat/cat7.jpg"" style=""float: right; width: 300px"">


<p>Can you find more cat in your dat?</p>

<p>We loved the participation and engagement with the first <a href=""https://www.kaggle.com/c/cat-in-the-dat"" target=""_blank"">Cat in the Dat</a> competition.</p>

<p>Because this is such a common task and important skill to master, we've put together a dataset that contains <strong>only</strong> categorical features, and includes:</p>

<ul>
<li>binary features</li>
<li>low- and high-cardinality nominal features</li>
<li>low- and high-cardinality  ordinal features</li>
<li>(potentially) cyclical features</li>
</ul>

<p>This follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables.  This challenge adds the additional complexity of feature interactions, as well as missing data.</p>

<p>This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.</p>

<p>If you're not sure how to get started, you can check out the <a href=""https://www.kaggle.com/alexisbcook/categorical-variables"" target=""_blank"">Categorical Variables</a>  section of Kaggle's <a href=""https://www.kaggle.com/learn/intermediate-machine-learning"" target=""_blank"">Intermediate Machine Learning course.</a></p><a href=""https://www.kaggle.com/learn/intermediate-machine-learning"" target=""_blank"">

<p>Have Fun!</p></a>","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    600000,0.5
    600001,0.5
    600002,0.5
    ...
"
Santa 2019 - Revenge of the Accountants,Oh what fun it is to revise . . .,https://www.kaggle.com/competitions/santa-2019-revenge-of-the-accountants,https://storage.googleapis.com/kaggle-competitions/kaggle/17800/logos/header.png?t=2019-12-23-21-40-18,"Optimization,Holidays and Cultural Events",106,110,436,"Santa was *thrilled* with the Kaggle community for [minimizing his workshop costs](https://www.kaggle.com/c/santa-workshop-tour-2019/leaderboard)! He had heard rumors that Kagglers were adept at cracking holiday challenges, but, wow, even Santa was surprised at this one.

Unfortunately, the North Pole accountants were less pleased. It turns out, the accountants didn't like being one-upped by machine learning experts on the internet.

To complicate matters, they've decided to allow an additional 1,000 families attend the workshop. And they've also ""fine tuned"" their accounting formula to try and trip up those fancy solvers some people have at their disposal.

Of course, we know that *nothing* trips up the Kaggle community! (Well, except for maybe over-fitting. But fortunately, that doesn't apply here!)

So this is a **bonus** Santa competition for those who want an additional challenge and the opportunity to continue to improve their optimization skills. Since Santa used up all his budget on accounting fees, this is strictly a Playground competition, with the chance to win some coveted Kaggle Swag.

Have fun, and Happy Holidays from the Kaggle Team!

### Attribution
Banner/Listing Photo by Helloquence on Unsplash
","Your submission is scored according to the penalty cost to Santa for suboptimal scheduling. The constraints and penalties are as follows:
- The total number of _people_ attending the workshop each day must be between **125 - 300**; if even one day is outside these occupancy constraints, the submission will error and will not be scored.
- Santa provides consolation gifts (of varying value) to families according to their assigned day relative to their preferences. These sum up per family, and the total represents the \\(preference \: cost\\).
 - `choice_0`: _no consolation gifts_
 - `choice_1`: one **$50** gift card to Santa's Gift Shop
 - `choice_2`: one **$50** gift card, and 25% off Santa's Buffet (value **$9**) for each family member
 - `choice_3`: one **$100** gift card, and 25% off Santa's Buffet (value **$9**) for each family member
 - `choice_4`: one **$200** gift card, and 25% off Santa's Buffet (value **$9**) for each family member
 - `choice_5`: one **$200** gift card, and 50% off Santa's Buffet (value **$18**) for each family member
 - `choice_6`: one **$300** gift card, and 50% off Santa's Buffet (value **$18**) for each family member
 - `choice_7`: one **$300** gift card, and free Santa's Buffet (value **$36**) for each family member
 - `choice_8`: one **$400** gift card, and free Santa's Buffet (value **$36**) for each family member
 - `choice_9`: one **$500** gift card, and free Santa's Buffet (value **$36**) for each family member, and 50% off North Pole Helicopter Ride tickets (value **$199**) for each family member
 - `otherwise`: one **$500** gift card, and free Santa's Buffet (value **$36**) for each family member, and free North Pole Helicopter Ride tickets (value **$398**) for each family member
- Santa's accountants have also developed an empirical equation for cost to Santa that arise from many different effects such as reduced shopping in the Gift Shop when it gets too crowded, extra cleaning costs, a very complicated North Pole tax code, etc. This cost in in addition to the consolation gifts Santa provides above, and is defined as:


$$
accounting\: penalty = \sum\_{d=100}^{1} \sum\_{j=1}^{5} \frac{(N\_{d} - 125)}{400} \frac{{N\_d}^{( \frac{1}{2} + \frac{\lvert N\_d - N\_{d+j} \rvert }{50} )}}{j^2}
$$

where \\(N\_d\\) is the occupancy of the current day, and \\(N\_{d+j}\\) is the occupancy of the \\(i^{th}\\) _previous_ day (since we're counting backwards from Christmas!). All \\(N\_{d+j > 100}\\) are equal to \\(N\_{100} \\).

To be clear on the above summation, it starts on the date 100 days before Christmas and ends on Christmas Eve.

And finally:

$$
score = preference \: cost + accounting\: penalty
$$
"
Natural Language Processing with Disaster Tweets,Predict which Tweets are about real disasters and which ones are not,https://www.kaggle.com/competitions/nlp-getting-started,https://storage.googleapis.com/kaggle-media/competitions/nlp1-cover.jpg,"Text,Binary Classification,NLP",869,894,3526,"###Welcome to one of our ""Getting Started"" competitions 👋

This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don’t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called [Kaggle Notebooks](https://www.kaggle.com/docs/kernels#the-kernels-environment).

###Competition Description

Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).

But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:

<img src=""https://storage.googleapis.com/kaggle-media/competitions/tweet_screenshot.png"" style= ""float: left; width: 250px; margin-right:10px;"">
<br><br>
The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.

In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a [quick tutorial](https://www.kaggle.com/philculliton/nlp-getting-started-tutorial) to get you up and running. 

Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.

###Acknowledgments

This dataset was created by the company figure-eight and originally shared on their [‘Data For Everyone’ website here](https://www.figure-eight.com/data-for-everyone/).

Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480","Submissions are evaluated using [F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) between the predicted and expected answers.

F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$

and:

> True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a _positive_ and that's _true_!
False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a _positive_, and that's _false_.
False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a _negative_, and that's _false_.

## Submission File
For each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise. The file should contain a header and have the following format:

> id,target
0,0
2,0
3,1
9,0
11,0
"
Bengali.AI Handwritten Grapheme Classification,Classify the components of handwritten Bengali,https://www.kaggle.com/competitions/bengaliai-cv19,https://storage.googleapis.com/kaggle-competitions/kaggle/14897/logos/header.png?t=2019-12-16-21-37-33,"Image,Multiclass Classification",2059,2623,38927,"**Challenge and dataset summary paper available at [https://arxiv.org/abs/2010.00170](https://arxiv.org/abs/2010.00170)**

Bengali is the 5th most spoken language in the world with hundreds of million of speakers. It’s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there’s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2Fa9a48686e3f385d9456b59bf2035594c%2Fdesc.png?generation=1576531903599785&alt=media"" style=""float: right; width: 300px"">

Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English’s 250 graphemic units).

Bangladesh-based non-profit [Bengali.AI](https://bengali.ai/) is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education.

For this competition, you’re given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics.

By participating in the competition, you’ll hopefully accelerate Bengali handwritten optical character recognition research and help enable the digitalization of educational resources. Moreover, the methods introduced in the competition will also empower cousin languages in the Indian subcontinent.

Acknowledgements:

<table>
 <tbody>
   <td><img src= https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1132983%2F855c588bba1f95466df0d5fab35bf36e%2Fapurba_logo.png?generation=1576881517547084&alt=media width=""150"" style=""float: left""></td>
   <td><b><a href= http://www.apurbatech.com>Apurba</a>: </b>Apurba is the exclusive sponsor of Bengali.AI for this competition. Apurba Technologies Inc. is founded by a group of technology veterans who have been working at the cutting edge of software development in Silicon Valley for many years. Apart from its many ventures, Apurba is a pioneer in Bengali NLP research today and is accelerating AI research in Bangladesh through its contributions.  </td>
</tr>
<tr>
   <td><img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1132983%2Fb96aa3c59828e5766a86d27cf95a63e5%2FIML_logo.png?generation=1576881594739031&alt=media"" width=""150""></a></td>
   <td><b>Intelligent Machines Limited:</b> Intelligent Machines Limited is the technical partner of Bengali.AI for this competition and is providing compute support to Bangladeshi students. IML is an Artificial Intelligence and Advanced Analytics startup offering customized solutions to businesses in Bangladesh. IML believes in the strength of Bangladeshi talented resources and in the possibility of a far greater and developed Bangladesh in the coming days.</td>
</tr>
</tbody></table>

**If you use this dataset in your research, please cite this paper**
@inproceedings{alam2021large,
  title={A Large Multi-target Dataset of Common Bengali Handwritten Graphemes},
  author={Alam, Samiul and Reasat, Tahsin and Sushmit, Asif Shahriyar and Siddique, Sadi Mohammad and Rahman, Fuad and Hasan, Mahady and Humayun, Ahmed Imtiaz},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={383--398},
  year={2021},
  organization={Springer}
}","Submissions are evaluated using a hierarchical macro-averaged [recall](https://en.wikipedia.org/wiki/Precision_and_recall). First, a standard macro-averaged recall is calculated for each component (grapheme root, vowel diacritic, or consonant diacritic). The final score is the weighted average of those three scores, with the grapheme root given double weight. You can replicate the metric with the following python snippet:

```
import numpy as np
import sklearn.metrics

scores = []
for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:
    y_true_subset = solution[solution[component] == component]['target'].values
    y_pred_subset = submission[submission[component] == component]['target'].values
    scores.append(sklearn.metrics.recall_score(
        y_true_subset, y_pred_subset, average='macro'))
final_score = np.average(scores, weights=[2,1,1])
```

## Submission File
For each image ID in the test set, you must classify the grapheme root, vowel diacritic, and consonant diacritic for all images. The prediction for each component goes on a separate row. The submission file should contain a header and have the following format:

    row_id,target
    Test_0_grapheme_root,3
    Test_1_grapheme_root,2
    Test_2_grapheme_root,1
    ...
"
Deepfake Detection Challenge,Identify videos with facial or voice manipulations,https://www.kaggle.com/competitions/deepfake-detection-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/16880/logos/header.png?t=2019-12-02-22-11-52,Video Data,2265,2904,8581,"**This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the [private leaderboard results have been finalized](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/157925). Late submissions will not be opened, due to an inability to replicate the unique design of this competition.**

Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights—especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.

 <img src=""https://storage.googleapis.com/kaggle-media/competitions/deepfakes/111419_Deepfake_Kaggle_GraphicDescription_Male_350x350.jpg"" style=""float: right; width: 280px"">

AWS, Facebook, Microsoft, the <a href=""https://www.partnershiponai.org/the-partnership-on-ai-steering-committee-on-ai-and-media-integrity/"">Partnership on AI’s Media Integrity Steering Committee</a>, and academics have come together to build the Deepfake Detection Challenge (DFDC). The goal of the challenge is to spur researchers around the world to build innovative new technologies that can help detect deepfakes and manipulated media.

Challenge participants must submit their code into a black box environment for testing. Participants will have the option to make their submission open or closed when accepting the prize. Open proposals will be eligible for challenge prizes as long as they abide by the open source licensing terms. Closed proposals will be proprietary and not be eligible to accept the prizes. Regardless of which track is chosen, all submissions will be evaluated in the same way. Results will be shown on the leaderboard. 

The PAI Steering Committee has emphasized the need to ensure that all technical efforts incorporate attention to how the resulting code and products based on it can be made as accessible and useful as possible to key frontline defenders of information quality such as journalists and civic leaders around the world. The DFDC results will be a contribution to this effort and building a robust response to the emergent threat deepfakes pose globally. 
","Submissions are scored on log loss:

<p>
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
</p>

where

- n is the number of videos being predicted
- \\( \hat{y}_i \\) is the predicted probability of the video being FAKE
- \\( y_i \\) is 1 if the video is FAKE, 0 if REAL
- \\( log() \\) is the natural (base e) logarithm

A smaller log loss is better. The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.

## Submission File
For each `filename` in the test set, you must predict a probability for the `label` variable. The file should contain a header and have the following format:

    filename,label
    10000.mp4,0
    10001.mp4,0.5
    10002.mp4,1
    etc.
"
Santa's Workshop Tour 2019,"In the notebook we can build a model, and pretend that it will optimize...",https://www.kaggle.com/competitions/santa-workshop-tour-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/17233/logos/header.png?t=2019-11-21-22-35-57,"Optimization,Holidays and Cultural Events",1618,1781,13783,"Hammers ring, are you listenin’
In the shop, toys are glistenin’
Should they see the sights?
There might be a fight…
Walkin’ ‘round the Workshop Wonderland

<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2Ffed064c8895cb32515e8d2fdac5fbbd1%2Fmarkus-spiske-coXB9EFuWWg-unsplash.jpg?generation=1574725385080737&alt=media"" style=""float: right; width: 360px"">


Families said, they want to see it
Santa said, he’d guarantee it
They pick a date
But they may have to wait
Walkin’  ‘round the Workshop Wonderland

We told Santa that he was a madman
He just wants to make sure they all smile
He’ll say “Are you flexible?“, They’ll say “Yeah man,
But can you help us make it worth our while?”

“Give them food, or sweater
the more they wait, the gifts get better”
Please help us rank
Or we’ll break the bank!
Walkin’ ’round the Workshop Wonderland


Santa has exciting news! For 100 days before Christmas, he opened up tours to his workshop. Because demand was so strong, and because Santa wanted to make things as fair as possible, he let each of the 5,000 families that will visit the workshop choose a list of dates they'd like to attend the workshop.

Now that all the families have sent Santa their preferences, he's realized it's impossible for everyone to get their top picks, so he's decided to provide extra perks for families that don't get their preferences. In addition, Santa's accounting department has told him that, depending on how families are scheduled, there may be some unexpected and hefty costs incurred.

Santa needs the help of the Kaggle community to optimize which day each family is assigned to attend the workshop in order to minimize any extra expenses that would cut into next years toy budget! Can you help Santa out?

### Attribution
Banner/Listing Photo by Nathan Lemon on Unsplash
Description Photo by Markus Spiske on Unsplash","Your submission is scored according to the penalty cost to Santa for suboptimal scheduling. The constraints and penalties are as follows:
- The total number of _people_ attending the workshop each day must be between **125 - 300**; if even one day is outside these occupancy constraints, the submission will error and will not be scored.
- Santa provides consolation gifts (of varying value) to families according to their assigned day relative to their preferences. These sum up per family, and the total represents the \\(preference \: cost\\).
 - `choice_0`: _no consolation gifts_
 - `choice_1`: one **$50** gift card to Santa's Gift Shop
 - `choice_2`: one **$50** gift card, and 25% off Santa's Buffet (value **$9**) for each family member
 - `choice_3`: one **$100** gift card, and 25% off Santa's Buffet (value **$9**) for each family member
 - `choice_4`: one **$200** gift card, and 25% off Santa's Buffet (value **$9**) for each family member
 - `choice_5`: one **$200** gift card, and 50% off Santa's Buffet (value **$18**) for each family member
 - `choice_6`: one **$300** gift card, and 50% off Santa's Buffet (value **$18**) for each family member
 - `choice_7`: one **$300** gift card, and free Santa's Buffet (value **$36**) for each family member
 - `choice_8`: one **$400** gift card, and free Santa's Buffet (value **$36**) for each family member
 - `choice_9`: one **$500** gift card, and free Santa's Buffet (value **$36**) for each family member, and 50% off North Pole Helicopter Ride tickets (value **$199**) for each family member
 - `otherwise`: one **$500** gift card, and free Santa's Buffet (value **$36**) for each family member, and free North Pole Helicopter Ride tickets (value **$398**) for each family member
- Santa's accountants have also developed an empirical equation for cost to Santa that arise from many different effects such as reduced shopping in the Gift Shop when it gets too crowded, extra cleaning costs, a very complicated North Pole tax code, etc. This cost in in addition to the consolation gifts Santa provides above, and is defined as:


$$
accounting\: penalty = \sum\_{d=100}^{1} \frac{(N\_{d} - 125)}{400} {N\_d}^{( \frac{1}{2} + \frac{\lvert N\_d - N\_{d+1} \rvert }{50} )}
$$

where \\(N\_d\\) is the occupancy of the current day, and \\(N\_{d+1}\\) is the occupancy of the _previous_ day (since we're counting backwards from Christmas!). For the initial condition of \\(d=100\\),  \\(N\_{101} = N\_{100} \\).

To be clear on the above summation, it starts on the date 100 days before Christmas and ends on Christmas Eve.

And finally:

$$
score = preference \: cost + accounting\: penalty
$$

This may seem complicated, but this nifty-difty [starter notebook](https://www.kaggle.com/inversion/santa-s-2019-starter-notebook) should get you started fast!







"
Google QUEST Q&A Labeling,Improving automated understanding of complex question answer content,https://www.kaggle.com/competitions/google-quest-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/7968/logos/header.png?t=2017-12-01-22-29-43,"Text,NLP",1571,1904,27817,"Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences. 

Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well...yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong. 

![](https://storage.googleapis.com/kaggle-media/competitions/google-research/human_computable_dimensions_1.png)

Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the [CrowdSource](https://crowdsource.google.com/) team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.

In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a ""common-sense"" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!

Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.","Submissions are evaluated on the mean column-wise [Spearman's correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient). The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.

## Submission File

For each `qa_id` in the test set, you must predict a probability for each target variable. The predictions should be in the range `[0,1]`. The file should contain a header and have the following format:

    qa_id,question_asker_intent_understanding,...,answer_well_written
    6,0.0,...,0.5
    8,0.5,...,0.1
    18,1.0,...,0.0
    etc."
TensorFlow 2.0 Question Answering,Identify the answers to real user questions about Wikipedia page content,https://www.kaggle.com/competitions/tensorflow2-question-answering,https://storage.googleapis.com/kaggle-competitions/kaggle/12863/logos/header.png?t=2019-10-10-15-55-15,"Text,Text Mining",1233,1417,9846,"*“Why is the sky blue?”*

This is a question an [open-domain question answering](https://en.wikipedia.org/wiki/Question_answering#Open_domain_question_answering) (QA) system should be able to respond to. QA systems emulate how people look for information by reading the web to return answers to common questions. Machine learning can be used to improve the accuracy of these answers.

Existing natural language models have been focused on extracting answers from a short paragraph rather than reading an entire page of content for proper context. As a result, the responses can be complicated or lengthy. A good answer will be both succinct and relevant.

In this competition, your goal is to predict short and long answer responses to real questions about Wikipedia articles. The dataset is provided by [Google's Natural Questions](https://ai.google.com/research/NaturalQuestions/dataset), but contains its own unique private test set. A [visualization of examples](https://ai.google.com/research/NaturalQuestions/visualization) shows long and—where available—short answers. In addition to prizes for the top teams, there is a special set of awards for using TensorFlow 2.0 APIs. 

If successful, this challenge will help spur the development of more effective and robust QA systems.

### About TensorFlow

TensorFlow is an open source platform for machine learning. With TensorFlow 2.0, tf.keras is the preferred high-level API for TensorFlow, to make model building easier and more intuitive. You may use the [tf.keras built-in compile()/fit() methods](https://www.tensorflow.org/guide/keras/train_and_evaluate#part_i_using_build-in_training_evaluation_loops), or write your own [custom training loops](https://www.tensorflow.org/guide/keras/train_and_evaluate#part_ii_writing_your_own_training_evaluation_loops_from_scratch). See the [Effective TensorFlow 2.0 guide](https://www.tensorflow.org/guide/effective_tf2) and the [tf.keras guide](https://www.tensorflow.org/guide/keras/) for more details.

TensorFlow 2.0 was [recently released](https://medium.com/tensorflow/tensorflow-2-0-is-now-available-57d706c2a9ab) and this competition is to challenge Kagglers to use TensorFlow 2.0’s APIs focused on usability, and easier, more intuitive development, to make advancements on Question Answering. ","Submissions are evaluated using [micro F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) between the predicted and expected answers.  Predicted long and short answers must match exactly the token indices of one of the ground truth labels ((or match YES/NO if the question has a yes/no short answer). There may be up to five labels for long answers, and more for short. If no answer applies, leave the prediction blank/null.

The metric in this competition diverges from the [original metric](https://github.com/google-research-datasets/natural-questions/blob/master/nq_eval.py) in two key respects: 1) short and long answer formats do not receive separate scores, but are instead combined into a [micro F1 ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)score across both formats, and 2) this competition's metric does _not_ use `confidence` scores to find an optimal threshold for predictions.

Additional detail:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$

where:

$$
precision = \frac{TP}{TP + FP}
$$

$$
recall = \frac{TP}{TP + FN}
$$

and:

> TP = the predicted indices match one of the possible ground truth indices
FP = the predicted indices do NOT match one of the possible ground truth indices, OR a prediction has been made where no ground truth exists
FN = no prediction has been made where a ground truth exists

In ""micro"" F1, both long and short answers count toward an overall precision and recall, which is then used to calculate a single F1 value. (In contrast, in ""macro"" F1 a separate F1 score is calculated for each class / label and then averaged.)


## Submission File
For each ID in the test set, you must predict a) a set of start:end token indices, b) a YES/NO answer if applicable (short answers ONLY), or c) a BLANK answer if no prediction can be made. The file should contain a header and have the following format:

    -7853356005143141653_long,6:18
    -7853356005143141653_short,YES
    -545833482873225036_long,105:200
    -545833482873225036_short,
    -6998273848279890840_long,
    -6998273848279890840_short,NO
"
2019 Data Science Bowl,Uncover the factors to help measure how young children learn,https://www.kaggle.com/competitions/data-science-bowl-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/16531/logos/header.png?t=2019-10-11-20-50-18,"Video Games,Education,People",3493,4404,81832,"### Illuminate Learning. Ignite Possibilities.

Uncover new insights in early childhood education and how media can support learning outcomes. Participate in our fifth annual Data Science Bowl, presented by Booz Allen Hamilton and Kaggle.

PBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life.  In this challenge, you’ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.

Data Science Bowl is the world’s largest data science competition focused on social good. Each year, this competition gives Kagglers a chance to use their passion to change the world. Over the last four years, more than 50,000+ Kagglers have submitted over 114,000+ submissions, to improve everything from lung cancer and heart disease detection to ocean health. 

For more information on the Data Science Bowl, please visit [DataScienceBowl.com](https://datasciencebowl.com/)

## Where does the data for the competition come from?
The data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user’s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool. 

PBS KIDS is committed to creating a safe and secure environment that family members of all ages can enjoy. The PBS KIDS Measure Up! app does not collect any personally identifying information, such as name or location. All of the data used in the competition is anonymous. To view the full PBS KIDS privacy policy, please visit: pbskids.org/privacy.

No one will be able to download the entire data set and the participants do not have access to any personally identifiable information about individual users. The Data Science Bowl and the use of data for this year’s competition has been reviewed to ensure that it meets requirements of applicable child privacy regulations by PRIVO, a leading global industry expert in children’s online privacy.


## What is the PBS KIDS Measure Up! app?
In the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. To learn more about PBS KIDS Measure Up!, please click [here](https://pbskids.org/apps/pbs-kids-measure-up.html).

*PBS KIDS and the PBS KIDS Logo are registered trademarks of PBS. Used with permission. The contents of PBS KIDS Measure Up! were developed under a grant from the Department of Education. However, those contents do not necessarily represent the policy of the Department of Education, and you should not assume endorsement by the Federal Government. The app is funded by a Ready To Learn grant (PR/AWARD No. U295A150003, CFDA No. 84.295A) provided by the Department of Education to the Corporation for Public Broadcasting.*
","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.

The outcomes in this competition are grouped into 4 groups (labeled `accuracy_group` in the data):
 - 3: the assessment was solved on the first attempt
 - 2: the assessment was solved on the second attempt
 - 1: the assessment was solved after 3 or more attempts
 - 0: the assessment was never solved

The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix <em>O</em> is constructed, such that <em>O<sub>i,j</sub></em> corresponds to the number of `installation_id`s <em>i</em> (actual) that received a predicted value <em>j</em>. An <em>N-by-N </em>matrix of weights, <em>w</em>, is calculated based on the difference between actual and predicted values:

$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$

An <em>N-by-N</em> histogram matrix of expected outcomes, <em>E</em>, is calculated assuming that there is no correlation between values.  This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that <em>E</em> and <em>O</em> have the same sum.

From these three matrices, the quadratic weighted kappa is calculated as: 

$$\kappa=1-\frac{\sum\_{i,j}w\_{i,j}O\_{i,j}}{\sum\_{i,j}w\_{i,j}E\_{i,j}}.$$

## Submission File
For each `installation_id` represented in the test set, you must predict the `accuracy_group` of the last assessment for that `installation_id`. The files must have a header and should look like the following:

    installation_id,accuracy_group
    00abaee7,3
    01242218,0
    etc.
"
Peking University/Baidu - Autonomous Driving,Can you predict vehicle angle in different settings?,https://www.kaggle.com/competitions/pku-autonomous-driving,https://storage.googleapis.com/kaggle-competitions/kaggle/9993/logos/header.png?t=2019-04-30-14-21-21,"Image,Computer Vision",864,1105,10001,"Who do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles ― and it's at the heart of our newest challenge. 

Self-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles’ ability to accurately perceive objects in traffic.

Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They’re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models.

Your challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment.

Succeed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies. 

Please cite the following paper when using the dataset:
ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving
@inproceedings{song2019apollocar3d,
  title={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},
  author={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5452--5462},
  year={2019}
}


","Submissions are evaluated on [mean average precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision) between the predicted pose information and the correct position and rotation.

We use the following C# code to determine the translation and rotation distances:

        public static double RotationDistance(Object3D o1, Object3D o2)
        {
            Quaternion q1 = Quaternion.CreateFromYawPitchRoll(o1.yaw, o1.pitch, 
                 o1.roll);
            Quaternion q2 = Quaternion.CreateFromYawPitchRoll(o2.yaw, o2.pitch, 
                 o2.roll);
            Quaternion diff = Quaternion.Normalize(q1) * 
                 Quaternion.Inverse(Quaternion.Normalize(q2));

            diff.W = Math.Clamp(diff.W, -1.0f, 1.0f);
            
            return Object3D.RadianToDegree( Math.Acos(diff.W) );
        }

        public static double TranslationDistance(Object3D o1, Object3D o2)
        {
            var dx = o1.x - o2.x;
            var dy = o1.y - o2.y;
            var dz = o1.z - o2.z;

            return Math.Sqrt(dx * dx + dy * dy + dz * dz);
        }

We then take the resulting distances between all pairs of objects and determine which predicted objects are closest to solution objects, and apply thresholds for both translation and rotation.  Confidence scores are used to sort submission objects. Units for rotation are radians; translation is meters.

If both of the distances between prediction and solution (as calculated above) are less than the threshold, then that prediction object is counted as a true positive for that threshold. If not the predicted object is counted as a false positive for that threshold.

Finally, mAP is calculated using these TP/FP determinations across all thresholds.

The thresholds are as follows:

Rotation: `50, 45, 40, 35, 30, 25, 20, 15, 10, 5`

Translation: `0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01`

## Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

## Submission File
For each image ID in the test set, you must predict a pose (position and rotation) for all unmasked cars in the image. The file should contain a header and have the following format:

    ImageId,PredictionString
    ID_1d7bc9b31,0.5 0.25 0.5 0.0 0.5 0.0 1.0
    ID_f9c21a4e3,0.5 0.5 0.5 0.0 0.0 0.0 0.9
    ID_e83dd7c22,0.5 0.5 0.5 0.0 0.0 0.0 1.0
    ID_1a050c9a4,0.5 0.5 0.5 0.0 0.0 0.0 0.25
    ID_d943d1083,0.5 0.5 0.5 0.0 0.0 0.0 1.0 0.5 0.5 0.5 0.0 0.0 0.0 1.0
    ID_3155084f7,0.5 0.5 0.5 0.0 0.0 0.0 1.0
    ID_f74dcaa3d,0.5 0.5 0.5 0.0 0.0 0.0 1.0
    ID_b183b55dd,0.5 0.5 0.5 0.0 0.0 0.0 1.0
    ID_ff5ea7211,0.5 0.5 0.5 0.0 0.0 0.0 1.0

Each 7-value element in `PredictionString` corresponds to `pitch`, `yaw`, `roll`, `x`, `y`, `z` and `confidence` for each car in the scene."
ASHRAE - Great Energy Predictor III,How much energy will a building consume?,https://www.kaggle.com/competitions/ashrae-energy-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/9994/logos/header.png?t=2019-10-04-23-03-48,"Tabular,Energy",3614,4342,39402,"Q: How much does it cost to cool a skyscraper in the summer?
A: A lot! And not just in dollars, but in environmental impact.

Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types.

In this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.

**About the Host**
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2Ff9ab8963dea5e7c1716f47310daa96ab%2FASHRAE_Logo_25.jpg?generation=1570808142334850&alt=media)

Founded in 1894, [ASHRAE](https://www.ashrae.org/) serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, [ASHRAE](https://www.ashrae.org/) supports research, standards writing, publishing and continuing education - shaping tomorrow’s built environment today.

Banner photo by Federico Beccari on Unsplash
","## Evaluation Metric
<p>The evaluation metric for this competition is Root Mean Squared Logarithmic Error.</p>
<p>The RMSLE is calculated as</p>
<p>$$<br />\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }<br />$$</p>
<p>Where:</p>
<p>\\(\epsilon\\) is the RMSLE value (score)<br />\\(n\\) is the total number of observations in the (public/private) data set,<br />\\(p_i\\) is your prediction of target, and<br />\\(a_i\\) is the actual target for \\(i\\). <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
Note that not all rows will necessarily be scored.

## Notebook Submissions
You can make submissions directly from Kaggle Notebooks. By adding your teammates as collaborators on a notebook, you can share and edit code privately with them.

## Submission File
For each id in the test set, you must predict the target variable. The file should contain a header and have the following format:

     id,meter_reading
     0,0
     1,0
     2,0
     etc."
NFL Big Data Bowl,How many yards will an NFL player gain after receiving a handoff?,https://www.kaggle.com/competitions/nfl-big-data-bowl-2020,https://storage.googleapis.com/kaggle-competitions/kaggle/15696/logos/header.png?t=2019-10-04-16-16-53,"Football,Sports",2038,2173,3097,"*“The running back takes the handoff… he breaks a tackle...spins... and breaks free! One man to beat! Past the 50-yard-line! To the 40! The 30! He! Could! Go! All! The! Way!”*

But will he?

<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/15696/logos/thumb76_76.png?t=2019-10-04-16-17-46"" style=""float: right; width: 180px"">

American football is a complex sport. From the 22 players on the field to specific characteristics that ebb and flow throughout the game, it can be challenging to quantify the value of specific plays and actions within a play.  Fundamentally, the goal of football is for the offense to run (rush) or throw (pass) the ball to gain yards, moving towards, then across, the opposing team’s side of the field in order to score. And the goal of the defense is to prevent the offensive team from scoring. 

In the National Football League (NFL), roughly a third of teams’ offensive yardage comes from run plays.. Ball carriers are generally assigned the most credit for these plays, but their teammates (by way of blocking), coach (by way of play call), and the opposing defense also play a critical role. Traditional metrics such as ‘yards per carry’ or ‘total rushing yards’ can be flawed; in this competition, the NFL aims to provide better context into what contributes to a successful run play.

As an “armchair quarterback” watching the game, you may think you can predict the result of a play when a ball carrier takes the handoff - but what does the data say? In this competition, you will develop a model to predict how many yards a team will gain on given rushing plays as they happen. You'll be provided game, play, and player-level data, including the position and speed of players as provided in the NFL’s Next Gen Stats data. And the best part - you can see how your model performs from your living room, as the leaderboard will be updated week after week on the current season’s game data as it plays out. 

Deeper insight into rushing plays will help teams, media, and fans better understand the skill of players and the strategies of coaches. It will also assist the NFL and its teams evaluate the ball carrier, his teammates, his coach, and the opposing defense, in order to make adjustments as necessary.

Additionally, the winning model will be provided to the NFL’s Next Gen Stats group to potentially share with teams. You could help the NFL Network generate models to use during games, or for pre-game/post-game breakdowns.

","Submissions will be evaluated on the Continuous Ranked Probability Score (CRPS). For each `PlayId`, you must predict a cumulative probability distribution for the yardage gained or lost.  In other words, each column you predict indicates the probability that the team gains <= that many yards on the play. The CRPS is computed as follows:

$$
C = \frac{1}{199N} \sum\_{m=1}^{N} \sum\_{n=-99}^{99} (P(y \\le n) -H(n - Y_m))^2, 
$$

where P is the predicted distribution, N is the number of plays in the test set, Y is the actual yardage and H(x) is the Heaviside step function (\\(H(x) = 1\\) for \\(x \ge 0\\) and zero otherwise).

The submission will not score if any of the predicted values has 

$$
P(y \le k) > P(y \le k+1)
$$

for any k (i.e. the CDF must be non-decreasing).

## Submission File

For each `PlayId`, you must predict 199 values that represent its cumulative distribution from -99 to 99 yards gained. The file must have a header and contain all 199 values in the following format:

    PlayId,Yards-99,Yards-98...Yards98,Yards99
    20190905000050,0.0,0.0, ... ,1.0,1.0
    20190905000195,0.0,0.0, ... ,1.0,1.0
    etc...
"
Kannada MNIST,MNIST like datatset for Kannada handwritten digits,https://www.kaggle.com/competitions/Kannada-MNIST,https://storage.googleapis.com/kaggle-competitions/kaggle/16017/logos/header.png?t=2019-09-11-17-07-02,"Image,Computer Vision",1212,1342,11077,"### Bored of MNIST?

The goal of this competition is to provide a simple extension to the classic [MNIST competition](https://www.kaggle.com/c/digit-recognizer/) we're all familiar with. Instead of using Arabic numerals, it uses a recently-released dataset of Kannada digits.

Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. [Wikipedia](https://en.wikipedia.org/wiki/Kannada)

![](https://storage.googleapis.com/kaggle-media/competitions/Kannada-MNIST/kannada.png)

This competition uses the same format as the [MNIST competition](https://www.kaggle.com/c/digit-recognizer/) in terms of how the data is structured, but it's different in that it is a synchronous re-run Kernels competition. You write your code in a Kaggle Notebook, and when you submit the results, your code is scored on both the public test set, as well as a private (unseen) test set.

### Technical Information

All details of the dataset curation has been captured in the paper titled: Prabhu, Vinay Uday. ""Kannada-MNIST: A new handwritten digits dataset for the Kannada language."" arXiv preprint [arXiv:1908.01242 (2019)](https://arxiv.org/abs/1908.01242)

The github repo of the author [can be found here](https://github.com/vinayprabhu/Kannada_MNIST).

On the [originally-posted dataset](https://www.kaggle.com/higgstachyon/kannada-mnist), the author suggests some interesting questions you may be interested in exploring. Please note, although this dataset has been released in full, the purpose of this competition is for practice, not to find the labels to submit a perfect score.

 In addition to the _main_ dataset, the author also disseminated an additional real world handwritten dataset (with 10k images), termed as the 'Dig-MNIST dataset' that can serve as an out-of-domain test dataset. It was created with the help of volunteers that were non-native users of the language, authored on a smaller sheet and scanned with different scanner settings compared to the _main_ dataset. This 'dig-MNIST' dataset serves as a more difficult test-set (An accuracy of 76.1% was reported in the paper cited above) and achieving ~98+% accuracy on this test dataset would be rather commendable.

### Acknowledgments
Kaggle thanks [Vinay Prabhu](https://www.kaggle.com/higgstachyon) for providing this interesting dataset for a Playground competition.

Image reference: https://www.researchgate.net/figure/speech-for-Kannada-numbers_fig2_313113588","This competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).

## Submission File Format

The file should contain a header and have the following format:
```
id,label
1,5
2,5
3,5
...
```"
RSNA Intracranial Hemorrhage Detection,Identify acute intracranial hemorrhage and its subtypes,https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/13451/logos/header.png?t=2019-08-26-19-47-30,Image,1345,723,2553,"Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. For example, intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient. 

Diagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patient’s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming. 

In this competition, your challenge is to build an algorithm to detect acute intracranial hemorrhage and [its subtypes](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview/hemorrhage-types). 

You’ll develop your solution using a rich image dataset provided by the Radiological Society of North America (RSNA®) in collaboration with members of the American Society of Neuroradiology and MD.ai. 

If successful, you’ll help the medical community identify the presence, location and type of hemorrhage in order to quickly and effectively treat affected patients.

Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from December 1-6, 2019.

###Collaborators

Four research institutions provided large volumes of de-identified CT studies that were assembled to create the challenge dataset: Stanford University, Thomas Jefferson University, Unity Health Toronto and Universidade Federal de São Paulo (UNIFESP), The American Society of Neuroradiology ([ASNR](https://www.asnr.org/)) organized a cadre of more than 60 volunteers to label over 25,000 exams for the challenge dataset. ASNR is the world’s leading organization for the future of neuroradiology representing more than 5,300 radiologists, researchers, interventionalists, and imaging scientists. MD.ai provided tooling and support for the data annotation process. 

The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for AI to assist in detection and classification of hemorrhages in order to prioritize and expedite their clinical work.

[A full set of acknowledgments can be found on this page](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview/acknowledgments).","Submissions are evaluated using a **weighted multi-label logarithmic loss**. Each hemorrhage sub-type is its own row for every image, and you are expected to predict a probability for that sub-type of hemorrhage. There is also an `any` label, which indicates that a hemorrhage of ANY kind exists in the image. The`any` label is weighted more highly than specific hemorrhage sub-types.

For each image `Id`, you must submit a set of predicted probabilities (a separate row for each sub-type). We then take the [log loss](https://www.kaggle.com/dansbecker/what-is-log-loss) for each predicted probability versus its true label.  Finally, loss is averaged across all samples.

In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>

## Submission File

There will be **6** rows per image `Id`.  The label indicated by a particular row will look like `[Image Id]_[Sub-type Name]`, as follows

There is also a target column, `Label`, indicating the probability of whether that type of hemorrhage exists in the indicated image.

For each image `Id` in the test set, you must predict a probability for each of the different possible sub-types. The file should contain a header and have the following format:


    Id,Label
    1_epidural,0
    1_intraparenchymal,0
    1_intraventricular,0
    1_subarachnoid,0.6
    1_subdural,0
    1_any,0.9
    2_epidural,0
    etc.
"
BigQuery-Geotab Intersection Congestion,Can you predict wait times at major city intersections?,https://www.kaggle.com/competitions/bigquery-geotab-intersection-congestion,https://storage.googleapis.com/kaggle-competitions/kaggle/14859/logos/header.png?t=2019-08-02-18-39-35,"Tabular,Regression,Cities and Urban Areas,Geospatial Analysis",432,487,3372,"We’ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you.

[Geotab](https://www.geotab.com/) provides a [wide variety of aggregate datasets](https://data.geotab.com/intelligence-data) gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges.

The dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia. 

This competition is being hosted in partnership with [BigQuery](https://cloud.google.com/bigquery/), a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface.

Kaggle recently released a BigQuery integration within our kernels notebook environment, and [this starter kernel](https://www.kaggle.com/sirtorry/bigquery-ml-template-intersection-congestion) gives you a great starting point for how to use BQ & BQML. You’re encouraged to use your data savvy, resourcefulness & intuition to find and join in additional external datasets that will increase your models’ predictive power.

Alright, stop waiting and get started!

### Acknowledgments

![Geotab](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2F2c4691c7ddb4cef68c23efaa16bcbf57%2Fgeotab-logo-300.jpg?generation=1566501523911770&alt=media)

**A big thanks to Geotab for providing the dataset for this competition!** Geotab is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab’s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab’s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, please visit [www.geotab.com](https://www.geotab.com/) and follow us [@GEOTAB](https://twitter.com/GEOTAB) and on [LinkedIn](https://www.linkedin.com/company/geotab/).  
","Submissions are scored on the **root mean squared error**. RMSE is defined as:
<p>\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]</p>
where \\( \hat{y} \\) is the predicted value, and \\( y \\) is the original value.

## Submission File
For each row in the test set, you must predict the value of six target outcomes as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:

    ID,TARGET
    0_1,0
    0_2,0
    0_3,0
    etc.
"
Lyft 3D Object Detection for Autonomous Vehicles,Can you advance the state of the art in 3D object detection? ,https://www.kaggle.com/competitions/3d-object-detection-for-autonomous-vehicles,https://storage.googleapis.com/kaggle-competitions/kaggle/15768/logos/header.png?t=2019-09-10-21-24-06,Image,546,659,5697,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/Lyft-Kaggle/Kaggle-01.png"" style=""float: right; width: 200px"">

Self-driving technology presents a rare opportunity to improve the quality of life in many of our communities. Avoidable collisions, single-occupant commuters, and vehicle emissions are choking cities, while infrastructure strains under rapid urban growth. Autonomous vehicles are expected to redefine transportation and unlock a myriad of societal, environmental, and economic benefits. You can apply your data analysis skills in this competition to advance the state of self-driving technology.

[Lyft](https://level5.lyft.com/), whose mission is to improve people’s lives with the world’s best transportation, is investing in the future of self-driving vehicles. Level 5, their self-driving division, is working on a fleet of autonomous vehicles, and currently has a team of 450+ across Palo Alto, London, and Munich working to build a leading self-driving system ([they’re hiring!](https://level5.lyft.com/#joinourteam)). Their goal is to democratize access to self-driving technology for hundreds of millions of Lyft passengers.

From a technical standpoint, however,  the bar to unlock technical research and development on higher-level autonomy functions like perception, prediction, and planning is extremely high. This implies technical R&D on self-driving cars has traditionally been inaccessible to the broader research community.

This dataset aims to democratize access to such data, and foster innovation in higher-level autonomy functions for everyone, everywhere. By conducting a competition, we hope to encourage the research community to focus on hard problems in this space—namely, 3D object detection over semantic maps. 


In this competition, you will build and optimize algorithms based on a large-scale dataset. This dataset features the raw sensor camera inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a restricted geographic area. 

If successful, you’ll make a significant contribution towards stimulating further development in autonomous vehicles and empowering communities around the world.
","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted 3D bounding volumes and ground truth bounding volumes is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: `(0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)`. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object.  A false negative indicates a ground truth object had no associated predicted object.

**Important note:** if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.

The average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$

In your submission, you are also asked to provide a `confidence` level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.

**NOTE:** In nearly all cases `confidence` will have **no** impact on scoring.  It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases.  None of these edge cases are known to exist in the data set.  If you do not wish to use or calculate `confidence` you can use a placeholder value - like `1.0` - to indicate that no particular order applies to the evaluation of your submission boxes.

Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.

## Intersection over Union (IoU)

Intersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects).  It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.

It can be visualized as the following:

![Image of Intersection over Union][1]

The two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together.  IoU would be low - and would likely not count as a ""hit"" at higher IoU thresholds.

## 3D Context

The difference between the 2D and 3D bounding volume contexts is small. In the 3D context we reduce the bounding volume to a `ground bounding box` and a `height`. The IoU is then the `intersection` of the `ground bounding boxes` * the `intersection` of the `height` differences, divided by the union of the bounding boxes.

## Submission File

The submission format requires a space delimited set of bounding volume parameters. For example:

`97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c,1.0 2742.15 673.16 -18.65 1.834 4.609 1.648 2.619 car`

indicates that sample `97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c` has a bounding volume with a `confidence` of 0.5, `center_x` of 2742.15, `center_y` of 673.16, `center_z` of -18.65, `width` of 1.834, `length` of 4.609, `height` of 1.648, `yaw` of 2.619, and a `class_name` of `car`.

The file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.

    Id,PredictionString
    db8b47bd4ebdf3b3fb21598bb41bd8853d12f8d2ef25ce76edd4af4d04e49341,
    97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c,1.0 2742.15 673.16 -18.65 1.834 4.609 1.648 2.619 car
    etc...


  [1]: https://storage.googleapis.com/kaggle-media/competitions/rsna/IoU.jpg
"
Categorical Feature Encoding Challenge,"Binary classification, with every feature a categorical",https://www.kaggle.com/competitions/cat-in-the-dat,https://storage.googleapis.com/kaggle-competitions/kaggle/14999/logos/header.png?t=2019-08-22-18-17-37,"Tabular,Binary Classification",1338,1387,13527,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/playground/cat_in_dat/cat7.jpg"" style=""float: right; width: 300px"">

<p>Is there a cat in your dat?</p>

<p>A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.</p>

<p>Because this is such a common task and important skill to master, we've put together a dataset that contains <strong>only</strong> categorical features, and includes:</p>

<ul>
<li>binary features</li>
<li>low- and high-cardinality nominal features</li>
<li>low- and high-cardinality  ordinal features</li>
<li>(potentially) cyclical features</li>
</ul>

<p>This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.</p>

<p>If you're not sure how to get started, you can check out the <a href=""https://www.kaggle.com/alexisbcook/categorical-variables"" target=""_blank"">Categorical Variables</a>  section of Kaggle's <a href=""https://www.kaggle.com/learn/intermediate-machine-learning"" target=""_blank"">Intermediate Machine Learning course.</a></p><a href=""https://www.kaggle.com/learn/intermediate-machine-learning"" target=""_blank"">

<p>Have Fun!</p></a>","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    300000,0.5
    300001,0.5
    300002,0.5
    ...
"
Understanding Clouds from Satellite Images,Can you classify cloud structures from satellites? ,https://www.kaggle.com/competitions/understanding_cloud_organization,https://storage.googleapis.com/kaggle-competitions/kaggle/13333/logos/header.png?t=2019-06-07-19-02-58,"Image,Atmospheric Science",1531,1857,29163,"![](https://storage.googleapis.com/kaggle-media/competitions/MaxPlanck/Teaser_AnimationwLabels.gif)

Climate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition’s dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world’s ever-changing atmosphere and they need your help to better understand the clouds.

Shallow clouds play a huge role in determining the Earth's climate. They’re also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models.

There are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features&mdash;such as clouds that resemble flowers.

In this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you’ll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections.

**Help us remove the haze from climate models and bring clarity to cloud identification.**

_For more information on the scientific background and how the labels were created see the following [paper](https://arxiv.org/abs/1906.01906)._


","This competition is evaluated on the mean <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"">Dice coefficient</a>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:

$$
\frac{2 * |X \cap Y|}{|X| + |Y|}
$$

where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each `<Image, Label>` pair in the test set.

###EncodedPixels 

*In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.* Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).

The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.

###IMPORTANT

The predicted encodings should be against images that are scaled by `0.25` per side. In other words, while the images in Train and Test are `1400 x 2100` pixels, the predictions should be scaled down to a `350 x 525` pixel image. The reduction is required to achieve reasonable submission evaluation times.

###Submission File Format

Your submission file should be in csv format, with a header and columns names : `Image_Label`, `EncodedPixels`. Each row in your submission represents a single predicted cloud type segmentation for the given `Image`, and predicted `Label`, and you should have the same number of rows as `num_images * num_labels`. The segment for cloud type in an image will be encoded into a single row, even if there are several non-contiguous cloud type locations in an image. If there is no area of a certain cloud type for an image, the corresponding `EncodedPixels` prediction should be left blank.

    Image_Label,EncodedPixels
    002f507.jpg_Fish,1 1
    002f507.jpg_Flower,1 1
    002f507.jpg_Gravel,2 183749
    etc...
"
Ciphertext Challenge III,BRBTvl0LNstxQLyxulCEEq1czSFje0Z6iajczo6ktGmitTE=,https://www.kaggle.com/competitions/ciphertext-challenge-iii,https://storage.googleapis.com/kaggle-competitions/kaggle/15447/logos/header.png?t=2019-08-02-21-04-59,Text,103,110,346,"## Ciphertext Challenge III: Wherefore Art Thou, Simple Ciphers?

We've done the [2010's](https://www.kaggle.com/c/ciphertext-challenge-ii), the [1990s](https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge)... now it's time for the 80s.

_The 1580s!!_

In this new decryption competition's dataset, we've gone from perfectly respectable sources of electronic horror to a time before computers—heck, before calculus was called ""calculus""!  Shakespeare's plays are encrypted, and we time travelers must un-encrypt them so people can do innovative stage productions with intricate makeup, costumes, and possibly—possibly!—Leonardo DiCaprio. Think about it, folks: _Leo_.\*

As in previous ciphertext challenges, [simple classic ciphers](https://www.kaggle.com/c/ciphertext-challenge-iii/data) have been used to encrypt this dataset, along with a _slightly_ less simple surprise that expands our definition of ""classic"" into the modern age. The mission is the same: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Meta-puzzles and difficulty await!

Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the leaderboard (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best *competition-related* kernels, in both visualization and cryptanalysis, based on upvotes.  Last, the coveted ""Phil Prize""—for the team that correctly deduces the form AND key of the final cipher—is up for grabs again.

Go ahead. Get cracking!

\* - _Leo!_

## Acknowledgements
*Many thanks to Kaggler [LiamLarson](https://www.kaggle.com/kingburrito666) for their excellent [Shakespeare dataset](https://www.kaggle.com/kingburrito666/shakespeare-plays).*

","Submissions are evaluated on [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) between the predicted plaintext `index` and the actual `index`.


## Submission File
For each ciphertext in the test set, you must predict the plaintext index. The file should contain a header and have the following format:

    ciphertext_id,index
    ID_0827e580b,0
    ID_bfcf14ce8,0
    ID_aab1e107a,0
    ID_061cc38c0,0
    ID_0e16534d3,0
    ID_b1387ef4a,0
    ID_e2275f924,0
    ID_d68e90960,0
    ID_0a3775e01,0
"
Severstal: Steel Defect Detection,Can you detect and classify defects in steel?,https://www.kaggle.com/competitions/severstal-steel-defect-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/14241/logos/header.png?t=2019-06-17-15-52-00,"Image,Manufacturing",2427,2867,51874,"Steel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.

<img 
    src=""https://storage.googleapis.com/kaggle-competitions/kaggle/14241/logos/thumb76_76.png?t=2019-06-17-15-52-14""
    style=""float: right; margin-left: 10px""
/>

[Severstal][1] is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry—and they take corporate responsibility seriously. The company recently created the country’s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production.

The production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it’s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm. 

In this competition, you’ll help engineers improve the algorithm by localizing and classifying surface defects on a steel sheet.

If successful, you’ll help keep manufacturing standards for steel high and enable Severstal to continue their innovation, leading to a stronger, more efficient world all around us.


  [1]: https://www.severstal.com/eng/","This competition is evaluated on the mean <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"">Dice coefficient</a>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:

$$
\frac{2 * |X \cap Y|}{|X| + |Y|}
$$

where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each `<ImageId, ClassId>` pair in the test set.

###EncodedPixels 

*In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.* Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).

The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.

###File Format

Your submission file should be in csv format, with a header and columns names : `ImageId_ClassId`, `EncodedPixels`. Each row in your submission represents a single predicted defect segmentation for the given `ImageId`, and predicted `ClassId`, and you should have the same number of rows as `num_images * num_defect_classes`. The segment for each defect class will be encoded into a single row, even if there are several non-contiguous defect locations on an image.

    ImageId_ClassId,EncodedPixels
    004f40c73.jpg_1,1 1
    004f40c73.jpg_2,1 1
    004f40c73.jpg_3,2 409599
    etc...
"
Kuzushiji Recognition,Opening the door to a thousand years of Japanese culture,https://www.kaggle.com/competitions/kuzushiji-recognition,https://storage.googleapis.com/kaggle-competitions/kaggle/13578/logos/header.png?t=2019-04-24-22-47-58,"Image,Multiclass Classification,History,Japan",293,338,2652,"###**Build a model to transcribe ancient Kuzushiji into contemporary Japanese characters**###
Imagine the history contained in a thousand years of books. What stories are in those books? What knowledge can we learn from the world before our time? What was the weather like 500 years ago? What happened when Mt. Fuji erupted? How can one fold 100 cranes using only one piece of paper? The answers to these questions are in those books.

Japan has millions of books and over a billion historical documents such as personal letters or diaries preserved nationwide. Most of them cannot be read by the majority of Japanese people living today because they were written in “Kuzushiji”.

Even though Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years, there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). Due to the lack of available human resources, there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. (More information in [About Kuzushiji](https://www.kaggle.com/c/kuzushiji-recognition/overview/about-kuzushiji))
 
This is where you come in. The hosts need help from machine learning experts to transcribe Kuzushiji into contemporary Japanese characters. With your help, Center for Open Data in the Humanities (CODH) will be able to develop better algorithms for Kuzushiji recognition. The model is not only a great contribution to the machine learning community, but also a great help for making millions of documents more accessible and leading to new discoveries in Japanese history and culture.


![](http://static.mxbi.net/umgy001-010-smallannomasked.jpg)

##Hosts##

**[Center for Open Data  in the Humanities (CODH)](http://codh.rois.ac.jp/)**  conducts research and development to enhance access to humanities data using state-of-the-art technology in informatics and statistics.

**[The National Institute of Japanese Literature (NIJL)](https://www.nijl.ac.jp/en/)** is an institution which strives to serve researchers in the field of Japanese literature as well as those working in various other humanities, by collecting in one location a vast storage of materials related to Japanese literature gathered from all corners of the country. 

**[The National Institute of Informatics (NII)](https://www.nii.ac.jp/en/)** is Japan's only general academic research institution seeking to create future value in the new discipline of informatics. NII seeks to advance integrated research and development activities in information-related fields, including networking, software, and content.

**Official Collaborators**
Mikel Bober-Irizar ([anokas](https://www.kaggle.com/anokas)) Kaggle Grandmaster and Alex Lamb (MILA. Quebec Artificial Intelligence Institute)


","Submissions are evaluated on a modified version of the [F1 Score](https://en.wikipedia.org/wiki/F1_score). To score a true positive, you must provide center point coordinates that are within the ground truth bounding box and a matching label. The ground truth bounding boxes are defined in the format `{label X Y Width Height}`, so if the ground truth label is `U+003F 1 1 10 10` then a prediction of  `U+003F 3 3` would pass. You can find [a Python version of the metric here](https://gist.github.com/SohierDane/a90ef46d79808fe3afc70c80bae45972).

## Submission File
For each image in the test set, you must locate and identify all of the kuzushiji characters. Not every image will necessarily be scored. The file should contain a header and have the following format:


```
image_id,labels
image_id,{label X Y} {...}
```

Do not make more than 1,200 predictions per page."
IEEE-CIS Fraud Detection,Can you detect fraud from customer transactions?,https://www.kaggle.com/competitions/ieee-fraud-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/14242/logos/header.png?t=2019-07-11-17-52-31,"Tabular,Binary Classification",6351,7389,125219,"Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.

Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”

While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the [IEEE Computational Intelligence Society](https://cis.ieee.org/) (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection,  you can get on with your chips without the hassle.

IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, [Vesta Corporation](https://trustvesta.com/), seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.

In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions  and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. 

If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.

*Acknowledgements*: 

<img src = ""https://storage.googleapis.com/kaggle-media/competitions/IEEE/Vesta-logo_200x.png""
    style=""margin-left: 10px""/>


Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions.  Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry.  Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually. 

Header Photo by Tim Evans on Unsplash","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each `TransactionID` in the test set, you must predict a probability for the `isFraud` variable. The file should contain a header and have the following format:

    TransactionID,isFraud
    3663549,0.5
    3663550,0.5
    3663551,0.5
    etc.
"
Open Images 2019 - Instance Segmentation,Outline segmentation masks of objects in images,https://www.kaggle.com/competitions/open-images-2019-instance-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/15043/logos/header.png?t=2019-06-18-19-29-49,Image,193,232,1566,"#Introduction
<iframe src=""https://www.kaggle.com/c/open-images-2019-instance-segmentation/overview""></iframe>
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.

[Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.

This year’s [Open Images V5](https://g.co/dataset/openimages) release enabled the second [Open Images Challenge](https://storage.googleapis.com/openimages/web/challenge2019.html) to include the following 3 tracks:

 1. [Object detection track](https://www.kaggle.com/c/open-images-2019-object-detection) for detecting bounding boxes around object instances, relaunched from 2018.

 2. [Visual relationship detection track](https://www.kaggle.com/c/open-images-2019-visual-relationship) for detecting pairs of objects in particular relations, also relaunched from 2018.

 3. [Instance segmentation track](https://www.kaggle.com/c/open-images-2019-instance-segmentation) for segmenting masks of objects in images, brand new for 2019.

Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.

#Instance Segmentation Track
In this track of the Challenge, you are asked **to provide segmentation masks of objects**.

This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art [interactive segmentation process](https://arxiv.org/pdf/1903.10830.pdf), where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.

<img alt=""wuxipark"" src=""https://storage.googleapis.com/kaggle-media/competitions/open-images/wuxipark.png"" width=""45%"" > <img alt=""catcafe"" src=""https://storage.googleapis.com/kaggle-media/competitions/open-images/catcafe.png"" width=""45%"" >

*Example train set annotations. Left: [Wuxi science park, 1995](https://www.flickr.com/photos/garysoup/3777131020) by [Gary Stevens](https://www.flickr.com/people/garysoup/). Right: [Cat Cafe Shinjuku calico](https://www.flickr.com/photos/picsoflife/6776736950) by [Ari Helminen](https://www.flickr.com/people/picsoflife/). Both images used under CC BY 2.0 license.*
<hr>
The results of this Challenge will be presented at a workshop at the [International Conference on Computer Vision](http://iccv2019.thecvf.com/).

We are excited to partner with Open Images for this second year of competitions, including this brand new track!
","
Submissions are evaluated by computing mean [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Average_precision), with the mean taken over the [300 segmentable classes of the challenge](https://storage.googleapis.com/openimages/challenge_2019/challenge-2019-classes-description-segmentable.csv). It follows the same spirit as the [Object Detection evaluation](https://www.kaggle.com/c/open-images-2019-object-detection/overview/evaluation), but takes into account mask-to-mask matching. The metric is [described in detail here](https://storage.googleapis.com/openimages/web/evaluation.html#instance_segmentation_eval). See also [this tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md#instance-segmentation-track) on running the evaluation in Python.


## Submission File

For each image in the test set, you must predict a list of instance segmentation masks and their associated detection score (`Confidence`). The submission csv file uses the following format:

<pre><code>ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2 ...
ImageBID,ImageBWidth,ImageBHeight,LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 …
</code></pre>

A sample with real values would be:
<pre><code>ImageID,ImageWidth,ImageHeight,PredictionString
721568e01a744247,1118,1600,/m/018xm 0.637833 eNqLi8xJM7BOTjS08DT2NfI38DfyM/Q3NMAJgJJ+RkBs7JecF5tnAADw+Q9I
7b018c5e3a20daba,1600,1066,/m/01g317 0.85117 eNqLiYrLN7DNCjDMMIj0N/Iz9DcwBEIDfyN/QyA2AAsBRfxMPcKTA1MMADVADIo=
</code></pre>


The binary segmentation masks are [run-length encoded](https://en.wikipedia.org/wiki/Run-length_encoding) (RLE), [zlib](https://en.wikipedia.org/wiki/Zlib) compressed, and [base64](https://en.wikipedia.org/wiki/Base64) encoded to be used in text format as `EncodedMask`. Specifically, we use the Coco masks RLE encoding/decoding (see the `encode` method of [COCO’s mask API](http://cocodataset.org/#download)), the zlib compression/decompression ([RFC1950](https://www.ietf.org/rfc/rfc1950.txt)), and vanilla base64 encoding.
  
An example python function to encode an instance segmentation mask would be:

<pre><code>import base64
import numpy as np
from pycocotools import _mask as coco_mask
import typing as t
import zlib


def encode_binary_mask(mask: np.ndarray) -> t.Text:
  """"""Converts a binary mask into OID challenge encoding ascii text.""""""
  
  # check input mask --
  if mask.dtype != np.bool:
    raise ValueError(
        ""encode_binary_mask expects a binary mask, received dtype == %s"" %
        mask.dtype)
  
  mask = np.squeeze(mask)
  if len(mask.shape) != 2:
    raise ValueError(
        ""encode_binary_mask expects a 2d mask, received shape == %s"" %
        mask.shape)
   
  # convert input mask to expected COCO API input --
  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
  mask_to_encode = mask_to_encode.astype(np.uint8)
  mask_to_encode = np.asfortranarray(mask_to_encode)
  
  # RLE encode mask --
  encoded_mask = coco_mask.encode(mask_to_encode)[0][""counts""]
  
  # compress and base64 encoding --
  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
  base64_str = base64.b64encode(binary_str)
  return base64_str
</code></pre>

(This code is [available as a gist here](https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c).)

There is a 5Gb file size limit on the submission csv file. This implicitly limits the number of detections to about 50~100 per image (on average)."
APTOS 2019 Blindness Detection,Detect diabetic retinopathy to stop blindness before it's too late ,https://www.kaggle.com/competitions/aptos2019-blindness-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/14774/logos/header.png?t=2019-06-21-17-16-37,"Image,Multiclass Classification,Medicine,Healthcare",2928,3507,71433,"
<img src=""https://storage.googleapis.com/kaggle-media/competitions/aravind"" style=""float: right; width: 200px"">

<p>Imagine being able to detect blindness before it happened.</p> 

<p>Millions of people suffer from <a href=""https://nei.nih.gov/health/diabetic/retinopathy"">diabetic retinopathy</a>, the leading cause of blindness among working aged adults. <b>Aravind Eye Hospital</b> in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospital’s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the <a href=""https://www.kaggle.com/c/aptos2019-blindness-detection/overview/aptos-2019"">4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium</a></p>

<p>Currently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be.</p>

<p>In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. You’ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.</p><p>

</p><p>Get started today!</p>
<p></p>","<p>Submissions are scored based on the&nbsp;quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0.&nbsp;The quadratic weighted kappa is calculated between the scores assigned by the human rater and the&nbsp;predicted scores.</p>
<p>Images&nbsp;have five&nbsp;possible ratings, 0,1,2,3,4.&nbsp; Each image&nbsp;is characterized by a tuple <em>(e</em>,<em>e)</em>, which corresponds to its scores by <em>Rater A</em> (human) and <em>Rater B</em> (predicted).&nbsp; The quadratic weighted kappa is calculated as follows. First, an N&nbsp;x&nbsp;N histogram matrix <em>O</em> is constructed, such that <em>O</em> corresponds to the number of images&nbsp;that received a rating <em>i</em> by<em>&nbsp;A</em> and a rating <em>j</em> by<em>&nbsp;B</em>.&nbsp;An <em>N-by-N </em>matrix of weights, <em>w</em>, is calculated based on the difference between raters' scores:</p>
<p></p>
<p>An <em>N-by-N</em> histogram matrix of expected ratings, <em>E</em>, is calculated, assuming that there is no correlation between rating scores.&nbsp; This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that <em>E</em> and <em>O</em> have the same sum.</p>
<p></p>
<p>Submissions should be formatted like:</p>
<p></p>
<pre>id_code,diagnosis<br>0005cfc8afb6,0<br>003f0afdcd15,0<br>etc.</pre>
"
Recursion Cellular Image Classification,CellSignal: Disentangling biological signal from experimental noise in cellular images,https://www.kaggle.com/competitions/recursion-cellular-image-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/14420/logos/header.png?t=2019-06-26-02-51-18,"Research,Biology,Image,Classification",865,1066,13232,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/recursion/Image%20within%20Description.png"" alt=""recursion"" style=""float: right"">
<p>The cost of some drugs and medical treatments has risen so high in recent years that many patients are having to go without. You can help with a classification project that could make researchers more efficient.</p>

<p>One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatments takes, on average, more than 10 years and costs hundreds of millions of dollars. </p>

<p>Recursion Pharmaceuticals, creators of the industry’s largest dataset of biological images, generated entirely in-house, believes AI has the potential to dramatically improve and expedite the drug discovery process. More specifically, your efforts could help them understand how drugs interact with human cells. </p>

<p>This competition will have you disentangling experimental noise from real biological signals. Your entry will classify images of cells under one of 1,108 different genetic perturbations. You can help eliminate the noise introduced by technical execution and environmental variation between experiments.</p>

<p>If successful, you could dramatically improve the industry’s ability to model cellular images according to their relevant biology. In turn, applying AI could greatly decrease the cost of treatments, and ensure these treatments get to patients faster.</p>

<p>This competition is a part of the <a href=""https://nips.cc/Conferences/2019/CallForCompetitions"">NeurIPS 2019 competition track</a>. Winners will be invited to contribute their solutions towards the workshop presentation.</p>

<h3>Acknowledgments</h3>

<p>Thank you to the following sponsors &amp; supporters of this competition:

</p><hr>

<img src=""https://storage.googleapis.com/kaggle-media/competitions/recursion/googlecloud.png"" alt=""Google Cloud"">

<p><b>Google Cloud</b>: Google Cloud is widely recognized as a global leader in delivering a secure, open and intelligent enterprise cloud platform. Our technology is built on Google’s private network and is the product of nearly 20 years of innovation in security, network architecture, collaboration, artificial intelligence and open source software. We offer a simply engineered set of tools and unparalleled technology across Google Cloud Platform and G Suite that help bring people, insights and ideas together. Customers across more than 150 countries trust Google Cloud to modernize their computing environment for today’s digital world.</p>

<hr>

<img src=""https://storage.googleapis.com/kaggle-media/competitions/recursion/doitintl.png"" alt=""DoiT"">

<p><b>DoiT</b>: You have the cloud and we have your back. For nearly a decade, we’ve been helping businesses build and scale cloud solutions with our world-class cloud engineering support. We help our customers with technical support and consulting on building and operating complex large-scale distributed systems, developing better machine learning models and setting up big data solutions using Google Cloud, Amazon AWS and Microsoft Azure.</p>

<hr>

<img src=""https://storage.googleapis.com/kaggle-media/competitions/recursion/nvidialogo.png"" alt=""NVIDIA"">

<p><b>NVIDIA</b>: NVIDIA’s (NASDAQ: NVDA) invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI — the next era of computing — with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at <a href=""http://nvidianews.nvidia.com"">http://nvidianews.nvidia.com</a>.</p>

<hr>

<img src=""https://storage.googleapis.com/kaggle-media/competitions/recursion/lambdalabs.png"" alt=""Lambda"">

<p><b>Lambda</b>: Lambda provides Deep Learning workstations, servers, and GPU cloud services. Lambda Deep Learning infrastructure is used by the world's leading AI research &amp; development organizations including Apple, Microsoft, MIT, Stanford, and the US Government. To learn more, visit <a href=""http://www.lambdalabs.com"">www.lambdalabs.com</a>.</p>

","Submissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.

## Submission File
For each `id_code` in the test set, you must predict the correct `sirna`. The file should contain a header and have the following format:

    id_code,sirna
    HEPG2-08_1_B03,911
    HEPG2-08_1_B04,911
    etc.
"
The 3rd YouTube-8M Video Understanding Challenge,Temporal localization of topics within video,https://www.kaggle.com/competitions/youtube8m-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/12716/logos/header.png?t=2019-04-29-17-47-47,Video Data,282,340,3747,"Imagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn’t title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby’s first steps or a game-winning goal -- and now we have the ability to quickly [find and share special video moments](https://ai.googleblog.com/2019/04/capturing-special-video-moments-with.html). This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.  

![ExampleImage](https://storage.googleapis.com/kaggle-media/competitions/yt8m-2019/image5.gif)

*An example of the detected action ""blowing out candles""*

In most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren’t always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as  improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others.

In previous years, participants worked on advancements in video-level annotations, building both [unconstrained](https://www.kaggle.com/c/youtube8m) and [constrained](https://www.kaggle.com/c/youtube8m-2018) models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?  

If successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear.

This competition is being hosted by [Google Research](https://ai.google/research/) as a part of the [International Conference on Computer Vision (ICCV) 2019](http://iccv2019.thecvf.com/) selected workshop session. Please refer to the [YouTube 8M Large-Scale Video Understanding Workshop Page](https://research.google.com/youtube8m/workshop2019/index.html) for details about the workshop.
","Submissions are evaluated according to the Mean Average Precision @ K (MAP@K), where \\( K = 100,000 \\).

$$MAP@100,000 = \frac{1}{C} \sum\_{c=1}^{C} \frac{\sum\_{k=1}^{n} P(k) \times rel(k)}{N\_{c}}$$

where \\( C \\) is the number of Classes, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number of Segments predicted per Class, \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) Class, or zero otherwise, and \\( N\_{c} \\) is the number of positively-labeled segments for the each Class.

**IMPORTANT:** The evaluation for this competition is different in some important ways:

 1. As noted above, for each `Class` you are predicting relevant `Segments` (and not the other way around).

 2. Not all test segments have been human-rated, and only human-rated segments are used in scoring. All segments that were not explicitly rated (as either containing a `Class`, or _not_ containing a `Class`) are removed from the prediction list before scoring.

 3. The Public/Private Test split is performed at the _Segment_ level, not the _Class_ level. In other words, all classes (i.e., submission rows) are evaluated for the Public and Private leaderboard, but only segments for the particular split will be used in the prediction and ground truth.


A python implementation of MAP@K can be found at [youtube-8m's github](https://github.com/google/youtube-8m/blob/master/mean_average_precision_calculator.py).

## Submission File

For each of the 1,000 `Class` values in the `sample_submission.csv` file, you may predict up to 100,000 `Segment` IDs that are predicted to contain that `Class`, sorted in order of confidence.

**IMPORTANT:** In order to minimize submission file sizes, for segment predictions, you should only include the video id and the segment start time, but _not_ the segment end time. (These are not needed, since all segments are 5 seconds in duration.)

The file should contain a header and have the following format:

    Class,Segments 
    3,002G:35 002G:40 002G:60 ... 
    7,002G:35 002G:40 002G:60  ...
    ..."
SIIM-ACR Pneumothorax Segmentation,Identify Pneumothorax disease in chest x-rays ,https://www.kaggle.com/competitions/siim-acr-pneumothorax-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/14652/logos/header.png?t=2019-05-17-18-25-20,Image,1475,538,1969,"Imagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer.

Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying—it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.

Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists.

The [Society for Imaging Informatics in Medicine (SIIM)](https://siim.org/) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help.

In this competition, you’ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives. 

If you’re up for the challenge, take a deep breath, and get started now.

**Note:** As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review [this tutorial](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/overview/siim-cloud-healthcare-api-tutorial) (or [in pdf format](https://storage.googleapis.com/kaggle-media/competitions/siim/SIIM%20Cloud%20Healthcare%20API%20Documentation.pdf)) for instructions on how to do so.

### Acknowledgments

**SIIM Machine Learning Committee Co-Chairs, Steven G. Langer, PhD, CIIP and George Shih, MD, MS** for tirelessly leading this effort and making the challenge possible in such a short period of time.

----------

**SIIM Machine Learning Committee Members** for their dedication in annotating the dataset, helping to define the most useful metrics and running tests to prepare the challenge for launch.

----------

**SIIM Hackathon Committee**, especially Mohannad Hussain, for their crucial technical support with data conversion.

----------

![ACR Logo](https://storage.googleapis.com/kaggle-media/competitions/siim/ACR%20logo.jpg) 

**American College of Radiology (ACR)**, @RadiologyACR: For Co-hosting the challenge and Co-sponsoring  the Prizes

----------

![STR Logo](https://storage.googleapis.com/kaggle-media/competitions/siim/STR%20logo.jpg) 

**Society of Thoracic Radiology (STR)**, @thoracicrad: For their unparalleled expertise in adjudicating the dataset

----------

![MD.ai Logo](https://storage.googleapis.com/kaggle-media/competitions/siim/MDai.jpg) 

**MD.ai**: For providing the annotation tool and helping with the first layer of annotations
","<p>This competition is evaluated on the mean <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"">Dice coefficient</a>. The Dice coefficient can be used to compare the pixel-wise&nbsp;agreement&nbsp;between a predicted segmentation&nbsp;and its corresponding ground truth. The formula is given by:</p>
<p>$$&nbsp;\frac{2 * |X \cap Y|}{|X| + |Y|},$$</p>
<p>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.</p>
<h2>Submission File</h2>
<p><strong>In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.</strong>&nbsp; Instead of submitting an exhaustive list of indices for your&nbsp;segmentation, you&nbsp;will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and&nbsp;running a total of 3 pixels (1,2,3).</p>
<p>The competition format requires&nbsp;a space delimited list of pairs. Note that this competition uses relative pixel positions, meaning that after the first pixel position, the remaining pixel values are simply offsets. For example, '1 3 10 5' implies pixels 1,2,3 are to be included in the mask, as well as 14,15,16,17,18. The metric checks that the pairs are sorted, positive, and the decoded&nbsp;pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.</p>
<p>The file should contain a header and have the following format:</p>
<pre>ImageId,EncodedPixels<br>0004d4463b50_01,1 1 5 1<br>0004d4463b50_02,1 1<br>0004d4463b50_03,1 1<br>etc.</pre>
<p>Submission files may take several minutes to process due to the size.</p>"
Open Images 2019 - Object Detection,Detect objects in varied and complex images,https://www.kaggle.com/competitions/open-images-2019-object-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/14315/logos/header.png?t=2019-04-24-05-13-32,"Image,Computer Vision",558,697,7375,"#Introduction

Computer vision has advanced considerably but is still challenged in matching the precision of human perception.

[Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.

This year’s [Open Images V5](https://g.co/dataset/openimages) release enabled the second [Open Images Challenge](https://storage.googleapis.com/openimages/web/challenge2019.html) to include the following 3 tracks:

 1. [Object detection track](https://www.kaggle.com/c/open-images-2019-object-detection) for detecting bounding boxes around object instances, relaunched from 2018.

 2. [Visual relationship detection track](https://www.kaggle.com/c/open-images-2019-visual-relationship) for detecting pairs of objects in particular relations, also relaunched from 2018.

 3. [Instance segmentation track](https://www.kaggle.com/c/open-images-2019-instance-segmentation) for segmenting masks of objects in images, brand new for 2019.

Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.

#Object Detection Track
In this track of the Challenge, you are asked **to predict a tight bounding box around object instances**.

The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).

![guitar](https://storage.googleapis.com/kaggle-media/competitions/open-images/guitarist.png)
![house](https://storage.googleapis.com/kaggle-media/competitions/open-images/table.png)

*Example annotations. Left: [Mark Paul Gosselaar plays the guitar](https://www.flickr.com/photos/rhysasplundh/5738556102) by [Rhys A](https://www.flickr.com/people/rhysasplundh/). Right: [the house](https://www.flickr.com/photos/krakluski/2950388100) by [anita kluska](https://www.flickr.com/photos/krakluski/). Both images used under CC BY 2.0 license.*

<hr>

Please refer to the [Open Images 2019 Challenge page](https://storage.googleapis.com/openimages/web/challenge2019.html) for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you.

The results of this Challenge will be presented at a workshop at the [International Conference on Computer Vision](http://iccv2019.thecvf.com/).

We are excited to partner with Open Images for this second year of competitions. See link here for last year’s [Object Detection](https://www.kaggle.com/c/google-ai-open-images-object-detection-track) competition.
","Submissions are evaluated by computing mean [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision) (mAP), modified to take into account the annotation process of [Open Images dataset](https://storage.googleapis.com/openimages/web/index.html) (mean is taken over per-class APs). The metric is described on the [Open Images Challenge website](https://storage.googleapis.com/openimages/web/evaluation.html#object_detection_eval). 

The final mAP is computed as the average AP over the 500 classes. The participants will be ranked on this final metric.
 
The metric is implemented as a part of [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). See [this Tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md) on running the evaluation in Python.

## Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as 

    ImageID,PredictionString
    ImageID,{Label Confidence XMin YMin XMax YMax} {...}"
Open Images 2019 - Visual Relationship,Detect pairs of objects in particular relationships,https://www.kaggle.com/competitions/open-images-2019-visual-relationship,https://storage.googleapis.com/kaggle-competitions/kaggle/14316/logos/header.png?t=2019-04-24-20-30-14,"Image,Computer Vision",200,246,2198,"#Introduction

Computer vision has advanced considerably but is still challenged in matching the precision of human perception.

[Open Images](https://storage.googleapis.com/openimages/web/factsfigures.html) is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.

This year’s [Open Images V5](https://g.co/dataset/openimages) release enabled the second [Open Images Challenge](https://storage.googleapis.com/openimages/web/challenge2019.html) to include the following 3 tracks:

 1. [Object detection track](https://www.kaggle.com/c/open-images-2019-object-detection) for detecting bounding boxes around object instances, relaunched from 2018.

 2. [Visual relationship detection track](https://www.kaggle.com/c/open-images-2019-visual-relationship) for detecting pairs of objects in particular relations, also relaunched from 2018.

 3. [Instance segmentation track](https://www.kaggle.com/c/open-images-2019-instance-segmentation) for segmenting masks of objects in images, brand new for 2019.

Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.

#Visual Relationship Track
In this track of the Challenge, you are asked **to detect pairs of objects and the relationships that connect them**.

The training set contains 329 relationship triplets with 375k training samples. These include both human-object relationships (e.g. ""woman playing guitar"", ""man holding microphone""), object-object relationships (e.g. ""beer on table"", ""dog inside car""), and also considers object-attribute relationships (e.g.""handbag is made of leather"" and ""bench is wooden"").

![man playing guitar](https://storage.googleapis.com/kaggle-media/competitions/open-images/man%20playing%20guitar.png)
![chair at table](https://storage.googleapis.com/kaggle-media/competitions/open-images/chair%20at%20table.png)

*Left: Example of ‘man playing guitar’ - [Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010](https://www.flickr.com/photos/tomjoad/4938460850) by [Andrea Sartorati](https://www.flickr.com/photos/tomjoad/). Right: Example of ‘chair at table’ - [Epic Fireworks - Loads A Room](https://www.flickr.com/photos/epicfireworks/4843249505) by [Epic Fireworks](https://www.flickr.com/photos/epicfireworks/)*

<hr>

Please refer to the [Open Images 2019 Challenge page](https://storage.googleapis.com/openimages/web/challenge2019.html) for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you.

The results of this Challenge will be presented at a workshop at the [International Conference on Computer Vision](http://iccv2019.thecvf.com/).

We are excited to partner with Open Images for this second year of competitions. See link here for last year’s [Visual Representation Detection](https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track) competition.
","Submissions are evaluated by computing the weighted mean of three metrics: mean [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision) (mAP) on relationships detection, [Recall@N](https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54) (where N=50), mean [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision) on phrase detection (mean in mean Average Precision is taken over per-relationship APs).
See more details about the metric on [Open Images Challenge website](https://storage.googleapis.com/openimages/web/evaluation.html#visual_relationships_eval).

The weights applied to each of the 3 metrics are `[0.4, 0.2, 0.4]`. 

The metric is implemented as a part of [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). See [this Tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md) on running the evaluation in Python.

## Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as 

      ImageId, PredictionString 
      ImageId, {Confidence Label1 XMin1 YMin1 XMax1 YMax1 Label2 XMin2 YMin2 XMax2 YMax2 RelationLabel}, {...}

"
Predicting Molecular Properties,Can you measure the magnetic interactions between a pair of atoms?,https://www.kaggle.com/competitions/champs-scalar-coupling,https://storage.googleapis.com/kaggle-competitions/kaggle/14313/logos/header.png?t=2019-05-16-16-54-31,"Chemistry,Tabular,Regression",2737,3296,47719,"<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/14313/logos/thumb76_76.png?t=2019-05-16-16-56-19"" style=""float: right; width: 180px"">

<p>Think you can use your data science smarts to make big predictions at a molecular level?</p> 

<p>This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules.</p>

<p>Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science. </p>

<p> This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication</p>

<p><b>Your Challenge</b></p>
<p>In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).</p>

<p>Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution.</p> 

<p><b>About Scalar Coupling</b></p>
<p>Using NMR to gain insight into a molecule’s structure and dynamics depends on the ability to accurately predict so-called “scalar couplings”. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule’s three-dimensional structure.</p> 

<p>Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows.</p>

<p>A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior. </p>

<p>Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease.</p>

<p>Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.</p>","Submissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.

$$score = \frac{1}{T} \sum\_{t=1}^{T} \log \left( \frac{1}{n\_{t}} \sum\_{i=1}^{n\_t} \lvert y\_i - \hat{y\_i} \rvert \right) $$

Where:

 - \\( T \\) is the number of scalar coupling types
 - \\( n\_{t}\\) is the number of observations of type \\( t \\) 
 - \\( y\_{i}\\) is the actual scalar coupling constant for the observation
 - \\( \hat{y_i}\\) is the predicted scalar coupling constant for the observation

For this metric, the MAE for any group has a floor of `1e-9`, so that the minimum (best) possible score for perfect predictions is approximately -20.7232.

## Submission File
For each `id` in the test set, you must predict the `scalar_coupling_constant` variable. The file should contain a header and have the following format:

    id,scalar_coupling_constant
    4659076,0.0
    4659077,0.0
    4659078,0.0
    etc.
"
Instant Gratification,A synchronous Kernels-only competition,https://www.kaggle.com/competitions/instant-gratification,https://storage.googleapis.com/kaggle-competitions/kaggle/14239/logos/header.png?t=2019-05-16-14-38-59,"Tabular,Binary Classification",1818,2021,35776,"Welcome to Instant (well, *almost*) Gratification!

In 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy--essentially a blinking cursor in a docker container--into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface.

We have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a [two-stage competition](https://www.kaggle.com/two-stage-frequently-asked-questions). However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work.

## Say Hello to Synchronous KO

We're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle:

 - With a truly withheld test set, we are practicing proper, rigorous machine learning.
 - We will be able to offer more varieties of competitions and intend to run many fewer confusing two-stage competitions.
 - You will be able to see if your code runs successfully on the withheld test set and have the leeway to intervene if it fails.
 - We will run all submissions against the private data, not just selected ones. Participants will get the complete and familiar public/private scores available in a traditional competition.
 - The final leaderboard can be released at the end of the competition, without the delay of rerunning Kernels.

This competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes.

Since the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions!
","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

    id,target
    ba88c155ba898fc8b5099893036ef205,0.5
    7cbab5cea99169139e7e6d8ff74ebb77,0.5
    7baaf361537fbd8a1aaa2c97a6d4ccc7,0.5
    etc.
"
Northeastern SMILE Lab - Recognizing Faces in the Wild,Can you determine if two individuals are related?,https://www.kaggle.com/competitions/recognizing-faces-in-the-wild,https://storage.googleapis.com/kaggle-competitions/kaggle/9992/logos/header.png?t=2019-01-09-19-48-28,"Image,Psychology",522,573,6911,"<p>Do you have your father’s nose? </p>

<p>Blood relatives often share facial features. Now researchers at Northeastern University want to improve their algorithm for facial image classification to bridge the gap between research and other familial markers like DNA results. That will be your challenge in this new Kaggle competition.</p>

<p>An automatic kinship classifier has been in the works at Northeastern since 2010. Yet this technology remains largely unseen in practice for a couple of reasons:</p>

<p>1. Existing image databases for kinship recognition tasks aren't large enough to capture and reflect the true data distributions of the families of the world.</p>

<p>2. Many hidden factors affect familial facial relationships, so a more discriminant model is needed than the computer vision algorithms used most often for higher-level categorizations (e.g. facial recognition or object classification).</p>

<p>In this competition, you’ll help researchers build a more complex model by determining if two people are blood-related based solely on images of their faces. If you think you can get it ""on the nose,"" this competition is for you.</p>
<hr>
<p><a href=""https://web.northeastern.edu/smilelab/"">The SMILE Lab</a> at Northeastern focuses on the frontier research of applied machine learning, social media analytics, human-computer interaction, and high-level image and video understanding. Their research is driven by the explosion of diverse multimedia from the Internet, including both personal and publicly-available photos and videos. They start by treating fundamental theory from learning algorithms as the soul of machine intelligence and arm it with visual perception. 
</p>","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target. Not all pairs will be scored.

## Submission File

For each `img_pair` in the test set, you must predict a probability for the `is_related` variable. The column `img_pair` describes the pair of images, i.e., `abcdef-ghijkl` means the pair of images `abcdef.jpg` and `ghijkl.jpg`. 

The file should contain a header and have the following format:

    img_pair,is_related
    X3Nk6Hfe5x-qcZrTXsfde,0.0
    X3Nk6Hfe5x-LD0pWDM8w_,0.0
    X3Nk6Hfe5x-PHwuDtHyGp,0.0
    X3Nk6Hfe5x-LO6lN_U4ot,0.0

    etc."
Freesound Audio Tagging 2019,Automatically recognize sounds and apply tags of varying natures,https://www.kaggle.com/competitions/freesound-audio-tagging-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/10700/logos/header.png?t=2019-03-08-16-57-14,Audio,880,506,645,"<p>One year ago, Freesound and Google’s Machine Perception hosted an audio tagging competition challenging Kagglers to build a general-purpose auto tagging system. This year they’re back and taking the challenge to the next level with multi-label audio tagging, doubled number of audio categories, and a <i>noisier than ever</i> training set. If you like raising your ML game, this challenge is for you.</p>

![Freesound][1]<div style=""text-align: center"" width=""100"">

<p>Here's the background: Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender?</p>

<p>Because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. A significant amount of manual effort goes into tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.</p>

<p>To tackle this problem, <a href=""https://freesound.org/"">Freesound</a> (an initiative by <a href=""https://www.upf.edu/web/mtg"">MTG-UPF</a> that maintains a collaborative database with over 400,000 Creative Commons Licensed sounds) and <a href=""https://research.google.com/teams/perception/"">Google Research’s Machine Perception Team</a>  (creators of <a href=""https://research.google.com/audioset/"">AudioSet</a>, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this new competition.</p>

<p>To win this competition, Kagglers will develop an algorithm to tag audio data automatically using a diverse vocabulary of 80 categories.</p> 

<p>If successful, your systems could be used for several applications, ranging from automatic labelling of sound collections to the development of systems that automatically tag video content or recognize sound events happening in real time.</p> 

<p>Ready to raise your game? Join the competition!</p>

<p>Note, this competition is similar in nature to <a href=""https://www.kaggle.com/c/freesound-audio-tagging"">this competition </a> with a new dataset, and multi-class labels. </p>

## Organizers

 - <a href=""http://www.eduardofonseca.net/"">Eduardo Fonseca</a>, <a href=""https://www.upf.edu/web/mtg"">MTG-UPF</a>, Barcelona
 - <a href=""https://ai.google/research/people/author8115"">Manoj Plakal</a>, <a href=""https://research.google.com/audioset////////about.html"">Google's Sound Understanding</a>, New York
 - <a href=""https://ffont.github.io/"">Frederic Font</a>, <a href=""https://www.upf.edu/web/mtg"">MTG-UPF</a>, Barcelona
 - <a href=""https://ai.google/research/people/DanEllis"">Dan Ellis</a>, <a href=""https://research.google.com/audioset////////about.html"">Google's Sound Understanding</a>, New York

>**This is a Kernels-only competition. Refer to <a href=""https://www.kaggle.com/c/freesound-audio-tagging-2019/overview/kernels-requirements"">Kernels Requirements</a> for details.**


  [1]: https://storage.googleapis.com/kaggle-media/competitions/freesound/task2_freesound_audio_tagging.png","The task consists of predicting the audio labels (tags) for every test clip. Some test clips bear one label while others bear several labels. The predictions are to be done at the clip level, i.e., no start/end timestamps for the sound events are required. 

The primary competition metric will be **label-weighted** [**label-ranking average precision**](https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision) (*lwlrap*, pronounced ""Lol wrap""). This measures the average precision of retrieving a ranked list of relevant labels for each test clip (i.e., the system ranks all the available labels, then the precisions of the ranked lists down to each true label are averaged). This is a generalization of the mean reciprocal rank measure (used in last year’s edition of the competition) for the case where there can be multiple true labels per test item. The novel ""label-weighted"" part means that the overall score is the average over all the *labels* in the test set, where each label receives equal weight (by contrast, plain *lrap* gives each *test item* equal weight, thereby discounting the contribution of individual labels when they appear on the same item as multiple other labels). 

We use label weighting because it allows per-class values to be calculated, and still have the overall metric be expressed as simple average of the per-class metrics (weighted by each label's prior in the test set). For participant’s convenience, a Python implementation of lwlrap is provided in this public [Google Colab](https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8).

## Submission File
For each `fname` in the test set, you must predict the probability of each label. The file should contain a header and have the following format:

    fname,Accelerating_and_revving_and_vroom,...Zipper_(clothing)
    000ccb97.wav,0.1,....,0.3
    0012633b.wav,0.0,...,0.8

As we will be switching out test data to re-evaluate kernels on stage 2 data to populate the private leaderboard, submissions must be named **submission.csv**."
Jigsaw Unintended Bias in Toxicity Classification,Detect toxicity across a diverse range of conversations,https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/12500/logos/header.png?t=2019-03-13-21-49-15,"Text,NLP",3165,3129,4377,"Can you help detect toxic comments ― *and* minimize unintended model bias? That's your challenge in this competition.

The Conversation AI team, a research initiative founded by [Jigsaw](https://jigsaw.google.com/) and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything *rude, disrespectful or otherwise likely to make someone leave a discussion*. 

Last year, in the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge#description), you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations. 

Here’s the background: When the Conversation AI team first built toxicity models, they found that the models [incorrectly learned to associate](https://medium.com/the-false-positive/unintended-bias-and-names-of-frequently-targeted-groups-8e0b81f80a23) the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. ""gay""), even when those comments were not actually toxic (such as ""I am a gay woman""). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.

In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.

*Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.*

### Acknowledgments
The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the [Online Hate Index Research Project](http://dh.berkeley.edu/projects/online-hate-index-ohi-research-project?utm_source=BCNM+Subscribers&utm_campaign=d5d78bba5e-natasha-schull-oct12_COPY_01&utm_medium=email&utm_term=0_eb59bfff9e-d5d78bba5e-281420185) at [D-Lab](https://dlab.berkeley.edu/), University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.  

> **This is a Kernels-only competition. Refer to [Kernels Requirements](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/Kernels-Requirements) for details.**","## Competition Evaluation

This competition will use a newly developed metric that combines several submetrics to balance overall performance with various aspects of unintended bias.

First, we'll define each submetric.

### Overall AUC
This is the ROC-AUC for the full evaluation set.

### Bias AUCs
To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias. You can learn more about these metrics in Conversation AI's recent paper *[Nuanced Metrics for Measuring Unintended Bias with Real Data in Text Classification](https://arxiv.org/abs/1903.04561)*.

**Subgroup AUC**: Here, we restrict the data set to only the examples that mention the specific identity subgroup. _A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity_.

**BPSN (Background Positive, Subgroup Negative) AUC**: Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. _A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not_, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.

**BNSP (Background Negative, Subgroup Positive) AUC**: Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. _A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not_, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.


### Generalized Mean of Bias AUCs
To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:

$$M\_p(m\_s) = \left(\frac{1}{N} \sum\_{s=1}^{N} m\_s^p\right)^\frac{1}{p}$$

where:

\\( M\_p \\)  = the \\(p\\)th power-mean function<br>
\\( m\_s \\)  = the bias metric \\( m \\) calulated for subgroup \\( s \\)<br>
\\( N \\)   = number of identity subgroups<br>

For this competition, we use a \\( p \\) value of -5 to encourage competitors to improve the model for the identity subgroups with the lowest model performance.

### Final Metric
We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:

$$score = w\_0 AUC_{overall} + \sum\_{a=1}^{A} w\_a M\_p(m\_{s,a})$$

where:

A   = number of submetrics (3)<br>
\\( m\_{s,a} \\)   = bias metric for identity subgroup \\( s \\) using submetric \\( a \\)<br>
\\( w\_a \\)  = a weighting for the relative importance of each submetric; all four \\(w\\) values set to 0.25<br>

**While the leaderboard will be determined by this single number, we highly recommend looking at the individual submetric results, [as shown in this kernel](https://www.kaggle.com/dborkan/benchmark-kernel), to guide you as you develop your models.**

## Submission File

    id,prediction
    7000000,0.0
    7000001,0.0
    etc."
iMet Collection 2019 - FGVC6,Recognize artwork attributes from The Metropolitan Museum of Art,https://www.kaggle.com/competitions/imet-2019-fgvc6,https://storage.googleapis.com/kaggle-competitions/kaggle/13251/logos/header.png?t=2019-03-14-16-01-52,"Image,Art",521,561,767,"The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. The online cataloguing information is generated by Subject Matter Experts (SME) and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the SME-generated annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes from the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.   


## About
This is an FGVCx competition hosted as part of the [FGVC6 workshop](https://sites.google.com/view/fgvc6/home) at [CVPR 2019](http://cvpr2019.thecvf.com/). View the [github page](https://github.com/visipedia/imet-fgvcx) for more details.

> **This is a Kernels-only competition. Refer to [Kernels Requirements](https://www.kaggle.com/c/imet-2019-fgvc6/overview/Kernels-Requirements) for details.**","Submissions will be evaluated based on their mean F2 score. The F score, commonly used in information retrieval, measures accuracy using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F2 score is given by:

$$\frac{(1 + \beta^2) pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 2.$$

Note that the F2 score weights recall higher than precision. The mean F2 score is formed by averaging the individual F2 scores for each id in the test set.

## Submission File
For each image in the test set, predict a space-delimited list of tags which you believe are associated with the image. The file should contain a header and have the following format:

    id,attribute_ids
    10023b2cc4ed5f68,0 1 2
    100fbe75ed8fd887,0 1 2
    101b627524a04f19,0 1 2
    etc...
"
Ciphertext Challenge II,553398 418126 467884 411 374106 551004 356535 539549 487091 290502 121468 556912 469347 515719 201909 101,https://www.kaggle.com/competitions/ciphertext-challenge-ii,https://storage.googleapis.com/kaggle-competitions/kaggle/13503/logos/header.png?t=2019-03-25-21-55-31,"Text,Internet",74,79,173,"## Ciphertext Challenge II: The Challengening!

It's baaaaaaack!

In our first [ciphertext competition][1], we hunted the wilds of the '90s-era internet. This time around, we're exploring the dark slow-broadband-y wastelands of 2011, with the [Movie Review Dataset][2]. In 2011 most of the internet hadn't even been *invented* yet\*, so wow, you're in for a treat.

Again, [simple classic ciphers][3] have been used to encrypt this dataset. Your mission this time: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Also, there are some new ciphers in play this time, which will involve some meta-puzzling. Enjoy!

Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the LB (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best *competition-related* kernels, in both visualization and cryptanalysis, based on upvotes.

Go ahead. Get cracking!

\* - This is not true.

## Acknowledgements
*Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. [online] Portland, Oregon, USA: Association for Computational Linguistics, pp. 142–150.* [Available here][4].


  [1]: https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge
  [2]: http://ai.stanford.edu/~amaas/data/sentiment/
  [3]: https://www.kaggle.com/c/ciphertext-challenge-ii/data
  [4]: http://www.aclweb.org/anthology/P11-1015.","Submissions are evaluated on [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) between the predicted plaintext `index` and the actual `index`.


## Submission File
For each ciphertext in the test set, you must predict the plaintext index. The file should contain a header and have the following format:

    ciphertext_id,index
    ID_0827e580b,0
    ID_bfcf14ce8,0
    ID_aab1e107a,0
    ID_061cc38c0,0
    ID_0e16534d3,0
    ID_b1387ef4a,0
    ID_e2275f924,0
    ID_d68e90960,0
    ID_0a3775e01,0
"
iWildCam 2019 - FGVC6,Categorize animals in the wild,https://www.kaggle.com/competitions/iwildcam-2019-fgvc6,https://storage.googleapis.com/kaggle-competitions/kaggle/12961/logos/header.png?t=2019-03-06-01-18-20,"Image,Multiclass Classification",336,389,2201,"


Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data?

In order to tackle this problem, we have prepared a challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. The species seen in each region overlap, but are not identical, and the challenge is to classify the test species correctly. To this end, we will allow training on our American Southwest data (from [CaltechCameraTraps][1]), on [iNaturalist 2017/2018][2] data, and on simulated data generated from [Microsoft AirSim][3]. We have provided a taxonomy file mapping our classes into the iNat taxonomy.

This is an FGVCx competition as part of the [FGVC6][4] workshop at [CVPR 2019][5], and is sponsored by [Microsoft AI for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth).  There is a github page for the competition [here][6]. Please open an issue if you have questions or problems with the dataset.

If you use this dataset in publication, please cite:
```
@article{beery2019iwildcam,
    title={The iWildCam 2019 Challenge Dataset},
    author={Beery, Sara and Morris, Dan and Perona, Pietro},
    journal={arXiv preprint arXiv:1907.07617},
    year={2019}
}
```

<p></p><p><br><i>Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.</i></p>

  [1]: https://beerys.github.io/CaltechCameraTraps/
  [2]: https://github.com/visipedia/inat_comp
  [3]: https://github.com/Microsoft/AirSim
  [4]: https://sites.google.com/view/fgvc6/home
  [5]: http://cvpr2019.thecvf.com/
  [6]: https://github.com/visipedia/iwildcam_comp","##Evaluation

Submissions will be evaluated based on their [macro F1 score](https://en.wikipedia.org/wiki/F1_score) - i.e. F1 will be calculated for each class of animal (including ""empty"" if no animal is present), and the submission's final score will be the unweighted mean of all class F1 scores.

## Submission Format

The submission format for the competition is a csv file with the following format:

    Id,Predicted
    58857ccf-23d2-11e8-a6a3-ec086b02610b,1
    591e4006-23d2-11e8-a6a3-ec086b02610b,5


The `Id` column corresponds to the test image id. The `Category` is an integer value that indicates the class of the animal, or `0` to represent the absence of an animal.
"
CareerCon 2019 - Help Navigate Robots ,Compete to get your resume in front of our sponsors,https://www.kaggle.com/competitions/career-con-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/13242/logos/header.png?t=2019-03-12-23-32-42,"Signal Processing,Tabular,Robotics",1443,1443,17165,"<p><b>CareerCon 2019 is upon us!</b></p>

<p>CareerCon is a digital event all about landing your first data science job — and <a href=""https://www.kaggle.com/careercon2019"">registration is now open!</a> Ahead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.</p>

<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/13242/logos/thumb76_76.png?t=2019-03-12-23-33-31"" alt=""“CareerCon”"" style=""float: right""></p>

<p>___________________________________</p>

<p><b>The Competition</b></p>

<p>Robots are smart… by design. To fully understand and properly navigate a task, however, they need input about their environment.</p>

<p>In this competition, you’ll help robots recognize the floor surface they’re standing on using data collected from Inertial Measurement Units (IMU sensors).</p>

<p>We’ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity.
Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won’t fall down on the job.</p> 

<p></p>
<b>Special thanks for making this competition possible:</b>
<p>The data for this competition has been collected by <a href=""http://www.cs.tut.fi/~hehu/"">Heikki Huttunen</a> and <a href=""https://tutcris.tut.fi/portal/en/persons/francesco-lomio(9a0f3b85-9c3d-44a0-ad24-c2646273495b)/publications.html"">Francesco Lomio</a> from the <a href=""https://www.tuni.fi/en/research/signal-processing-research-center"">Department of Signal Processing</a> and Damoon Mohamadi, Kaan Celikbilek, <a href=""https://tutcris.tut.fi/portal/en/persons/pedram-ghazi(67c5c491-7da1-4fcd-bd2b-bb2d4b7878cb).html"">Pedram Ghazi</a> and <a href=""https://tutcris.tut.fi/portal/en/persons/reza-ghabcheloo(4578be8d-8940-426f-8061-3caa8f482c77).html"">Reza Ghabcheloo</a> from the <a href=""https://www.tuni.fi/en/study-with-us/automation-engineering-factory-automation-and-robotics"">Department of Automation and Mechanical
  Engineering</a> both from <a href=""https://www.tuni.fi/en"">Tampere University</a>, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!</p>
","Submissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.

## Submission File
For each `series_id` in the test set, you must predict a value for the `surface` variable. The file should have the following format:

    series_id,surface
    0,fine_concrete
    1,concrete
    2,concrete
    etc.
"
Aerial Cactus Identification,Determine whether an image contains a columnar cactus,https://www.kaggle.com/competitions/aerial-cactus-identification,https://storage.googleapis.com/kaggle-competitions/kaggle/13435/logos/header.png?t=2019-03-07-17-24-10,"Earth and Nature,Image,Plants",1221,1312,6776,"To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the [VIGIA project](https://jivg.org/research-projects/vigia/), which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.

**This is a kernels-only competition, meaning you must submit predictions using Kaggle Kernels. [Read the basics here](https://www.kaggle.com/docs/competitions#submitting-predictions).**

### Acknowledgments

Kaggle is hosting this competition for the machine learning community to use for fun and practice. The original version of this data can be found [here](https://www.kaggle.com/irvingvasquez/cactus-aerial-photos), with details in the following paper:

Efren López-Jiménez, Juan Irving Vasquez-Gomez, Miguel Angel Sanchez-Acevedo, Juan Carlos Herrera-Lozada, Abril Valeria Uriarte-Arcia, Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach. Ecological Informatics. 2019.

Acknowledgements to Consejo Nacional de Ciencia y Tecnología. Project cátedra 1507. Instituto Politècnico Nacional. Universidad de la Cañada. Contributors: Eduardo Armas Garca, Rafael Cano Martnez and Luis Cresencio Mota Carrera. J.I. Vasquez-Gomez, JC. Herrera Lozada. Abril Uriarte, Miguel Sanchez.","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

## Submission File
For each ID in the test set, you must predict a probability for the `has_cactus` variable. The file should contain a header and have the following format:

    id,has_cactus
    000940378805c44108d287872b2f04ce.jpg,0.5
    0017242f54ececa4512b4d7937d1e21e.jpg,0.5
    001ee6d8564003107853118ab87df407.jpg,0.5
    etc."
Google Cloud & NCAA® ML Competition 2019-Men's,Apply Machine Learning to NCAA® March Madness®,https://www.kaggle.com/competitions/mens-machine-learning-competition-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/11999/logos/header.png?t=2019-02-12-18-56-32,"Basketball,Sports",862,947,1545,"<p> As a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
</p>
<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/march-madness-2018/lockup_cloud.png"" alt=""March Madness Imagery"" style=""float: right""></p>
<p>In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a <a href=""https://www.kaggle.com/datasets?modal=true"">dataset</a>. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.</p>
<p> As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on! </p>
<p><i>This page is for the NCAA Division I Men's tournament. Check out the <a href=""https://www.kaggle.com/c/womens-machine-learning-competition-2019/"">NCAA Division I Women's tournament here</a>. </i></p>","<p>Submissions are scored on the log loss:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2019 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2,278 matchups. </p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2014_1107_1110"" indicates team 1107 potentially played team 1110 in the year 2014. You must predict the probability that the team with the lower id beats the team with the higher id.</p>
<p>The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br>2014_1107_1110,0.5<br>2014_1107_1112,0.5<br>2014_1107_1113,0.5<br>...</pre>"
Google Cloud & NCAA® ML Competition 2019-Women's,Apply Machine Learning to NCAA® March Madness®,https://www.kaggle.com/competitions/womens-machine-learning-competition-2019,https://storage.googleapis.com/kaggle-competitions/kaggle/12000/logos/header.png?t=2019-02-12-18-48-27,"Basketball,Sports",497,524,925,"<p> As a result of the continued collaboration between Google Cloud and the NCAA®, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
</p>
<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/march-madness-2018/lockup_cloud.png"" alt=""March Madness Imagery"" style=""float: right""></p>
<p>In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a <a href=""https://www.kaggle.com/datasets?modal=true"">dataset</a>. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.</p>
<p> As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on! </p>
<p><i>This page is for the NCAA Division I Women's tournament. Check out the <a href=""https://www.kaggle.com/c/mens-machine-learning-competition-2019/"">NCAA Division I Men's tournament here</a>. </i></p>","

<p>Submissions are scored on the log loss:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2019 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2 &nbsp;= 2,016 matchups.&nbsp;</p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_3104_3129"" indicates team 3104&nbsp;played team 3129&nbsp;in the year 2013. You must&nbsp;predict the probability that the team with the lower id beats the team with the higher id.</p>
<p>The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br>2013_3103_3107,0.5<br>2013_3103_3112,0.5<br>2013_3103_3125,0.5<br>...</pre>"
Santander Customer Transaction Prediction,Can you identify who will make a transaction?,https://www.kaggle.com/competitions/santander-customer-transaction-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/10385/logos/header.png?t=2018-09-12-18-50-51,"Banking,Tabular,Binary Classification",8751,9787,104121,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/santander/atm_image.png"" alt=""atms"" style=""float: right""></p>

<p>At <a href=""https://www.santanderbank.com"">Santander </a> our mission is to help people and businesses prosper.  We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals. </p>
<p>Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure  we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?</p>

<p>In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.</p>","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

## Submission File
For each Id in the test set, you must make a binary prediction of the target variable. The file should contain a header and have the following format:

     ID_code,target
     test_0,0
     test_1,1
     test_2,0
     etc.
"
Don't Overfit! II,A Fistful of Samples,https://www.kaggle.com/competitions/dont-overfit-ii,https://storage.googleapis.com/kaggle-competitions/kaggle/12896/logos/header.png?t=2019-02-05-18-26-29,"Tabular,Binary Classification",2315,2441,35262,"### *Long ago, in the distant, fragrant mists of time, there was a competition...*

It was not just any competition.

It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples... _without overfitting_.

Data scientists ― including Kaggle's very own Will Cukierski ― competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse.

So... we're doing it again.

## Don't Overfit II: The Overfittening

This is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you _not to overfit_. Do your best, model without overfitting, and add, perhaps, to your own legend.

In addition to bragging rights, the winner also gets swag.  Enjoy!

### Acknowledgments

We hereby salute the hard work that went into the [original competition][1], created by Phil Brierly. Thank you!


  [1]: https://www.kaggle.com/c/overfitting
","Submissions are evaluated using [AUCROC](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted target and the actual target value.


## Submission File
For each `id` in the test set, you must predict a probability for the `target` variable. The file should contain a header and have the following format:

     id,target
     300,0
     301,0
     302,0
     303,0
     304,0
     305,0
     306,0
     307,0
     308,0"
TMDB Box Office Prediction,Can you predict a movie's worldwide box office revenue?,https://www.kaggle.com/competitions/tmdb-box-office-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/10300/logos/header.png?t=2019-02-05-19-35-41,"Tabular,Movies and TV Shows",1395,1615,18974,"We're going to make you an offer you can't refuse: a Kaggle competition! 

In a world... where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's ""You had me at 'Hello.'"" For others, the trailer falls short of expectations and you think ""What we have here is a failure to communicate.""

In this competition, you're presented with metadata on over 7,000 past films from [The Movie Database][1] to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.  You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.

Join in, ""make our day"", and then ""you've got to ask yourself one question: 'Do I feel lucky?'""

  [1]: https://www.themoviedb.org
","<p>It is your job to predict the international box office revenue for each movie.&nbsp;For each <code>id</code> in the test set, you must predict the value of the revenue variable.&nbsp;</p>
<p>Submissions are evaluated on <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" target=""_blank"">Root-Mean-Squared-Logarithmic-Error (RMSLE)</a>&nbsp;between the predicted value&nbsp;and the actual revenue. Logs are taken to not overweight blockbuster revenue movies. </p>
<h3>Submission File Format</h3>
<p>The file should contain a header and have the following format:</p>
<pre>id,revenue<br>1461,1000000<br>1462,50000<br>1463,800000000<br>etc.</pre>
<p>You can download an example submission file (sample_submission.csv) on the&nbsp;<a href=""https://www.kaggle.com/c/tmdb-box-office-prediction/data"" target=""_blank"">Data page.</a></p>"
Gendered Pronoun Resolution,Pair pronouns to their correct entities,https://www.kaggle.com/competitions/gendered-pronoun-resolution,https://storage.googleapis.com/kaggle-competitions/kaggle/12797/logos/header.png?t=2019-02-01-18-45-25,"Text,NLP",838,329,617,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/GoogleAI-GenderedPronoun/PronounResolution.png"" alt=""PronounResolution"" style=""float: right"">

<p>Can you help end gender bias in pronoun resolution?

</p><p>Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.

</p><p>Unfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers. <a href=""https://ai.google/research/teams/language/"" target=""_blank""> Google AI Language</a> aims to improve gender-fairness in modeling by releasing the <a href=""http://goo.gl/language/gap-coreference"" target=""_blank"">Gendered Ambiguous Pronouns (GAP) dataset</a>, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns).

</p><p>In this <a href=""https://www.kaggle.com/two-stage-frequently-asked-questions"" target=""_blank"">two-stage competition</a>, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time.


</p><p>----------


</p><p>Please cite the original paper if you use GAP in your work:

</p><div><pre>@inproceedings{webster2018gap,
  title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns},
  author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},
  booktitle = {Transactions of the ACL},
  year =      {2018},
  pages =     {to appear},
}
</pre></div>
","<p>Submissions are evaluated using the multi-class logarithmic loss. Each pronoun&nbsp;has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of samples&nbsp;in the test set, M is 3,&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the probabilities that a pronoun refers to A, B, or NEITHER. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>ID,A,B,NEITHER
development-1,0.33333,0.33333,0.33333
development-2,0.33333,0.33333,0.33333
development-3,0.33333,0.33333,0.33333
etc.
</pre>"
LANL Earthquake Prediction,Can you predict upcoming laboratory earthquakes?,https://www.kaggle.com/competitions/LANL-Earthquake-Prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/11000/logos/header.png?t=2019-01-04-23-26-44,"Physics,Earth Science,Signal Processing",4516,5454,59891,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/LANL/nik-shuliahin-585307-unsplash.jpg"" alt=""map"" style=""float: right"">
<p>Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: <b>when</b> the event will occur, <b>where</b> it will occur, and <b>how large</b> it will be.</p>  

<p>In this competition, you will address <b>when</b> the earthquake will take place. Specifically, you’ll predict the time remaining before laboratory earthquakes occur from real-time seismic data. </p>

<p>If this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure.</p>

<p>This challenge is hosted by  <a href=""https://www.lanl.gov/"">Los Alamos National Laboratory</a> which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.</p>


<p><b> Acknowledgments:</b> </p>



<table>
 <tbody><tr>
   <td><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/11000/logos/thumb76_76.png?t=2019-01-03-23-31-16"" alt=""LANL"" style=""float: left""></td>
   <td><b>Geophysics Group:</b> The competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data for the competition.</td>
</tr>
<tr>
   <td><a href=""https://www.psu.edu//""><img src=""https://storage.googleapis.com/kaggle-media/competitions/LANL/PS-HOR-RGB-2C.png"" alt=""Penn State""></a></td>
   <td><b>Department of Geosciences: </b>Data are from experiments performed by  Chas Bolton, Jacques Riviere, Paul Johnson and Prof. Chris Marone.</td>
</tr>
<tr>
   <td><a href=""https://www.purdue.edu/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/LANL/PurdueCropped.png"" alt=""Purdue""></a></td>
   <td><b>Department of Physics &amp; Astronomy:</b> This competition stemmed from the DOE Council workshop “Information is in the Noise: Signatures of Evolving Fracture and Fracture Networks” held March 2018 that was organized by Prof. Laura J. Pyrak-Nolte.</td>
</tr>
<tr>
   <td><b><h2>Department of Energy</h2></b></td>
   <td><b>Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division:</b> The Geosciences core research.</td>
</tr>
</tbody></table>

Photo by Nik Shuliahin on Unsplash
","Submissions are evaluated using the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) between the predicted time remaining before the next lab earthquake and the act remaining time.

## Submission File
For each `seg_id` in the test set folder, you must predict `time_to_failure`, which is the remaining time before the next lab earthquake. The file should contain a header and have the following format:

    seg_id,time_to_failure
    seg_00030f,0
    seg_0012b5,0
    seg_00184e,0
    ..."
PetFinder.my Adoption Prediction,How cute is that doggy in the shelter?,https://www.kaggle.com/competitions/petfinder-adoption-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/10686/logos/header.png?t=2018-12-19-11-59-43,"Image,Text",2023,2329,3136,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/Petfinder/PetFinder%20-%20Logo.png
"" alt=""Petfinder"" style=""width: 60%""></p>

<p>Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved — and more happy families created.</p>
<p><a href=""https://PetFinder.my/"">PetFinder.my</a> has been Malaysia’s leading animal welfare platform since 2008, with a database of more than 150,000 animals. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.</p>
<p>Animal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos.</p> 
<p>In this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization.</p>
<p>Top participants may be invited to collaborate on implementing their solutions into AI tools for assessing and improving pet adoption performance, which will benefit global animal welfare.</p>

<h3>Important Note</h3>
<p>Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output.  

</p><p> Photo by Krista Mangulsone on Unsplash </p>","<p> As we will be switching out test data to re-evaluate kernels on stage 2 data to populate the private leaderboard, submissions must be named <b>submission.csv</b></p>
<p>Submissions are scored based on the <b>quadratic weighted kappa</b>, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores which are expected/known and the predicted scores.</p>
<p>Results have 5 possible ratings, 0,1,2,3,4.  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix <em>O</em> is constructed, such that <em>O<sub>i,j</sub></em> corresponds to the number of adoption records that have a rating  of <em>i</em> (<em>actual</em>) and received a <em>predicted</em> rating <em>j</em>. An <em>N-by-N </em>matrix of weights, <em>w</em>, is calculated based on the difference between actual and predicted rating scores:</p>
<p><span>$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$</span></p>
<p>An <em>N-by-N</em> histogram matrix of expected ratings, <em>E</em>, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings, normalized such that <em>E</em> and <em>O</em> have the same sum.</p>
<p>From these three matrices, the quadratic weighted kappa is calculated as: </p>
<p><span>$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}.$$</span></p>
<h2><span>Submission Format</span></h2>
<p><span>You must submit a csv file with the product id and a predicted search relevance for each search record. The order of the rows does not matter. The file must have a header and should look like the following:</span></p>
<pre>PetID,AdoptionSpeed<br />378fcc4fc,3<br />73c10e136,2<br />72000c4c5,1<br />e147a4b9f,4<br />etc..</pre>"
VSB Power Line Fault Detection,Can you detect faults in above-ground electrical lines?,https://www.kaggle.com/competitions/vsb-power-line-fault-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/10684/logos/header.png?t=2018-12-12-21-49-25,"Tabular,Binary Classification,Signal Processing",1445,1589,19481,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/Enet%20Centre/logo_vyska.png"" alt=""logo"" style=""float: right""></p>

<p>Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge — an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.</p>

<p> Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the <a href=""http://cenet.vsb.cz/en/"">ENET Centre</a> at <a href=""https://www.vsb.cz/en/university/who-we-are/"">VŠB</a>. Effective classifiers using this data will make it possible to continuously monitor power lines for faults.</p>

<p>ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials.</p>

<p>By developing a solution to detect partial discharge you’ll help reduce  maintenance costs, and prevent power outages.</p>
","<p>Submissions are evaluated on the <a href=""https://en.wikipedia.org/wiki/Matthews_correlation_coefficient"">Matthews correlation coefficient</a> (MCC) between the predicted and the observed response. The MCC is given by:</p>
<p>$$ MCC = \frac{(TP*TN) - (FP * FN)}{\sqrt{(TP+FP)(TP+FN)(TN + FP)(TN+FN)}}, $$</p>
<p>where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.</p>
<h2>Submission File</h2>
<p>For each signal in the test set, you must predict a binary prediction for the target variable. The file should contain a header and have the following format:</p>
<pre>signal_id,target<br />0,0<br />1,1<br />2,0<br />etc.</pre>"
20 Newsgroups Ciphertext Challenge,V8g{9827$A${?^*?}$$v7*.yig$w9.8},https://www.kaggle.com/competitions/20-newsgroups-ciphertext-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/12133/logos/header.png?t=2018-11-29-02-51-36,"Multiclass Classification,Text",142,145,1139,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/playground/20-newsgroups-ciphertext-challenge/enigma_small.jpg"" alt=""Inside an Enigma machine."" style=""float: right""></p>

<p>This isn't your classic decoder ring puzzle found in a cereal box. There's a twist.</p>

<p>Welcome to the Ciphertext Challenge! In this competition, we've encrypted parts of a well-known dataset -- the <code>20 Newsgroups</code> dataset -- with several <a href=""https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge/data"">simple, classic ciphers</a>.  This dataset is commonly used as a multi-class and NLP sample set, noted for its small size, varied nature, and the first-hand look it offers into the deep existential horrors of the 90s-era internet. With 20 fairly distinct classes and lots of clues, it allows for a wide variety of successful approaches.</p>

<p>We've made the problem a little harder to solve.</p>

<p>Fabulous Kaggle swag will go to the top competitors - the highest-scoring teams (which might be the first to crack the code!), and the most popular kernel.  Note that this is a <em>short</em> competition, so use your submissions wisely.</p>

<p>* = Note: It is possible to apply a number of techniques using ONLY the ciphertext.</p>

<h3>Acknowledgements</h3>

<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. </p>

<p>You can view and download the unencrypted dataset from <a href=""http://qwone.com/~jason/20Newsgroups/"">Jason Rennie's homepage</a>.  In the words of the host:</p>

<p><strong>The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his <em>Newsweeder: Learning to filter netnews</em> paper, though he does not explicitly mention this collection.</strong></p>

<p>If you use the dataset in a scientific publication, please reference (at a minimum) the above website.</p>

<p>Photo by <a href=""https://www.usafa.af.mil/News/Photos/igphoto/2000255213/"">U.S. Air Force photo/Don Branum</a></p>","Submissions will be evaluated based on their [macro F1 score](https://en.wikipedia.org/wiki/F1_score).


## Submission File
For each ID in the test set, you must predict which newsgroup a particular item came from. The file should contain a header and have the following format:

    Id,Predicted
    ID_e93d1d4c6,0
    ID_f5f7560ec,0
    ID_6ebe8f07f,0
    ID_26222d9b7,0
    ID_a0632653b,0
    ID_641f090da,0
    ID_888b6c7a5,0
    ID_7b5b3bb47,0
    ID_70f08430c,0
    etc.
"
Humpback Whale Identification,Can you identify a whale by its tail?,https://www.kaggle.com/competitions/humpback-whale-identification,https://storage.googleapis.com/kaggle-competitions/kaggle/6961/logos/header.png,"Image,Animals",2120,2451,37466,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/happy-whale.jpg"" alt=""Planet Aerial Imagery"" style=""float: right""></p>
<p>After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.</p>
<p>To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.</p>
<p>In this competition, you’re challenged to build an algorithm to identify individual whales in images. You’ll analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe.</p>
<p>Note, this competition is similar in nature to <a href=""https://www.kaggle.com/c/whale-categorization-playground"">this competition </a> with an expanded and updated dataset. </p>

<p>We'd like to thank <a href=""https://happywhale.com/"">Happywhale </a> for providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.

</p>","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):

$$MAP@5 = \frac{1}{U} \sum\_{u=1}^{U}  \sum\_{k=1}^{min(n,5)} P(k) \times rel(k)$$

where \\( U \\)  is the number of images, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number predictions per image, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) label, zero otherwise.

Once a correct label has been scored for *an observation*, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.

    [A, B, C, D, E]
    [A, A, A, A, A]
    [A, B, A, C, A]


## Submission File

For each `Image`&nbsp;in the test set, you may predict up to 5 labels for the whale `Id`. Whales that are not predicted to be one of the labels in the training data should be labeled as `new_whale`. The file should contain a header and have the following format:

    Image,Id 
    00028a005.jpg,new_whale w_23a388d w_9b5109b w_9c506f6 w_0369a5c 
    000dcf7d8.jpg,new_whale w_23a388d w_9b5109b w_9c506f6 w_0369a5c 
    ...
"
Elo Merchant Category Recommendation,Help understand customer loyalty,https://www.kaggle.com/competitions/elo-merchant-category-recommendation,https://storage.googleapis.com/kaggle-competitions/kaggle/10445/logos/header.png?t=2018-10-24-17-13-50,"Tabular,Regression,Banking",4110,4712,81771,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/10445/logos/thumb76_76.png?t=2018-10-24-17-14-05"" alt=""TGS"" style=""float: right""></p>

<p>Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner!</p>

<p>Right now, <a href=""https://www.cartaoelo.com.br/"">Elo</a>, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.
</p>

<p>Elo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in.</p>

<p>In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers.</p>

","### Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
<p>\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]</p>
where \\( \hat{y} \\) is the predicted loyalty score for each `card_id`, and \\( y \\) is the actual loyalty score assigned to a `card_id`.

## Submission File

    card_id, target
    C_ID_9e86007114,0
    C_ID_1c9f77086c,0.5
    C_ID_07b20e9908,0
    C_ID_63d6bac69a,0
    C_ID_bbc26a86eb,0
    C_ID_f749aad790,0
    C_ID_7b5c15ff41,-0.25
    C_ID_ec6b0f2d30,0
    C_ID_0a11e759c5,0
"
Don't call me turkey!,Thanksgiving Edition: Find the turkey in the sound bite,https://www.kaggle.com/competitions/dont-call-me-turkey,https://storage.googleapis.com/kaggle-competitions/kaggle/11880/logos/header.png?t=2018-11-15-19-27-55,"Binary Classification,Tabular,Animals",266,277,1001,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/turkey/chan-swan-481027-unsplash.jpg"" alt=""TGS"" style=""float: right""></p>

<p>Hungry for a new competition? Give thanks for this opportunity to avoid those awkward family political dinner discussions and endless holiday movie marathons over the Thanksgiving break. Spend time with your Kaggle family instead to find the real turkey! </p>

<p>In this competition you are tasked with finding the turkey sound signature from pre-extracted audio features. A simple binary problem, or is it? What does a turkey really sound like? How many sounds are similar? Will you be able to find the turkey or will you go a-fowl? </p> 

<p>This is a short, fun, holiday, playground competition. Please, do not ruin the fun for yourself and for everyone by using a model trained on the answers.  Don't be a turkey!</p>
","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.


## Submission File
For each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:

      vid_id,is_turkey
      pyKh38FXD3E,0
      THhP1idrWXA,0.3
      etc.
"
Traveling Santa 2018 - Prime Paths,"But does your code recall, the most efficient route of all?",https://www.kaggle.com/competitions/traveling-santa-2018-prime-paths,https://storage.googleapis.com/kaggle-competitions/kaggle/10733/logos/header.png?t=2018-10-19-18-23-20,Optimization,1867,2042,21235,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/santa/Reindeer.jpg"" style=""float: right""> 

<p>Rudolph the red-nosed reindeer
<br>Had some very tired hooves
<br>But he had a job to finish
<br>Could he do it with the shortest moves?</p>

<p>All of the other reindeer
<br>Used to laugh and mock his code
<br>They always said poor Rudolph
<br>Couldn't handle the workload</p>

<p>Then one foggy Christmas Eve
<br>Santa came to say
<br>I see you've taken number theory
<br>Please make this night a bit less dreary?</p>

<p>Then how the reindeer loved him
<br>and each enrolled in an AI degree
<br>Rudolph the red-nosed reindeer
<br>We get to go to bed early!</p>

<p>Rudolph has always believed in working smarter, not harder. And what better way to earn the respect of Comet and Blitzen than showing the initiative to improve Santa's annual route for delivering toys on Christmas Eve?</p>

<p>This year,  Rudolph believes he can motivate the overworked Reindeer team by wisely choosing the order in which they visit the houses on Santa's list. The houses in <em>prime</em> cities always leave carrots for the Reindeers alongside the usual cookies and milk. These carrots are just the sustenance the Reindeers need to keep pace. In fact, Rudolph has found that if the Reindeer team doesn't originate from a prime city exactly every 10th step, it takes the 10% longer than it normally would to make their next destination!</p>

<p>Can you help Rudolph solve the Traveling Santa problem subject to his carrot constraint? His team--and Santa--are counting on you!</p>

<h3>Attributions:</h3>
<p>Reindeer Photo:&nbsp;<a href=""https://unsplash.com/photos/KBKHXjhVQVM"">Norman Tsui</a><br>
Stocking Photo: <a href=""https://unsplash.com/photos/mPXdLgrHOmc""> Wesley Tingey</a>
</p>","Your submission is scored on the Euclidean distance of your submitted path, subject to the constraint that every 10th step is 10% more lengthy unless coming from a [prime](https://en.wikipedia.org/wiki/Prime_number) `CityId`.

## Submission file
Your submission file contains the ordered `Path` that Santa should use to visit all the cities. Paths must start and end at the North Pole (`CityId = 0`) and you must visit every city exactly once. Submission files must have a header and should look like:

    Path
    0
    1
    2
    ...
    0

"
Histopathologic Cancer Detection,Identify metastatic tissue in histopathologic scans of lymph node sections,https://www.kaggle.com/competitions/histopathologic-cancer-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/11848/logos/header.png?t=2018-11-15-01-52-19,"Cancer,Medicine,Research",1149,1347,20132,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/playground/Microscope"" alt=""Microscope"" style=""float: right"">

In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) <a href=""https://github.com/basveeling/pcam"">benchmark dataset</a> (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).

</p><p>PCam is highly interesting for both its size, simplicity to get started on, and approachability. In the authors' words:</p>

<p></p><blockquote> [PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability. </blockquote><p></p>

<p></p><h3> Acknowledgements </h3><p></p>

<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Bas Veeling, with additional input from Babak Ehteshami Bejnordi, Geert Litjens, and Jeroen van der Laak. </p>

<p>You may view and download the official Pcam dataset from  <a href=""https://github.com/basveeling/pcam"">GitHub</a>. The data is provided under the <a href=""https://choosealicense.com/licenses/cc0-1.0/"">CC0 License</a>, following the license of Camelyon16.</p>

<p>If you use PCam in a scientific publication, please reference the following papers:</p>

<p>[1] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. ""Rotation Equivariant CNNs for Digital Pathology"". <a href=""http://arxiv.org/abs/1806.03962""> arXiv:1806.03962</a></p>

<p>[2] Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199–2210. <a href=""https://doi.org/10.1001/jama.2017.14585"">doi:jama.2017.14585</a></p>

<p>Photo by <a href=""https://unsplash.com/photos/gKUC4TMhOiY"">Ousa Chea</a></p>","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

## Submission File
For each `id` in the test set, you must predict a probability that center 32x32px region of a patch contains at least one pixel of tumor tissue. The file should contain a header and have the following format:

    id,label
    0b2ea2a822ad23fdb1b5dd26653da899fbd2c0d5,0
    95596b92e5066c5c52466c90b69ff089b39f2737,0
    248e6738860e2ebcf6258cdc1f32f299e0c76914,0
    etc.

"
Quora Insincere Questions Classification,Detect toxic content to improve online conversations,https://www.kaggle.com/competitions/quora-insincere-questions-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/10737/logos/header.png?t=2018-10-24-22-07-48,"Text,Binary Classification",4037,1726,2506,"
<p><img src=""https://storage.googleapis.com/kaggle-organizations/407/thumbnail.png?r=95"" alt=""Quora"" style=""float: right""></p>

<p>An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.</p>

<p><a href=""https://www.quora.com/"">Quora</a> is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.</p>

<p>In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.</p>

<p>Here's your chance to combat online trolls at scale. Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge.</p>

<h3>Important Note</h3>
<p>Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output. Please read the <a href=""https://www.kaggle.com/c/quora-insincere-questions-classification#Kernels-FAQ"">Kernels FAQ</a> and the <a href=""https://www.kaggle.com/c/quora-insincere-questions-classification/data"">data page</a> very carefully to fully understand how this is designed.</p>","Submissions are evaluated on [F1 Score](https://en.wikipedia.org/wiki/F1_score) between the predicted and the observed targets.

## Submission File
For each `qid` in the test set, you must predict whether the corresponding `question_text` is insincere (`1`) or not (`0`). Predictions should only be the integers `0` or `1`. The file should contain a header and have the following format:

    qid,prediction
    0000163e3ea7c7a74cd7,0
    00002bd4fb5d505b9161,0
    00007756b4a147d2b0b3,0
    ...

## Kernel Submissions
For this competition, you will make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them. For more details, please visit the [Kernels-FAQ](https://www.kaggle.com/c/quora-insincere-questions-classification#Kernels-FAQ) for this competition."
PUBG Finish Placement Prediction (Kernels Only),Can you predict the battle royale finish of PUBG Players?,https://www.kaggle.com/competitions/pubg-finish-placement-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/10335/logos/header.png?t=2018-10-02-21-04-56,"Video Games,Tabular",1528,1772,12747,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/PUBG/PUBG%20Inlay.jpg"" alt=""PUBG"" style=""float: right""></p>

<p>So, where we droppin' boys and girls?</p>

<p>Battle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink. </p>

<p>PlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.  </p>

<p>The team at <a href=""https://www.pubg.com"">PUBG</a> has made official game data available for the public to explore and scavenge outside of ""The Blue Circle."" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the <a href=""https://developer.pubg.com"">PUBG Developer API</a>.</p> 

<p>You are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings. </p>

<p>What's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!</p>","Submissions are evaluated on [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) between your predicted winPlacePerc and the observed winPlacePerc.

## Submission File
For each Id in the test set, you must predict their placement as a percentage (0 for last, 1 for first place) for the winPlacePerc variable. The file should contain a header and have the following format:

      Id,winPlacePerc
      47734,0
      47735,0.5
      47736,0
      47737,1
      etc.

See sample_submission.csv on the data page for a full sample submission."
Human Protein Atlas Image Classification,Classify subcellular protein patterns in human cells,https://www.kaggle.com/competitions/human-protein-atlas-image-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/10418/logos/header.png?t=2018-08-20-03-03-58,"Image,Classification",2160,2679,54934,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/proteins/description_NACC_cropped_opt.png"" alt=""Protein"" style=""float: right""></p>
<p>In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images.&nbsp;<a href=""https://www.proteinatlas.org/"">The Human Protein Atlas</a>&nbsp;will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image.</p>
<p>Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells.</p>
<p>Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.</p>
<p>&nbsp;</p>
<p><em><a href=""https://www.nature.com/nmeth"">Nature Methods</a>&nbsp;has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper.</em></p>
<p><em>Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.</em></p>
<p>&nbsp;</p>
<p></p><h3>Acknowledgements</h3><p></p>
<p>The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (<a href=""https://www.nature.com/articles/nbt.4225"">Sullivan et al, Nature Biotechnology, Oct 2018</a>).</p>","Submissions will be evaluated based on their [macro F1 score](https://en.wikipedia.org/wiki/F1_score).

## Submission File
For each Id in the test set, you must predict a class for the `Target` variable as described in [the data page](https://www.kaggle.com/c/human-protein-atlas-image-classification/data).  Note that multiple labels can be predicted for each sample.

The file should contain a header and have the following format:

    Id,Predicted  
    00008af0-bad0-11e8-b2b8-ac1f6b6435d0,0 1  
    0000a892-bacf-11e8-b2b8-ac1f6b6435d0,2 3
    0006faa6-bac7-11e8-b2b7-ac1f6b6435d0,0  
    0008baca-bad7-11e8-b2b9-ac1f6b6435d0,0  
    000cce7e-bad4-11e8-b2b8-ac1f6b6435d0,0  
    00109f6a-bac8-11e8-b2b7-ac1f6b6435d0,1 28  
    ...

"
PLAsTiCC Astronomical Classification,Can you help make sense of the Universe?,https://www.kaggle.com/competitions/PLAsTiCC-2018,https://storage.googleapis.com/kaggle-competitions/kaggle/10384/logos/header.png?t=2018-09-12-20-57-06,"Tabular,Astronomy",1089,1320,22851,"<p><img src=""https://cdn2.webdamdb.com/md_0ipHDU0F8Pdq.png?1531519934"" alt=""LSST"" style=""float: right""></p>
<p>Help some of the world's leading astronomers grasp the deepest properties of the universe.</p>

<p>The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the <a href=""https://lsst-tvssc.github.io/"">Large Synoptic Survey Telescope (LSST)</a> -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented!</p>

<p>The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover. </p>

<p>More background information is available <a href=""https://arxiv.org/abs/1810.00001"">here</a>.</p>

<p></p><h3>Acknowledgements</h3><p></p>
<p>PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto.  Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA).  The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC).</p> 
<p>The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future.   </p>
<br><a href=""http://www.nsf.gov""><img src=""https://www.lsst.org/sites/all/themes/edu/images/NSF_tiny_white_logo.png""></a><a href=""http://www.aura-astronomy.org/""><img src=""https://www.lsst.org/sites/all/themes/edu/images/AURA_tiny_white_logo.png""></a><a href=""http://science.energy.gov/""><img src=""https://www.lsst.org/sites/all/themes/edu/images/DOE_tiny_white_logo.png""></a><a href=""https://www6.slac.stanford.edu/""><img src=""https://www.lsst.org/sites/all/themes/edu/images/SLAC_Logo-web.png""></a> 
<a href=""http://www.simonyifund.org/""><img src=""https://www.lsst.org/sites/all/themes/edu/images/Simonyi_tiny_white_logo.png""></a><a href=""https://www.lsstcorporation.org/""><img src=""https://www.lsst.org/sites/all/themes/edu/images/LSSTC-logo-web.png""></a>
<br><a href=""https://tvs.science.lsst.org/""><img src=""https://tvs.science.lsst.org/sites/default/files/lsst_tvs_logo_tiny_0.jpg""></a>  <a href=""http://lsst-desc.org/""><img src="" http://lsst-desc.org/sites/default/files/desc-logo-small.png""></a><p><a href=""https://gallery.lsst.org/bp/#/folder/4495689/63466005"">Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF</a></p>


  [1]: https://arxiv.org/abs/1810.00001","<p>Submissions are evaluated using a weighted multi-class logarithmic loss. The overall effect is such that each class is roughly equally important for the final score.</p>


Each object has been labeled with one type. For each object, you must submit a set of predicted probabilities (one for every category). The formula is then

<p>$$\text{Log Loss} = - \left( \frac{\sum^{M}_{i=1} w_{i} \cdot \sum_{j=1}^{N_{i}} \frac{y_{ij}}{N_{i}} \cdot \ln  p_{ij} }{\sum^{M}_{i=1} w_{i}} \right)$$</p>


<p>where N is the number of objects in the class set, M is the number of classes,  \\(ln\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation (i) belongs to class (j) and 0 otherwise, \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\). </p>

<p>The submitted probabilities for a given object are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>

## Submission File
For each object ID in the test set, you must predict a probability for each of the different possible classes. The file should contain a header and have the following format:


    object_id,class_6,class_15,class_16,class_42,class_52,class_53,class_62,class_64,class_65,class_67,class_88,class_90,class_92,class_95,class_99
    13,0,0.1,0,0.1,0,0.3,0,0,0,0,0,0.5,0,0,0
    14,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
    17,0.75,0.23,0,0,0.01,0,0,0,0,0.01,0,0,0,0,0
    etc.
"
"Quick, Draw! Doodle Recognition Challenge",How accurately can you identify a doodle?,https://www.kaggle.com/competitions/quickdraw-doodle-recognition,https://storage.googleapis.com/kaggle-competitions/kaggle/10200/logos/header.png?t=2018-09-28-21-51-34,Image,1309,1563,21314,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/quickdraw/what-does-a-bee-look-like-1.png"" alt=""Doodle"" style=""float: right""></p>
<p><a href=""https://quickdraw.withgoogle.com/"" target=""_blank"">""Quick, Draw!""</a> was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories.</p>
 
<p>Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution.</p>

<p>Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) &amp; NLP (Natural Language Processing).</p>","Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):

$$MAP@3 = \frac{1}{U} \sum_{u=1}^{U}  \sum_{k=1}^{min(n,3)} P(k)$$

where `U` is the number of scored drawings in the test data, `P(k)` is the precision at cutoff `k`, and `n` is the number predictions per drawing.

You can learn more about this metric works from [this kernel](https://www.kaggle.com/wendykan/map-k-demo), and from this [python code](https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py).

## Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

## Submission File
For each `key_id` in the test set, you should predict up to 3 `word` values. The file should contain a header and have the following format. **IMPORTANT:** Some ""words"" are actually more than one word! The training data aligns to the Quick Draw dataset that that was previously released, and uses spaces to delimit multi-word labels. The Kaggle metric for this competition requires labels with no spaces, so you will need to adjust your label predictions to replace spaces with underscores. For example, ""roller coaster"" should be predicted as ""roller_coaster"". 

    key_id,word
    9000003627287624,The_Eiffel_Tower airplane donut
    9000010688666847,The_Eiffel_Tower airplane donut
    etc.


"
Two Sigma: Using News to Predict Stock Movements,Use news analytics to predict stock price performance,https://www.kaggle.com/competitions/two-sigma-financial-news,https://storage.googleapis.com/kaggle-competitions/kaggle/9933/logos/header.png?t=2018-08-22-18-55-34,"Finance,News,Currencies and Foreign Exchange",2927,813,1138,"<img src=""https://storage.googleapis.com/kaggle-media/competitions/two-sigma/kaggle---twosigma-logo.jpg"" style=""float: right; width: 180px"">

**August 2019 Update: this competition is closed and is no longer accepting submissions.  The data has been removed from this competition and is not available for use. Thanks for participating!**

<p>Can we use the content of news analytics to predict stock price performance? The ubiquity of data today enables investors at any scale to make better investment decisions. The challenge is ingesting and interpreting the data to determine which data is useful, finding the signal in this sea of information. <a href=""http://www.twosigma.com/"">Two Sigma </a> is passionate about this challenge and is excited to share it with the Kaggle community.</p>


<p>As a scientifically driven investment manager, Two Sigma has been applying technology and data science to financial forecasts for over 17 years. Their pioneering advances in big data, AI, and machine learning have pushed the investment industry forward. Now, they're eager to engage with Kagglers in this continuing pursuit of innovation.</p>

<p>By analyzing news data to predict stock prices, Kagglers have a unique opportunity to advance the state of research in understanding the predictive power of the news. This power, if harnessed, could help predict financial outcomes and generate significant economic impact all over the world. </p>

<p>Data for this competition comes from the following sources:</p>
<ul>
<li>Market data provided by Intrinio.</li>
<li>News data provided by Thomson Reuters. Copyright Thomson Reuters, 2017. All Rights Reserved. Use, duplication, or sale of this service, or data contained herein, except as described in the Competition Rules, is strictly prohibited.</li>
</ul>

<a href=""https://intrinio.com/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/two-sigma/Intrinio-Logo-1100-px.png"" style=""width: 200px""></a>
<a href=""https://www.thomsonreuters.com/en/products-services/financial.html/""><img src=""https://storage.googleapis.com/kaggle-media/competitions/two-sigma/thomson_reuters.png"" style=""width: 300px""></a>
<p><em>The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license.</em></p>","In this competition, you must predict a signed confidence value, \\( \hat{y}\_{ti} \in [-1,1] \\) , which is multiplied by the market-adjusted return of a given `assetCode` over a ten day window. If you expect a stock to have a large positive return--compared to the broad market--over the next ten days, you might assign it a large, positive `confidenceValue` (near 1.0). If you expect a stock to have a negative return, you might assign it a large, negative `confidenceValue` (near -1.0). If unsure, you might assign it a value near zero.

For each day in the evaluation time period, we calculate:
$$
x\_t = \sum\_i \hat{y}\_{ti}  r\_{ti}  u\_{ti},
$$
where \\( r\_{ti} \\) is the 10-day market-adjusted leading return for day t for instrument i, and \\( u\_{ti} \\) is a 0/1 `universe` variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.

Your submission score is then calculated as the mean divided by the standard deviation of your daily \\(x\_t\\) values:
$$
\text{score} = \frac{\bar{x}\_t}{\sigma(x_t)}.
$$
If the standard deviation of predictions is 0, the score is defined as 0.

## Submission File
You must make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

The kernels environment automatically formats and creates your submission files in this competition when calling `env.write_submission_file()`. There is no need to manually create your submissions. Submissions will have the following format:

    time,assetCode,confidenceValue  
    2017-01-03,RPXC.O,0.1
    2017-01-04,RPXC.O,0.02
    2017-01-05,RPXC.O,-0.3
    etc.

"
Google Analytics Customer Revenue Prediction,Predict how much GStore customers will spend,https://www.kaggle.com/competitions/ga-customer-revenue-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/10038/logos/header.png?t=2018-06-21-23-13-17,"Regression,Tabular",3611,1369,4171,"<p>The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.</p>

<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/RStudio/google_store.jpg"" alt=""GStore"" style=""float: right""></p>

<p>RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.</p>

<p>In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.</p>
","<h3>Root Mean Squared Error (RMSE)</h3>
<p>Submissions are scored on the root mean squared error. RMSE is defined as:</p>
<p>\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]</p>
<p>where y hat is the natural log of the predicted revenue for a customer and y is the natural log of the actual summed revenue value plus one.</p>

## Submission File

For each `fullVisitorId` in the test set, you must predict the **natural log** of their total revenue in `PredictedLogRevenue`. The submission file should contain a header and have the following format:

    fullVisitorId,PredictedLogRevenue
    0000000259678714014,0
    0000049363351866189,0
    0000053049821714864,0
    etc."
Inclusive Images Challenge,Stress test image classifiers across new geographic distributions,https://www.kaggle.com/competitions/inclusive-images-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/10386/logos/header.png?t=2018-08-27-20-05-33,"Image,Multiclass Classification",468,148,475,"
<br>
<br>
<p>Making products that work for people all over the globe is an important value at <a href=""https://ai.google/"" target=""_blank"">Google AI</a>. In the field of classification, this means developing models that work well for regions all over the world. </p>

<p>Today, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn’t represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse &amp; representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach.</p>

<p>Google AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on.</p>

<p>By finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere.</p>

<p>Note: This competition is run in two stages. Refer to the <a href=""https://www.kaggle.com/c/inclusive-images-challenge#Inclusive-Images-FAQ"" target=""_blank"">FAQ</a> for an explanation of how this works &amp; the <a href=""https://www.kaggle.com/c/inclusive-images-challenge#Timeline"" target=""_blank"">Timeline</a> for specific dates.

</p><p>This competition is a part of the <a href=""https://nips.cc/Conferences/2018/CompetitionTrack"" target=""_blank"">NIPS 2018 competition track</a>. Winners will be invited to attend and present their solutions at the workshop.</p>
<br>

<br>
<br>
<p> Shankar et al. ""No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World"" NIPS 2017 Workshop on Machine Learning for the Developing World </p>","<p>For this competition each image has multiple ground truth labels. We will use Mean F2 score to measure the algorithm quality. The metric is also known as the example based F-score with a beta of 2.</p>
<p>The F2 metric weights recall more heavily than precision, but a good recognition algorithm will still balance precision and recall. Moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.</p>

## Submission File

For every image in the dataset, submission files should contain two columns: image id and predicted labels. Labels should be a space-delimited list. Note that if the algorithm doesn’t predict anything, the column can be left blank. The file must have a header and should look like the following:

    image_id,labels
    2b2b327132556c767a736b3d,/m/0sgh53y /m/0g4cd0
    2b2b394755692f303963553d,/m/0sgh70d /m/0g44ag
    etc"
RSNA Pneumonia Detection Challenge,Can you build an algorithm that automatically detects potential pneumonia cases?,https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/10338/logos/header.png?t=2018-08-21-19-48-11,"Image,Medicine",1499,561,2001,"<p>In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.</p>

<p>Here’s the backstory and why solving this problem matters.</p>

<p>Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.</p>

<p>While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.</p>

<p>CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.</p>

<p>To improve the efficiency and reach of diagnostic services, the <a href=""http://www.rsna.org/"">Radiological Society of North America (RSNA®)</a> has reached out to Kaggle’s machine learning community and collaborated with the <a href=""https://www.nih.gov/"">US National Institutes of Health</a>, <a href=""http://thoracicrad.org/"">The Society of Thoracic Radiology</a>, and <a href=""https://www.md.ai/"">MD.ai</a> to develop a rich dataset for this challenge.</p>

![RSNA Banner][1]

<p>The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.</p>

<p>Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.</p>

<h3>Acknowledgements</h3> 
<p>Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].</p>

* NIH News release: [NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community][2]

* [Original source files and documents][3]

Also, [a big thank you][4] to the competition organizers!

<h3>References</h3>

1.    Rui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables.  Table 27.  Available from: [www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf][5]

2.    Deaths: Final Data for 2015.  Supplemental Tables. Tables I-21, I-22.  Available from: [www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf][6]

3.    Franquet T.  Imaging of community-acquired pneumonia.  J Thorac Imaging 2018 (epub ahead of print).  PMID 30036297

4.    Kelly B.  The Chest Radiograph. Ulster Med J 2012;81(3):143-148

5.    Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf


  [1]: https://storage.googleapis.com/kaggle-media/competitions/rsna/Kaggle_Banner.jpg ""RSNA-Banner""
  [2]: https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community
  [3]: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345
  [4]: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge#Acknowledgements
  [5]: http://www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf
  [6]: http://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.4 to 0.75 with a step size of 0.05: `(0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75)`. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object.  A false negative indicates a ground truth object had no associated predicted object.

**Important note:** if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.

The average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$

In your submission, you are also asked to provide a `confidence` level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.

**NOTE:** In nearly all cases `confidence` will have **no** impact on scoring.  It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases.  None of these edge cases are known to exist in the data set.  If you do not wish to use or calculate `confidence` you can use a placeholder value - like `1.0` - to indicate that no particular order applies to the evaluation of your submission boxes.

Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.

## Intersection over Union (IoU)

Intersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects).  It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.

It can be visualized as the following:

![Image of Intersection over Union][1]

The two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together.  IoU would be low - and would likely not count as a ""hit"" at higher IoU thresholds.

## Submission File

The submission format requires a space delimited set of bounding boxes. For example:

`0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100`

indicates that image `0004cfab-14fd-4e49-80ba-63a80b6bddd6` has a bounding box with a `confidence` of 0.5, at `x` == 0 and `y` == 0, with a `width` and `height` of 100.

The file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.

    patientId,PredictionString
    0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100
    00313ee0-9eaa-42f4-b0ab-c148ed3241cd,
    00322d4d-1c29-4943-afc9-b6754be640eb,0.8 10 10 50 50 0.75 100 100 5 5
    etc...


  [1]: https://storage.googleapis.com/kaggle-media/competitions/rsna/IoU.jpg
"
Airbus Ship Detection Challenge,Find ships on satellite images as quickly as possible,https://www.kaggle.com/competitions/airbus-ship-detection,https://storage.googleapis.com/kaggle-competitions/kaggle/9988/logos/header.png?t=2018-07-16-11-04-58,Image,878,1104,12475,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/Airbus/ships.jpg"" alt=""Shipping"" style=""float: right; width: 300px""></p>
<p>Airbus is excited to challenge Kagglers to build a model that detects all ships in satellite images as quickly as possible. Can you find them even in imagery with clouds or haze? </p>
 
<p>Here’s the backstory: Shipping traffic is growing fast. 
More ships increase the chances of infractions at sea like environmentally devastating ship accidents, piracy, illegal fishing, drug trafficking, and illegal cargo movement. This has compelled many organizations, from environmental protection agencies to insurance companies and national government authorities, to have a closer watch over the open seas.</p>
<p>
</p><p><a href=""http://www.intelligence-airbusds.com/satellite-data/"" target=""_blank"">Airbus</a> offers comprehensive maritime monitoring services by building a meaningful solution for wide coverage, fine details, intensive monitoring, premium reactivity and interpretation response.
Combining its proprietary-data with highly-trained analysts, they help to support the maritime industry to increase knowledge, anticipate threats, trigger alerts, and improve efficiency at sea.</p>
<p>A lot of work has been done over the last 10 years to automatically extract objects from satellite images with significative results but no effective operational effects.  Now Airbus is turning to Kagglers to increase the accuracy <b>and</b>  speed of automatic ship detection.</p> 

<p><b>Algorithm Speed Prize: </b>After the Kaggle challenge is complete,  competitors may submit their model via a private Kaggle kernel for a speed evaluation based upon the inference time on over 40.000  images chips (typical size of a full satellite image) to win a special algorithm speed prize.</p>
<p>&nbsp;</p>

If you're interested to explore more Airbus data, you are welcomed to check out the <a href=""https://www.intelligence-airbusds.com/sandbox"" target=""_blank"">OneAtlas Sandbox</a>. And for more insights on our Maritime Surveillance capabilities, have a look at Airbus Intelligence <a href=""https://www.intelligence-airbusds.com/en/8208-maritime"" target=""_blank"">page</a>.","This competition is evaluated on the F2 Score at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an F2 Score. The threshold values range from 0.5 to 0.95 with a step size of 0.05: `(0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)`. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), the F2 Score value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects. The following equation is equivalent to F2 Score when \\( \beta \\) is set to 2:

$$
F_\beta(t) = \frac{(1 + \beta^2) \cdot TP(t)}{(1 + \beta^2) \cdot TP(t) + \beta^2 \cdot FN(t) + FP(t)}.
$$

A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average F2 Score of a single image is then calculated as the mean of the above F2 Score values at each IoU threshold:

$$
\frac{1}{|thresholds|} \sum_t F_2(t).
$$

Lastly, the score returned by the competition metric is the mean taken over the individual average F2 Scores of each image in the test dataset.

## Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

## Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).

The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed 
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc. A prediction of of ""no ship in image"" should have a blank value in the `EncodedPixels` column.

The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.

The file should contain a header and have the following format. Each row in your submission represents a single predicted ship segmentation for the given image.

      ImageId,EncodedPixels
      00002bd58.jpg,1 3
      00015efb6.jpg,
      00023d5fc.jpg,1 3 10 5
      etc.
"
New York City Taxi Fare Prediction,Can you predict a rider's taxi fare?,https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/10170/logos/header.png?t=2018-07-12-22-07-30,"Regression,Tabular",1483,1566,20088,"In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations.  While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see [the starter code][1] for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques! 

To learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the [Machine Learning with TensorFlow on Google Cloud Platform][2] specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to [Coursera.org/NEXTextended][3] to claim this specialization for free for the first month!


  [1]: https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model
  [2]: https://www.coursera.org/specializations/machine-learning-tensorflow-gcp?utm_source=googlecloud&utm_medium=institutions&utm_campaign=kaggle_competition_email
  [3]: https://www.coursera.org/promo/NEXTExtended?utm_source=googlecloud&utm_medium=institutions&utm_campaign=kaggle_competition_2018","<p>The evaluation metric for this competition is the <a href=""http://en.wikipedia.org/wiki/Root-mean-square_deviation"">root mean-squared error</a> or RMSE. RMSE measures the difference between the predictions of a model, and the corresponding ground truth. A large RMSE is equivalent to a large average error, so smaller values of RMSE are better.  One nice property of RMSE is that the error is given in the units being measured, so you can tell very directly how incorrect the model might be on unseen data.</p>
<p>RMSE is given by:</p>
<p>\[ \text{RMSE} = \sqrt{\frac{1}{n} \sum^{n}_{i=1} (\hat{y}_i - y_i)^2} \]</p>
<p>where \\(  y_i \\) is the <em>i<sup>th</sup></em> observation and $$\hat{y}_i $$ is the prediction for that observation. </p>
<p>Example 1. Suppose we have one observation, with an actual value of 12.5 and a prediction of 12.5 (good job!).  The RMSE will be:</p>
<p>\[ \text{RMSE}_\text{example1} = \sqrt{\frac{1}{1} (12.5 - 12.5)^2} = 0\]</p>
<p>Example 2. We'll add another data point. Your prediction for the second data point is 11.0 and the actual value is 14.0. The RMSE will be:</p>
<p>\[ \text{RMSE}_\text{example2} = \sqrt{\frac{1}{2}((12.5 - 12.5)^{2} + (11.0-14.0)^{2})} = \sqrt{\frac{9}{2}} \approx 2.12 \]</p>

## Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

## Submission File
For each `key` in the test set, you must predict a value for the `fare_amount` variable. The file should contain a header and have the following format:
<pre>key,fare_amount<br />2015-01-27 13:08:24.0000002,11.00<br /><span style=""font-size: 14px; line-height: 1.4em;"">2015-02-27 13:08:24.0000002,12.05<br />2015-03-27 13:08:24.0000002,11.23<br />2015-04-27 13:08:24.0000002,14.17<br />2015-05-27 13:08:24.0000002,15.12<br />etc</span><span style=""font-size: 14px; line-height: 1.4em;""><br /></span></pre>"
TGS Salt Identification Challenge,Segment salt deposits beneath the Earth's surface,https://www.kaggle.com/competitions/tgs-salt-identification-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/10151/logos/header.png?t=2018-07-18-15-01-00,"Geology,Image",3219,3726,76185,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/TGS/drilling.jpg"" alt=""TGS"" style=""float: right""></p>
<p>Several areas of Earth with large accumulations of oil and gas <b><i>also</i></b> have huge deposits of salt below the surface.</p>

<p>But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.</p>

<p>To create the most accurate seismic images and 3D renderings, <a href=""http://www.tgs.com/"">TGS (the world’s leading geoscience data company)</a> is hoping Kaggle’s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.</p>","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: `(0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)`. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.

## Submission File

In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).

The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed 
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.

The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.

The file should contain a header and have the following format. Each row in your submission represents a single predicted salt segmentation for the given image.

    id,rle_mask
    3e06571ef3,1 1
    a51b08d882,1 1
    c32590b06f,1 1
    etc.

"
Costa Rican Household Poverty Level Prediction,Can you identify which households have the highest need for social welfare assistance?,https://www.kaggle.com/competitions/costa-rican-household-poverty-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/9840/logos/header.png?t=2018-07-11-23-24-08,"Tabular,Multiclass Classification",616,673,6495,"<p>The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?

</p><p>Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.

</p><p>In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.</p>

<p>While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.</p>

<p>To improve on PMT, the <a href=""https://www.iadb.org/en"">IDB (the largest source of development financing for Latin America and the Caribbean)</a> has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.</p>

<p>Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world.</p>

<p>This is a <b>Kernels-Only Competition</b>, so you must submit your code through Kernels, rather than uploading .csv predictions. You can create private Kernels and even share/edit your work with teammates by adding them as collaborators.</p>","Submissions will be evaluated based on their [macro F1 score](https://en.wikipedia.org/wiki/F1_score).

## Kernel Submissions
As this is a Kernels-Only Competition, you **must** make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

## Submission File
For each Id in the test set, you must predict a class for the `Target` variable as described in [the data page](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data).

The file should contain a header and have the following format:
<pre>Id,Target<br />ID_2f6873615,1<br /><span style=""font-size: 14px; line-height: 1.4em;"">ID_1c78846d2,2<br />ID_e5442cf6a,3<br />ID_a8db26a79,4<br />ID_a62966799,4 <br>etc</span><span style=""font-size: 14px; line-height: 1.4em;""><br /></span></pre>
"
Store Item Demand Forecasting Challenge,Predict 3 months of item sales at different stores ,https://www.kaggle.com/competitions/demand-forecasting-kernels-only,https://storage.googleapis.com/kaggle-competitions/kaggle/9999/logos/header.png?t=2018-06-28-21-19-41,Tabular,459,484,6135,"This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset. 

You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.

What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?

This is a great competition to explore different models and improve your skills in forecasting.","Submissions are evaluated on <a href=""https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error"">SMAPE</a> between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.

## Kernel Submissions
You can only make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.

## Submission File
For each `id` in the test set, you must predict a probability for the `sales` variable. The file should contain a header and have the following format:

    id,sales
    0,35
    1,22
    2,5
    etc."
What's Cooking? (Kernels Only),Use recipe ingredients to categorize the cuisine,https://www.kaggle.com/competitions/whats-cooking-kernels-only,https://storage.googleapis.com/kaggle-competitions/kaggle/10012/logos/header.png?t=2018-06-20-19-44-38,"Multiclass Classification,Text,Food",520,552,4949,"<p><em>Picture yourself strolling through your local, open-air market... </em><em>What do you see? What do you smell? What will you make for dinner tonight?</em></p>
<p>If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.</p>
<p>Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients. </p>
<h3>Acknowledgements</h3>
<p>We want to thank <a href=""http://www.yummly.com/"" target=""_blank"">Yummly</a> for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4526/media/Yummly_logo.png"" alt="""" width=""165"" height=""61"" style=""display: block; margin-left: auto; margin-right: auto;"" /></p>","<p>Submissions are evaluated on the categorization accuracy (the percent of dishes that you correctly classify).</p>
<h2>Submission File</h2>
<p>Your submission file should predict the cuisine for each recipe in the test set. The file should contain a header and have the following format:</p>
<pre>id,cuisine<br />35203,italian<br />17600,italian<br />35200,italian<br />17602,italian<br />...<br />etc.</pre>"
Movie Review Sentiment Analysis (Kernels Only),Classify the sentiment of sentences from the Rotten Tomatoes dataset,https://www.kaggle.com/competitions/movie-review-sentiment-analysis-kernels-only,https://storage.googleapis.com/kaggle-competitions/kaggle/10025/logos/header.png?t=2018-06-21-18-32-17,"Text,Multiclass Classification",409,430,3806,"<p>""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.""</p>
<p>The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases&nbsp;on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png"" alt=""Treebank""></p>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of <a href=""http://www.socher.org/"">Socher</a> et al [2].&nbsp;We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper:</p>
<p><a href=""http://nlp.stanford.edu/sentiment/"">http://nlp.stanford.edu/sentiment/</a></p>
<p>There you will find have source code, a live demo, and even an online interface to help train the model.</p>
<p>[1]&nbsp;Pang and L. Lee. 2005. <em>Seeing stars: Exploiting class&nbsp;relationships for sentiment categorization with respect&nbsp;to rating scales</em>. In ACL, pages 115–124.</p>
<p>[2]&nbsp;<em>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</em>, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).</p>
<p></p>","<p>Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:</p>
<p>0 - negative<br>1 - somewhat negative<br>2 - neutral<br>3 - somewhat positive<br>4 - positive</p>
<h2>Submission Format</h2>
<p>For each phrase&nbsp;in the test set, predict a label for the sentiment.&nbsp;Your submission should have a header and look like the following:</p>
<pre>PhraseId,Sentiment<br>156061,2<br>156062,2<br>156063,2<br>...</pre>"
Forest Cover Type (Kernels Only),Use cartographic variables to classify forest categories,https://www.kaggle.com/competitions/forest-cover-type-kernels-only,https://storage.googleapis.com/kaggle-competitions/kaggle/9985/logos/header.png?t=2018-06-19-16-26-02,"Tabular,Forestry",358,377,3499,"<p><a href=""http://en.wikipedia.org/wiki/Random_forest"">Random forests?</a>&nbsp;<a href=""http://en.wikipedia.org/wiki/Cover_tree"">Cover trees?</a>&nbsp;Not so fast, computer nerds. We're talking about the real thing.</p>
<p>In this competition you are asked to predict the forest cover type (the predominant&nbsp;kind&nbsp;of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.</p>
<p>This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.</p>
<p>This competition originally ran in 2015. We are relaunching it as a kernels-only version here.</p>
<h2>Acknowledgements</h2>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was&nbsp;provided by Jock A. Blackard and Colorado State University.&nbsp;We also thank the UCI machine learning repository for <a href=""https://archive.ics.uci.edu/ml/datasets/Covertype"">hosting the dataset</a>. If you use the problem in publication, please cite:</p>
<p><em>Bache, K. &amp; Lichman, M. (2013). <a href=""http://archive.ics.uci.edu/ml"">UCI Machine Learning Repository</a>. Irvine, CA: University of California, School of Information and Computer Science</em></p>","<p>Submissions are evaluated on multi-class&nbsp;classification accuracy.</p>
<h2>Submission File</h2>
<p>Your submission file should have&nbsp;the observation Id and a&nbsp;predicted cover type (an integer between 1 and 7, inclusive).&nbsp;The file should contain a header and have the following format:</p>
<pre>Id,Cover_Type<br>15121,1<br>15122,1<br>15123,1<br>...</pre>"
Santander Value Prediction Challenge,Predict the value of transactions for potential customers.,https://www.kaggle.com/competitions/santander-value-prediction-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/9717/logos/header.png?t=2018-05-15-23-00-08,"Banking,Finance",4463,4865,54750,"<p>According to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception. </p>

<p>The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service. 
In their 3rd Kaggle competition, <a href=""https://www.santanderbank.com/us/personal"">Santander Group</a> aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction.  This means anticipating customer needs in a more concrete, but also simple and personal way.  With so many choices for financial services, this need is greater now than ever before. </p>

<p>In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.</p>
","<p>The evaluation metric for this competition is Root Mean Squared Logarithmic Error.</p>
<p>The RMSLE is calculated as</p>
<p>$$<br />\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }<br />$$</p>
<p>Where:</p>
<p>\\(\epsilon\\) is the RMSLE value (score)<br />\\(n\\) is the total number of observations in the (public/private) data set,<br />\\(p_i\\) is your prediction of target, and<br />\\(a_i\\) is the actual target for \\(i\\). <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>

<h2>Submission File</h2>
<p><strong>For every row in the test.csv</strong>, submission files should contain two columns: ID and target.  The ID corresponds to the column of that ID in the test.tsv. The file should contain a header and have the following format:</p>
<pre>ID,target<br />000137c73,5944923.322036332<br />00021489f,5944923.322036332<br />0004d7953,5944923.322036332<br />etc.</pre>"
The 2nd YouTube-8M Video Understanding Challenge,Can you create a constrained-size model to predict video labels?,https://www.kaggle.com/competitions/youtube8m-2018,https://storage.googleapis.com/kaggle-competitions/kaggle/9301/logos/header.png?t=2018-05-08-18-21-41,Video Data,312,380,2554,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/youtube/YT8M.png"" style=""float: right""></p><p>The world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day.</p>

<p>To spur advances in analyzing and understanding video,  Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted <a href=""https://www.kaggle.com/c/youtube8m"">Google Cloud &amp; YouTube-8M Video Understanding Challenge</a>, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on <b>learning video representation under budget constraints</b>.
</p>

<p>For a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user’s personal mobile phones.</p>

<p>In this competition, you’re challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.</p>

<p> This competition is being hosted by Google AI (previously known as Google Research) as a part of the <a href=""https://eccv2018.org/program/workshops_tutorials/"">European Conference on Computer Vision (ECCV) 2018 selected workshop session</a>. Please refer to the <a href=""https://research.google.com/youtube8m/workshop2018/index.html"">YouTube 8M Large-Scale Video Understanding Workshop Page</a> for details about the workshop.</p>","<p>Submissions are evaluated the Global Average Precision (GAP) at k, where k=20. For each video, you will submit a list of predicted labels and their corresponding confidence scores. The evaluation takes the predicted labels that have the highest k confidence scores for each video, then treats each prediction and the confidence score as an individual data point in a long list of global predictions, to compute the Average Precision across all of the predictions and all the videos. </p>
<p>If a submission has N predictions (label/confidence pairs) sorted by its confidence score, then the Global Average Precision is computed as:</p>
<p>$$GAP = \sum_{i=1}^N p(i) \Delta r(i)$$</p>
<p>where N is the number of final predictions (if there are 20 predictions for each video, then N = 20 * #Videos ), p(i) is the precision, and r(i) is the recall.  </p>
<p>A python implementation of GAP can be found at youtube-8m's <a href=""https://github.com/google/youtube-8m/blob/master/average_precision_calculator.py#L179"">github</a>. </p>
<h2>Submission File</h2>
<p>For each VideoId in the test set, you must predict a list of labels and their corresponding confidence scores. The file should contain a header and have the following format:</p>
<pre>VideoId,LabelConfidencePairs<br />000c,1 0.5 2 0.3 3 0.1 4 0.05 5 0.05<br />etc.</pre>"
Home Credit Default Risk,Can you predict how capable each applicant is of repaying a loan?,https://www.kaggle.com/competitions/home-credit-default-risk,https://storage.googleapis.com/kaggle-competitions/kaggle/9120/logos/header.png?t=2018-04-02-23-51-59,"Tabular,Banking",7176,8373,131888,"<p>Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.</p>

<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/home-credit/about-us-home-credit.jpg"" alt=""Home Credit Group"" style=""float: right""></p>

<p><a href=""http://www.homecredit.net/"">Home Credit</a> strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.</p>

<p>While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.</p>","Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.

## Submission File

For each `SK_ID_CURR` in the test set, you must predict a probability for the `TARGET` variable. The file should contain a header and have the following format:

    SK_ID_CURR,TARGET
    100001,0.1
    100005,0.9
    100013,0.2
    etc."
TrackML Particle Tracking Challenge,High Energy Physics particle tracking in CERN detectors,https://www.kaggle.com/competitions/trackml-particle-identification,https://storage.googleapis.com/kaggle-competitions/kaggle/7707/logos/header.png?t=2017-11-07-20-38-12,"Physics,Tabular",651,739,5776,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/CERN/cern_graphic.png"" alt=""CERN Detector Imagery"" style=""float: right""></p>
<p>To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors.</p>
 
<p>While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge.</p>

<p>Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data.</p>

<p>To help address this problem, a team of Machine Learning experts and physics scientists working at <a href=""http://home.web.cern.ch/"">CERN</a> (the world largest high energy physics laboratory),  has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles?</p>
 
 
<p>Specifically, in this competition, you’re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases:</p>
<ul>
<li>
The Accuracy phase has run on Kaggle from May to 13th August 2018 (Winners to be announced by end September). Here we’ll be focusing on the highest score, irrespective of the evaluation time. This phase is an official <a href=""http://www.ecomp.poli.br/~wcci2018/"">IEEE WCCI</a> competition (Rio de Janeiro, Jul 2018).
</li>
<li>
 The Throughput phase will run on Codalab starting in September 2018. Participants will submit their software which is evaluated by the platform. Incentive is on the throughput (or speed) of the evaluation while reaching a good score. This phase is an official <a href=""https://nips.cc/"">NIPS</a> competition (Montreal, Dec 2018).
</li>
</ul>

All the necessary information for the Accuracy phase is available here on Kaggle site. The overall TrackML challenge web site is <a href=""https://sites.google.com/site/trackmlparticle/"">there</a>.","## Custom metric

The evaluation metric for this competition is a custom metric. In one line : *it is the intersection between the reconstructed tracks and the ground truth particles, normalized to one for each event, and averaged on the events of the test set*. 

First, each hit is assigned a weight: 

 - the few first (starting from the center of the detector) and last hits have a larger weight
 - hits from the more straight tracks (more rare, but more interesting) have a larger weight
 - random hits or hits from very short tracks have weight zero 
 - the sum of the weights of all the hits of one event is 1 by construction
 - the hit weights are available in the truth file. They are not revealed for the test dataset

Then, the score is constructed as follows: 

 - tracks are uniquely matched to particles by the double majority rule:
     - for a given track, the matching particle is the one to which the absolute majority (strictly more that 50%) of the track points belong.
     - the track should have the absolute majority of the points of the matching particle.
If any of these constraints is not met, the score for this track is zero
 - the score of a surviving track is the sum of the weights of the points of the intersection between the track and the matching particle.
 - the score of an event is the sum of the score of all its tracks.
 - the final score is the average on the events of the public and private leaderboard test respectively.

A perfect algorithm will have a score of 1, while a random one will have a score 0.
An example implementation can be found in the [trackml python library](https://github.com/LAL/trackml-library).

## Submission Format

The submission file should contain three columns: event_id, hit_id, track_id, and should have exactly one line for every hit of every event.

 - event_id is the event number
 - hit_id is the hit number, within that event
 - track_id is the user defined numerical identifier (non negative integer) of the track (the track being the group or cluster of hits). 

The file should contain a header and have the following format:

    event_id,hit_id,track_id
    0,0,21
    0,1,49
    0,3,32
    0,4,0
    0,5,21
    etc...
"
Avito Demand Prediction Challenge,Predict demand for an online classified ad,https://www.kaggle.com/competitions/avito-demand-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/8586/logos/header.png?t=2018-02-21-02-09-25,"Text,Tabular,Image",1868,2313,43509,"<p>When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. Details like:</p><br>

<img src=""https://storage.googleapis.com/kaggle-media/competitions/Avito/product_description_qualities_2.png"" alt=""Avito Qualities"">

<p>And, even with an optimized product listing, demand for a product may simply not exist–frustrating sellers who may have over-invested in marketing.</p>

<p><a href=""https://www.avito.ru/"">Avito</a>, Russia’s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced).</p>

<p>In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.</p>
","<h3>Root Mean Squared Error (RMSE)</h3>
<p>Submissions are scored on the root mean squared error. RMSE is defined as:</p>
<p>\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]</p>
<p>where y hat is the predicted value and y is the original value.</p>

## Submission File

For each `item_id` in the test set, you must predict a probability for the `deal_probability`. Your predictions must be in the range [0, 1]. The submission file should contain a header and have the following format:

    item_id,deal_probability
    2,0
    5,0
    6,0
    etc."
DonorsChoose.org Application Screening,Predict whether teachers' project proposals are accepted,https://www.kaggle.com/competitions/donorschoose-application-screening,https://storage.googleapis.com/kaggle-competitions/kaggle/8426/logos/header.png?t=2018-02-03-04-51-52,"Crowdfunding,Binary Classification",580,617,6401,"Founded in 2000 by a high school teacher in the Bronx, [DonorsChoose.org][1] empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount.

DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website. 

Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:

1. How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible

2. How to increase the consistency of project vetting across different volunteers to improve the experience for teachers

3. How to focus volunteer time on the applications that need the most assistance

The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.

With an algorithm to pre-screen applications, DonorsChoose.org can auto-approve some applications quickly so that volunteers can spend their time on more nuanced and detailed project ​vetting processes, including doing more to help teachers develop projects that qualify for specific funding opportunities. 

Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country. 


### Getting Started with Kernels

Get familiar with the competition data and the machine learning objective quickly using Kernels. Google's engineering education team has put together [a starter tutorial implementing benchmark linear classification model][2]. 

### Acknowledgments

[Machine Learning Crash Course][3] was created by Google's engineering education team in partnership with numerous Machine Learning subject matter experts across Google.


  [1]: https://www.donorschoose.org/
  [2]: https://www.kaggle.com/skleinfeld/getting-started-with-the-donorschoose-data-set
  [3]: https://developers.google.com/machine-learning/crash-course/","The goal of this competition is to predict whether an application to DonorsChoose is accepted. Submissions are evaluated on [area under the ROC curve][1] between the predicted probability and the observed target.


## Submission File

For each `id` in the test set, you must predict a probability for the `project_is_approved` variable. The file should contain a header and have the following format (order does not matter):

    id,project_is_approved
    p233245,0.84
    p096795,0.84
    p236235,0.84
    etc.


  [1]: https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
Google Cloud & NCAA® ML Competition 2018-Men's,Apply Machine Learning to NCAA® March Madness®,https://www.kaggle.com/competitions/mens-machine-learning-competition-2018,https://storage.googleapis.com/kaggle-competitions/kaggle/8310/logos/header.png?t=2018-02-23-21-05-08,Basketball,933,1061,1655,"<p>Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
</p>
<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/march-madness-2018/lockup_cloud.png"" alt=""March Madness Imagery"" style=""float: right""></p>
<p>In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a <a href=""https://www.kaggle.com/datasets/about"">dataset</a>. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results.</p>
<p><i>This page is for the NCAA Division I Men's tournament. Check out the <a href=""https://www.kaggle.com/c/womens-machine-learning-competition-2018/"">NCAA Division I Women's tournament here</a>. </i></p>","<p>Submissions are scored on the log loss:</p>
<p>$$<br />\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br />$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p><span style=""font-size: 1em; line-height: 1.5em;"">A smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored). </span>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2018 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. </p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.</p>
<p>The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br />2013_1103_1107,0.5<br />2013_1103_1112,0.5<br />2013_1103_1125,0.5<br />...</pre>"
Predict Future Sales,"Final project for ""How to win a data science competition"" Coursera course",https://www.kaggle.com/competitions/competitive-data-science-predict-future-sales,https://storage.googleapis.com/kaggle-competitions/kaggle/8587/logos/header.png?t=2018-02-17-16-28-16,Tabular,15953,17037,135239,"This challenge serves as final project for the  <a href=""https://www.coursera.org/learn/competitive-data-science/home/welcome"">""How to win a data science competition""</a> Coursera course.

<p>In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - <strong><a href=""http://1c.ru/eng/title.htm"">1C Company</a></strong>. </p>
<p>We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.</p>","<p>Submissions are evaluated by&nbsp;<a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"">root mean squared error (RMSE)</a>. True target values are clipped into [0,20] range.</p>
<p><strong>Submission File</strong></p>
<p>For each id in the test set, you must predict a total number of sales. The file should contain a header and have the following format:</p>
<pre>ID,item_cnt_month<br>0,0.5<br>1,0.5<br>2,0.5<br>3,0.5<br>etc.</pre>"
Google Landmark Retrieval Challenge,"Given an image, can you find all of the same landmarks in a dataset?",https://www.kaggle.com/competitions/landmark-retrieval-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/8396/logos/header.png?t=2018-02-01-17-57-10,Image,209,283,3116,"**[UPDATE] 2019 challenge launched:** https://kaggle.com/c/landmark-retrieval-2019

Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.


In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any).


The new dataset is the largest worldwide dataset for image retrieval research, comprising more than a million images of 15K unique landmarks. We hope that this release will accelerate progress in this important research problem.


This challenge is organized in conjunction with the Landmark Recognition Challenge (https://www.kaggle.com/c/landmark-recognition-challenge). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.
","Submissions are evaluated according to mean Average Precision @ 100 (\\(mAP@100\\)):

$$mAP@100 = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{min(m_q, 100)} \sum_{k=1}^{min(n_q,100)} P_q(k) rel_q(k)$$

where:

- \\(Q\\) is the number of query images that depict landmarks from the index set
- \\(m_q\\) is the number of index images containing a landmark in common with the query image \\(q\\) (note that this is only for queries which depict landmarks from the index set, so \\(m_q \neq 0\\))
- \\(n_q\\) is the number of predictions made by the system for query \\(q\\)
- \\(P_q(k)\\) is the precision at rank \\(k\\) for the \\(q\\)-th query
- \\(rel_q(k)\\) denotes the relevance of prediciton \\(k\\) for the \\(q\\)-th query: it’s 1 if the \\(k\\)-th prediction is correct, and 0 otherwise

Some query images will have no associated index images to retrieve. These queries are ignored in scoring.

## Submission File

For each query id in the test set, you must predict a space-delimited list of index images that depict the same landmarks as the query. The list should be sorted, such that the first index image is considered the most relevant one, and the last the least relevant one. The file should contain a header and have the following format:

    id,images
    000088da12d664db,0370c4c856f096e8 766677ab964f4311 e3ae4dcee8133159...
    etc.
"
Google Landmark Recognition Challenge,Label famous (and not-so-famous) landmarks in images,https://www.kaggle.com/competitions/landmark-recognition-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/7456/logos/header.png?t=2018-01-30-21-42-08,Image,476,625,8074,"**[UPDATE] 2019 challenge launched:** https://kaggle.com/c/landmark-recognition-2019

Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections.


Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.


Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are a total of 15K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.


This challenge is organized in conjunction with the Landmark Retrieval Challenge ( https://www.kaggle.com/c/landmark-retrieval-challenge ). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.
","<p>Submissions are evaluated using Global Average Precision (GAP) at \\(k\\), where \\(k=1\\). This metric is also known as micro Average Precision (microAP), as per [1]. It works as follows:</p>
<p>For each query image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions (sorted in descending order by confidence scores), and computes the Average Precision based on this list. </p>
<p>If a submission has \\(N\\) predictions (label/confidence pairs) sorted in descending order by their confidence scores, then the Global Average Precision is computed as:</p>
<p>$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$</p>
<p>where:
</p><ul>
<li> \\(N\\) is the total number of predictions returned by the system, across all queries</li>
<li> \\(M\\) is the total number of queries with at least one landmark from the training set visible in it (note that some queries may not depict landmarks) </li>
<li> \\(P(i)\\) is the precision at rank \\(i\\)</li>
<li> \\(rel(i)\\) denotes the relevance of prediciton \\(i\\): it’s 1 if the \\(i\\)-th prediction is correct, and 0 otherwise</li>
</ul>
<p>[1] F. Perronnin, Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,"" Proc. CVPR'09</p>
<h2>Submission File</h2>
<p>For each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some query images may contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. The submission file should contain a header and have the following format (larger scores denote more confident matches):</p>
<pre>id,landmarks
000088da12d664db,8815 0.03
0001623c6d808702,
0001bbb682d45002,5328 0.5
etc.</pre>
"
2018 Data Science Bowl ,Find the nuclei in divergent images to advance medical discovery,https://www.kaggle.com/competitions/data-science-bowl-2018,https://storage.googleapis.com/kaggle-competitions/kaggle/8089/logos/header.png?t=2018-01-10-17-54-22,Biology,3634,1095,1881,"<h3>Spot Nuclei. Speed Cures.</h3>
<p>Imagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection.</p>
<p>We&rsquo;ve all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer&rsquo;s, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster.</p>
<p>By automating nucleus detection, you could help unlock cures faster&mdash;from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? <a href=""https://www.youtube.com/watch?v=eHwkfhmJexs&feature=youtu.be"">View this video.</a></p>
<h3>Why nuclei?</h3>
<p>Identifying the cells&rsquo; nuclei is the starting point for most analyses because most of the human body&rsquo;s 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work.</p>
<p>By participating, teams will work to automate the process of identifying nuclei, which will  allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. <a href=""https://datasciencebowl.com/2018dsbtutorial/"">Check out this video overview to find out more.</a></p>

<h3>What will participants do?</h3>
<p>Teams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther.</p>
<p>Visit <a href=""http://www.datasciencebowl.com"">DataScienceBowl.com</a> to: <br /> &bull; Sign up to&nbsp;<a href=""http://www.datasciencebowl.com/contact/"">receive news</a> about the competition<br /> &bull; Learn about the&nbsp;<a href=""http://www.datasciencebowl.com/competitions/"">history of the Data Science Bowl</a> and past competitions<br /> &bull; Read our&nbsp;<a href=""http://www.datasciencebowl.com/data-science-insights/"">latest insights</a> on emerging analytics techniques</p>
<img src=""https://storage.googleapis.com/kaggle-media/competitions/dsb-2018/dsb.jpg""></img>","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: `(0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)`. In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.

At each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.

## Submission File

In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).

The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed 
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.

The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.

The file should contain a header and have the following format. Each row in your submission represents a single predicted nucleus segmentation for the given `ImageId`.

    ImageId,EncodedPixels  
    0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5,1 1  
    0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,1 1  
    0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,2 3 8 9  
    etc...

Submission files may take several minutes to process due to the size."
Humpback Whale Identification Challenge,Can you identify a whale by the picture of its fluke?,https://www.kaggle.com/competitions/whale-categorization-playground,https://storage.googleapis.com/kaggle-competitions/kaggle/6961/logos/header.png,"Image,Animals",527,599,5005,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/happy-whale.jpg"" alt=""Planet Aerial Imagery"" style=""float: right""></p>
<p>After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.</p>
<p>To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.</p>
<p>In this competition, you’re challenged to build an algorithm to identifying whale species in images. You’ll analyze Happy Whale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe.</p>
<p>We'd like to thank <a href=""https://happywhale.com/"">Happy Whale </a> for providing this data and problem. Happy Whale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.</p>","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):

$$MAP@5 = \frac{1}{U} \sum_{u=1}^{U}  \sum_{k=1}^{min(n,5)} P(k)$$

where `U` is the number of images, `P(k)` is the precision at cutoff `k`, and `n` is the number predictions per image.


## Submission File

For each `Image` in the test set, you may predict up to 5 labels for the whale `Id`. Whales that are not predicted to be one of the labels in the training data should be labeled as `new_whale`. The file should contain a header and have the following format:

    Image,Id 
    00029b3a.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46
    0003c693.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46
    ...
"
ImageNet Object Localization Challenge,Identify the objects in images,https://www.kaggle.com/competitions/imagenet-object-localization-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/6799/logos/header.png,Image,75,80,122,"While It's pretty easy for people to identify subtle differences in photos, computers still have a ways to go. Visually similar items are tough for computers to count, like this overlapping bunch of bananas:

<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/bananas.png"" alt=""drawing"" width=""448""/>

<p>Or, consider this photo of a family of foxes camouflaged in the wild - where do the foxes end and where does the grass begin? </p>

<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/kit_fox.JPG"" alt=""drawing"" width=""448""/>

To solve this problem and enhance the state of the art in object detection and classification, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) began in 2010. Kaggle is excited and honored to be the new home of the official ImageNet Object Localization competition. Participants are challenged with identifying all objects within an image so those images can then be classified and annotated. 
 
Already, because of this competition, there’s been a 4.2× reduction in image classification error (from 28.2% to 6.7%) and a 1.7× reduction in localization error (from 42.5% to 25.3%) between 2010 and 2014 alone. Can you improve the accuracy even further?

### Competition Overview

The validation and test data will consist of 150,000 photographs, collected from Flickr and other search engines, hand labeled with the presence or absence of 1000 object categories. The 1000 object categories contain both internal nodes and leaf nodes of ImageNet, but do not overlap with each other.

A random subset of 50,000 of the images with labels will be released as the training set along with a list of the 1000 categories. The remaining images will be used as the test set.

The validation and test data for this competition are not contained in the ImageNet training data.","## Evaluation 
In this competition, the error for each image is defined as 

$$e=min\_i(min\_j(max(d\_{ij},f\_{ij})))$$

where

\\(d=0\\) if the labels of the two boxes are the same, and \\(d=1\\) otherwise;

\\(f=0\\) if the overlap of the two boxes >= 50%, and \\(f=1\\) otherwise;

\\(i\\) is the predicted labels/bounding boxes, and \\(j\\) is the ground truth labels/bounding boxes. 

For example, let's assume for a given image, there are 2 boxes as ground truth (`g_0`, `g_1`), and you predict 3 boxes in your prediction (`p_0`, `p_1`, `p_2`). We iterate through your prediction boxes, and see if they can find a ""match"" with any of the ground truth boxes. If there's a match, then the min error for this image is 0, otherwise, the min error is 1. 

A match is defined as 

1. the prediction box has a class label that is the same as the ground truth box, and 

2. the prediction bounding box has over 50% match in the area with the ground truth bounding box. 

The psudo-code:
<pre>
min_error_list = []
for prediction_box in [p_0, p_1, p_2]:
    max_error_list = []
    for ground_truth_box in [g_0, g_1]:
        if label(prediction_box) == label(ground_truth_box):
            d = 0
        else:
            d = 1
        if overlap(prediction_box, ground_truth_box) > 0.5:
            f = 0
        else:
            f = 1
        max_error_list.append(max(d,f))   # the first max
    min_error_list.append(min(max_list))  # the first min

return min(min_error_list)  # the second min
</pre>

Note the min error is either 0 or 1 for each image. 

The total error is then computed as the average of all min errors of all the images in the test dataset. 

## Submission File

For each image in the test dataset, you will predict a list of label and bounding boxes. 

It contains two columns:
  - `ImageId`: the id of the test image, for example `ILSVRC2012_test_00000001`
  - `PredictionString`: the prediction string should be a space delimited of 5 integers. For example, `1000 240 170 260 240` means it's label 1000, with a bounding box of coordinates (x_min, y_min, x_max, y_max). We accept up to 5 predictions. For example, if you submit `862 42 24 170 186 862 292 28 430 198 862 168 24 292 190 862 299 238 443 374 862 160 195 294 357 862 3 214 135 356` which contains 6 bounding boxes, we will only take the first 5 into consideration. 

<pre>ImageId,PredictionString
ILSVRC2012_test_00000001,1000 240 170 260 240
ILSVRC2012_test_00000002,825 240 170 260 240 829 152 331 246 415
ILSVRC2012_test_00000003,862 42 24 170 186 862 292 28 430 198 862 168 24 292 190 862 299 238 443 374 862 160 195 294 357
</pre>
"
IEEE's Signal Processing Society - Camera Model Identification,Identify from which camera an image was taken,https://www.kaggle.com/competitions/sp-society-camera-model-identification,https://storage.googleapis.com/kaggle-competitions/kaggle/8078/logos/header.png?t=2017-12-22-17-32-04,Image,580,760,10798,"<p>Finding footage of a crime caught on tape is an investigator's dream. But even with crystal clear, damning evidence, one critical question always remains–is the footage real?</p>

<p>Today, one way to help authenticate footage is to identify the camera that the image was taken with. Forgeries often require splicing together content from two different cameras. But, unfortunately, the most common way to do this now is using image metadata, which can be easily falsified itself.</p>

<p>This problem is actively studied by several researchers around the world. Many machine learning solutions have been proposed in the past: least-squares estimates of a camera's color demosaicing filters as classification features, co-occurrences of pixel value prediction errors as features that are passed to sophisticated ensemble classifiers, and using CNNs to learn camera model identification features. However, this is a problem yet to be sufficiently solved.</p>

<p>For this competition, the IEEE Signal Processing Society is challenging you to build an algorithm that  identifies which camera model captured an image by using traces intrinsically left in the image. Helping to solve this problem would have a big impact on the verification of evidence used in criminal and civil trials and even news reporting.</p>","This competition is evaluated on the weighted categorization accuracy of your predictions (the percentage of camera models correctly predicted).

 $$\text{weighted_accuracy}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} \frac{w_i  (y_i = \hat{y}_i)}{\sum{w_i}} $$

where `n` is the number of samples in the test set, `y` is the true camera label, `y_hat` is the predicted camera label, and `w_i` is 0.7 for unaltered images, and 0.3 for altered images.

## Submission File

For each image `fname` in the test set, you must predict a the correct `camera` model. The submission file should contain a header and have the following format:

    fname,camera
    img_0002a04_manip.tif,iPhone-6
    img_001e31c_unalt.tif,iPhone-6
    img_00275cf_manip.tif,iPhone-6
    img_0034113_unalt.tif,iPhone-6"
Toxic Comment Classification Challenge,Identify and classify toxic online comments,https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/8076/logos/header.png?t=2017-12-15-21-30-35,Text,4539,5372,92230,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/jigsaw/003-avatar.png"" alt=""Toxic Comments"" style=""float: right""></p>
<p>Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.</p>

<p>The <a href=""https://conversationai.github.io/"" target=""_new"">Conversation AI</a> team, a research initiative founded by <a href=""https://jigsaw.google.com/"">Jigsaw</a> and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the <a href=""https://perspectiveapi.com/ "">Perspective API</a>, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).</p>

<p>In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s <a href=""https://github.com/conversationai/unintended-ml-bias-analysis"">current models</a>. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.</p>

<p><em>Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.</em></p>","Update: Jan 30, 2018. Due to changes in the competition dataset, we have changed the evaluation metric of this competition.

Submissions are now evaluated on the mean column-wise ROC AUC. In other words, the score is the average of the individual AUCs of each predicted column.

## Submission File

For each `id` in the test set, you must predict a probability for each of the six possible types of comment toxicity (toxic, severe_toxic, obscene, threat, insult, identity_hate). The columns must be in the same order as shown below. The file should contain a header and have the following format:

    id,toxic,severe_toxic,obscene,threat,insult,identity_hate
    00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5
    0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5
    etc."
Nomad2018 Predicting Transparent Conductors,Predict the key properties of novel transparent semiconductors,https://www.kaggle.com/competitions/nomad2018-predict-transparent-conductors,https://storage.googleapis.com/kaggle-competitions/kaggle/7299/logos/header.png?t=2017-12-01-16-08-27,Chemistry,878,944,13192,"<p>Innovative materials design is needed to tackle some of the most important health, environmental, energy, social, and economic challenges of this century. In particular, improving the properties of materials that are intrinsically connected to the generation and utilization of energy is crucial if we are to mitigate environmental damage due to a growing global demand. <a href=""https://en.wikipedia.org/wiki/Transparent_conducting_film"">Transparent conductors</a>&nbsp;are an important class of compounds that are both electrically conductive and have a low absorption in the visible range, which are typically competing properties. A combination of both of these characteristics is key for the operation of a variety of technological devices such as photovoltaic cells, light-emitting diodes for flat-panel displays, transistors, sensors, touch screens, and lasers. However, only a small number of compounds are currently known to display both transparency and conductivity suitable enough to be used as transparent conducting materials. </p>
<p><a href=""https://en.wikipedia.org/wiki/Aluminium_oxide"">Aluminum</a>&nbsp;(Al), <a href=""https://en.wikipedia.org/wiki/Gallium(III)_oxide"">gallium</a>&nbsp;(Ga), <a href=""https://en.wikipedia.org/wiki/Indium(III)_oxide"">indium</a>&nbsp;(In) sesquioxides are some of the most promising transparent conductors because of a combination of both large&nbsp;<a href=""https://en.wikipedia.org/wiki/Band_gap#Photovoltaic_cells"">bandgap</a>&nbsp;energies, which leads to optical transparency over the visible range, and high&nbsp;<a href=""https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity"">conductivities</a>. These materials are also chemically stable and relatively inexpensive to produce. Alloying of these binary compounds in ternary or quaternary mixtures could enable the design of a new material at a specific composition with improved properties over what is current possible. These alloys are described by the formula \((Al_{x}Ga_{y}In_{z})_{2N}O_{3N}\); where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the <a href=""https://en.wikipedia.org/wiki/Crystal_structure""></a><em></em></p>
<p><em></em></p>","Submissions are evaluated on the column-wise <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">root mean squared  logarithmic error</a>.
<p>The RMSLE for a single column calculated as</p>
<p>$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },<br />$$</p>
<p>where:</p>
<p>\\(n\\) is the total number of observations <br />\\(p_i\\) is your prediction<br />\\(a_i\\) is the actual value <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<p>The final score is the mean of the RMSLE over all columns (in this case, 2).</p>

## Submission File
For each `id` in the test set, you must predict a value for both `formation_energy_ev_natom` and `bandgap_energy_ev`. The file should contain a header and have the following format:

    id,formation_energy_ev_natom,bandgap_energy_ev
    1,0.1779,1.8892
    2,0.1779,1.8892
    3,0.1779,1.8892
    ..."
Plant Seedlings Classification,Determine the species of a seedling from an image,https://www.kaggle.com/competitions/plant-seedlings-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/7880/logos/header.png?t=2017-11-21-19-51-20,"Plants,Image,Multiclass Classification",833,871,7385,"Can you differentiate a weed from a crop seedling?

The ability to do so effectively can mean better crop yields and better stewardship of the environment.

The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.

<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/seedlings-classify/seedlings.png"" width=""650"" /></p>

We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas.

### Acknowledgments

We extend our appreciation to the Aarhus University Department of Engineering Signal Processing Group for hosting the [original data](https://vision.eng.au.dk/plant-seedlings-dataset/). 

### Citation

[A Public Image Database for Benchmark of Plant Seedling Classification Algorithms](https://arxiv.org/abs/1711.05458)","Submissions are evaluated on **MeanFScore**,
 which at Kaggle is actually a [micro-averaged F1-score](https://en.wikipedia.org/wiki/F1_score).

Given positive/negative rates for each class *k*, the resulting score is computed this way:
 $$Precision_{micro} =  \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k + FP_k}$$
 $$Recall_{micro} =  \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k +  FN_k}$$
F1-score is the harmonic mean of precision and recall
$$MeanFScore = F1_{micro}= \frac{2 Precision_{micro} Recall_{micro}}{Precision_{micro} + Recall_{micro}}$$


## Submission File

For each`file` in the test set, you must predict a probability for the `species` variable. The file should contain a header and have the following format:

    file,species
    0021e90e4.png,Maize
    003d61042.png,Sugar beet
    007b3da8b.png,Common wheat
    etc."
Spooky Author Identification,Share code and discuss insights to identify horror authors from their writings,https://www.kaggle.com/competitions/spooky-author-identification,https://storage.googleapis.com/kaggle-competitions/kaggle/7516/logos/header.png,"Multiclass Classification,Literature,Linguistics",1241,1353,10627,"<p><img src=""https://storage.googleapis.com/kaggle-media/competitions/spooky-books/dmitrij-paskevic-44124.jpg"" alt=""Spooky books"" style=""float: right""></p>
<p>As I scurried across the candlelit chamber, manuscripts in hand, I thought I'd made it. Nothing would be able to hurt me anymore. Little did I know there was one last fright lurking around the corner.</p>

<p>DING! My phone pinged me with a disturbing notification. It was Will, the scariest of Kaggle moderators, sharing news of another data leak. </p>

<p>""<i>ph’nglui mglw’nafh Cthulhu R’lyeh wgah’nagl fhtagn!</i>"" I cried as I clumsily dropped my crate of unbound, spooky books. Pages scattered across the chamber floor. How will I ever figure out how to put them back together according to the authors who wrote them? Or are they lost, forevermore? Wait, I thought... I know, machine learning!</p>

<p>In this year's Halloween playground competition, you're challenged to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. We're encouraging you <a href="" https://www.kaggle.com/c/spooky-author-identification#Prizes"">(with cash prizes!)</a> to share your insights in the competition's discussion forum and code in Kernels. We've designated prizes to reward authors of kernels and discussion threads that are particularly valuable to the community. Click the <a href=""https://www.kaggle.com/c/spooky-author-identification"">""Prizes"" tab</a> on this overview page to learn more.</p>

<h3>Getting Started</h3>

<p>New to Kernels or working with natural language data? We've put together some starter kernels in <a href=""https://www.kaggle.com/rtatman/beginner-s-tutorial-python"">Python</a> and <a href=""https://www.kaggle.com/rtatman/beginner-s-tutorial-r"">R</a> to help you hit the ground running.</p>","Submissions are evaluated using multi-class logarithmic loss. Each id has one true class. For each id, you must submit a predicted probability for each author. The formula is then:

<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of observations in the test set, M is the number of class labels (3 classes),  \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).</p>

The submitted probabilities for a given sentences are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).

<h3>Submission File</h3>

You must submit a csv file with the id, and a probability for each of the three classes.
The order of the rows does not matter. The file must have a header and should look like the following:

<pre>id,EAP,HPL,MWS
id07943,0.33,0.33,0.33
...
</pre>"
Statoil/C-CORE Iceberg Classifier Challenge,"Ship or iceberg, can you decide from space?",https://www.kaggle.com/competitions/statoil-iceberg-classifier-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/7380/logos/header.png,"Binary Classification,Weather and Climate,Image",3330,3639,40702,"<p>
<a href=""https://imgur.com/q7uAjTM""><img src=""https://i.imgur.com/q7uAjTM.jpg"" style=""float: right""></a>
</p>

<p>
Drifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada.
</p>

<p>Currently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs.  However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite.</p>

<p><a href=""https://www.statoil.com/"">Statoil</a>, an international energy company operating worldwide, has worked closely with companies like <a href=""https://www.c-core.ca/"">C-CORE</a>. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible.</p>

<p>In this competition, you’re challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.</p>","Submissions are evaluated on the [log loss](https://www.kaggle.com/wiki/LogLoss) between the predicted values and the ground truth.</p>

# Submission File

For each id in the test set, you must predict the probability that the image contains an iceberg (a number between 0 and 1). The file should contain a header and have the following format:


<pre>id,is_iceberg<br />809385f7,0.5<br />7535f0cd,0.4<br />3aa99a38,0.9<br />etc.</pre>"
Corporación Favorita Grocery Sales Forecasting,Can you accurately predict sales for a large grocery chain?,https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting,https://storage.googleapis.com/kaggle-competitions/kaggle/7391/logos/header.png,"Regression,Food,Tabular",1671,1868,31241,"<p>Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming.</p>

<p>The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. <a href=""http://www.corporacionfavorita.com/"">Corporación Favorita</a>, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves.</p>

<p><a href=""http://www.corporacionfavorita.com/"">Corporación Favorita</a> has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time.</p>","Submissions are evaluated on the Normalized Weighted Root Mean Squared Logarithmic Error (NWRMSLE), calculated as follows:

$$ NWRMSLE = \sqrt{ \frac{\sum_{i=1}^n w_i \left( \ln(\hat{y}_i + 1) - \ln(y_i +1)  \right)^2  }{\sum_{i=1}^n w_i}} $$

where for row i, \\(\hat{y}_i\\) is the predicted unit_sales of an item and \\(y_i\\) is the actual unit_sales;  `n` is the total number of rows in the test set.

The weights, \\(w_i\\), can be found in the `items.csv` file (see the [Data page](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/data)). Perishable items are given a weight of `1.25` where all other items are given a weight of `1.00`.

This metric is suitable when predicting values across a large range of orders of magnitudes. It avoids penalizing large differences in prediction when both the predicted and the true number are large: predicting 5 when the true value is 50 is penalized more than predicting 500 when the true value is 545.

## Submission File

For each`id` in the test set, you must predict the`unit_sales`. Because the metric uses `ln(y+1)`, submissions are validated to ensure there are no negative predictions. 

The file should contain a header and have the following format:

    id,unit_sales
    125497040,2.5
    125497041,0.0
    125497042,27.9
    etc."
Porto Seguro’s Safe Driver Prediction,Predict if a driver will file an insurance claim next year.,https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction,https://storage.googleapis.com/kaggle-competitions/kaggle/7082/logos/header.png,"Tabular,Binary Classification",5156,5784,93568,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/driver.png"" alt=""NYC taxi"" style=""float: right""></p>
<p>Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.</p>

<p><a href=""https://www.portoseguro.com.br/"">Porto Seguro</a>, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.</p>

<p>In this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.</p>","## Scoring Metric
Submissions are evaluated using the Normalized Gini Coefficient.

During scoring, observations are sorted from the largest to the smallest predictions. Predictions are only used for ordering observations; therefore, the relative magnitude of the predictions are not used during scoring. The scoring algorithm then compares the cumulative proportion of positive class observations to a theoretical uniform proportion.

The Gini Coefficient ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score. The theoretical maximum for the discrete calculation is `(1 - frac_pos) / 2`.

The Normalized Gini Coefficient adjusts the score by the theoretical maximum so that the maximum score is 1.

The code to calculate Normalized Gini Coefficient in a number of different languages can be found in [this forum thread](https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703).

## Submission File
For each `id` in the test set, you must predict a probability of an insurance claim in the `target` column. The file should contain a header and have the following format:</p>
<pre>id,target<br />0,0.1<br />1,0.9<br />2,1.0<br />etc.</pre>"
Dog Breed Identification,Determine the breed of a dog in an image,https://www.kaggle.com/competitions/dog-breed-identification,https://storage.googleapis.com/kaggle-competitions/kaggle/7327/logos/header.png,"Multiclass Classification,Animals,Image",1280,1405,8708,"Who's a good dog? Who likes ear scratches? Well, it seems those fancy deep neural networks don't have *all* the answers. However, maybe they can answer that ubiquitous question we all ask when meeting a four-legged stranger: what kind of good pup is that?

In this playground competition, you are provided a strictly canine subset of [ImageNet][2] in order to practice fine-grained image categorization. How well you can tell your Norfolk Terriers from your Norwich Terriers? With 120 breeds of dogs and a limited number training images per class, you might find the problem more, err, ruff than you anticipated.

![Border collies][1]


### Acknowledgments

We extend our gratitude to the creators of the [Stanford Dogs Dataset][3] for making this competition possible: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.


  [1]: https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/border_collies.png
  [2]: https://www.kaggle.com/c/imagenet-object-detection-challenge
  [3]: http://vision.stanford.edu/aditya86/ImageNetDogs/","<p>Submissions are evaluated on <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">Multi Class Log Loss</a> between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each image in the test set, you must predict a probability for each of the different breeds. The file should contain a header and have the following format:</p>
<pre>id,affenpinscher,afghan_hound,..,yorkshire_terrier<br />000621fb3cbb32d8935728e48679680e,0.0083,0.0,...,0.0083<br />etc.</pre>"
WSDM - KKBox's Churn Prediction Challenge,Can you predict when subscribers will churn?,https://www.kaggle.com/competitions/kkbox-churn-prediction-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/7163/logos/header.png,Binary Classification,574,844,6251,"<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/7162/media/WSDM%20Banner.jpg""><p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/inline_image1.png"" alt=""Planet Aerial Imagery"" style=""float: right""></p>
<p>The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build an algorithm that predicts whether a subscription user will churn using a donated dataset from <a href=""https://www.kkbox.com"">KKBOX</a>. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models.</p>
<p>For a subscription business, accurately predicting churn is critical to long-term success. Even slight variations in churn can drastically affect profits.</p>
<p><a href=""https://www.kkbox.com"">KKBOX</a> is Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They offer a generous, unlimited version of their service to millions of people, supported by advertising and paid subscriptions. This delicate model is dependent on accurately predicting churn of their paid users.</p>

<p>In this competition you’re tasked to build an algorithm that predicts whether a user will churn after their subscription expires. Currently, the company uses survival analysis techniques to determine the residual membership life time for each subscriber. By adopting different methods, KKBOX anticipates they’ll discover new insights to why users leave so they can be proactive in keeping users dancing.</p>

<p>Winners will present their findings at the WSDM conference February 6-8, 2018 in Los Angeles, CA.  For more information on the conference, click <a href=""http://www.wsdm-conference.org/2018/call-for-participants.html"">here</a>.</p>","<p>The evaluation metric for this competition is&nbsp;<strong>Log Loss</strong></p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N {(y_i\log(p_i) + (1 - y_i)\log(1 - p_i))}$$</p>
<p>where N is the number of observations, \\(log\\) is the natural logarithm, \\(y_{i}\\) is the binary target, and \\(p_{i}\\) is the predicted probability that \\(y_i\\) equals 1.</p>
<p>Note: the actual submitted predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>For each user id (msno)&nbsp;in the test set, you must predict the probability of churn (a number between 0 and 1). The file should contain a header and have the following format:</p>
<pre>msno,is_churn<br>ugx0CjOMzazClkFzU2xasmDZaoIqOUAZPsH1q0teWCg=,0.5<br>zLo9f73nGGT1p21ltZC3ChiRnAVvgibMyazbCxvWPcg=,0.4<br>f/NmvEzHfhINFEYZTR05prUdr+E+3+oewvweYz9cCQE=,0.9<br>etc.</pre>"
Cdiscount’s Image Classification Challenge,Categorize e-commerce photos,https://www.kaggle.com/competitions/cdiscount-image-classification-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/7115/logos/header.png,Multiclass Classification,626,751,5848,"<a href=""https://www.cdiscount.com""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/Cdiscount.png""></a>

<p><a href=""https://www.kaggle.com/c/cdiscount-image-classification-challenge/rules"">Rules Update:</a> The CDiscount team has updated their rules to allow for use of this dataset for research and academic purposes only. To access the data, go to rules and accept the terms to download the data.</p>

<p><a href=""https://www.cdiscount.com/"">Cdiscount.com</a> generated nearly 3 billion euros last year, making it France’s largest non-food e-commerce company. While the company already sells everything from TVs to trampolines, the list of products is still rapidly growing. By the end of this year, Cdiscount.com will have over 30 million products up for sale. This is up from 10 million products only 2 years ago. Ensuring that so many products are well classified is a challenging task.</p>

<p>Currently, Cdiscount.com applies machine learning algorithms to the text description of the products in order to automatically predict their category. As these methods now seem close to their maximum potential, Cdiscount.com believes that the next quantitative improvement will be driven by the application of data science techniques to images.</p>

<p>In this challenge you will be building a model that automatically classifies the products based on their images. As a quick tour of Cdiscount.com's website can confirm, one product can have one or several images. The data set Cdiscount.com is making available is unique and characterized by superlative numbers in several ways:</p>

<ul>
<li>Almost 9 million products: half of the current catalogue</li>
<li>More than 15 million images at 180x180 resolution</li>
<li>More than 5000 categories: yes this is quite an extreme multi-class classification!</li>
</ul>","<h2>Goal</h2>
The goal of this competition is to predict the category of a product based on its image(s). Note that a product can have one or several images associated. For every product <i>_id</i> in the test set, you should predict the correct <i>category_id</i>.

<h2>Metric</h2>
This competition is evaluated on the categorization accuracy of your predictions (the percentage of products you get correct).
<h2>Submission File</h2>
<p>For each <i>_id</i>&nbsp;in the test set, you must predict a <i>category_id</i>. The file should contain a header and have the following format:</p>
<pre>_id,category_id<br>2,1000000055<br>5,1000016018<br>6,1000016055<br>etc.</pre>"
Text Normalization Challenge - Russian Language,Convert Russian text from written expressions into spoken forms,https://www.kaggle.com/competitions/text-normalization-challenge-russian-language,https://storage.googleapis.com/kaggle-competitions/kaggle/7043/logos/header.png,"Text,Linguistics,Languages",162,175,872,"<p>As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine.</p>
<p>Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents.""&nbsp;</p>
<p>However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that&nbsp;requires quite a bit of linguistic sophistication and native speaker intuition.</p>

<pre>Проверено    	&lt;self&gt;
12 февраля 2013    двенадцатого февраля две тысячи тринадцатого года
,    sil
Архивировано    &lt;self&gt;
из    &lt;self&gt;
первоисточника    &lt;self&gt;
15 февраля 2013    февраля две тысячи тринадцатого года
.    sil
</pre>

<p>In this competition, you are challenged&nbsp;to automate the process of developing&nbsp;text normalization grammars via&nbsp;machine learning.&nbsp;This track will focus on Russian, while a separate will focus on English here:&nbsp;<a href=""text-normalization-challenge-english-language"" target=""_blank"">English Text Normalization Challenge</a></p>
<h2>About the sponsor</h2>
Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.<p></p>","<p>Submissions are evaluated on prediction accuracy (the total percent of correct tokens). The predicted and actual string must match exactly in order to count as correct.  In other words, we are measuring sequence accuracy, in that any error in the output for a given token in the input sequence means that that error is wrong. For example, if the input is ""145"" and the predicted output is ""one forty five"" but the correct output is ""one hundred forty five"", this is counted as a single error. </p>
<h2>Submission File</h2>
<p>For each token (id)&nbsp;in the test set, you must predict the normalized text. The file should contain a header and have the following format:</p>
<pre>id,after<br>0_0,""Производится""<br>0_1,""в""<br>0_2,""Азии""<br>...</pre>"
Text Normalization Challenge - English Language,Convert English text from written expressions into spoken forms,https://www.kaggle.com/competitions/text-normalization-challenge-english-language,https://storage.googleapis.com/kaggle-competitions/kaggle/7042/logos/header.png,"Linguistics,Languages,Text",260,279,1833,"<p>As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine.</p>
<p>Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents.""&nbsp;</p>
<p>However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that&nbsp;requires quite a bit of linguistic sophistication and native speaker intuition.</p>

<pre>A    &lt;self&gt;
baby    &lt;self&gt;
giraffe    &lt;self&gt;
is    &lt;self&gt;
6ft    six feet
tall    &lt;self&gt;
and    &lt;self&gt;
weighs    &lt;self&gt;
150lb    one hundred fifty pounds
.    sil
</pre>

<p>In this competition, you are challenged&nbsp;to automate the process of developing&nbsp;text normalization grammars via&nbsp;machine learning.&nbsp;This track will focus on English, while a separate will focus on Russian here:&nbsp;<a href=""text-normalization-challenge-russian-language"" target=""_blank"">Russian Text Normalization Challenge</a></p>
<h2>About the sponsor</h2>
Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.<p></p>","<p>Submissions are evaluated on prediction accuracy (the total percent of correct tokens). The predicted and actual string must match exactly in order to count as correct.  In other words, we are measuring sequence accuracy, in that any error in the output for a given token in the input sequence means that that error is wrong. For example, if the input is ""145"" and the predicted output is ""one forty five"" but the correct output is ""one hundred forty five"", this is counted as a single error. </p>
<h2>Submission File</h2>
<p>For each token (id)&nbsp;in the test set, you must predict the normalized text. The file should contain a header and have the following format:</p>
<pre>id,after<br>0_0,""the""<br>0_1,""quick""<br>0_2,""fox""<br>...</pre>"
Carvana Image Masking Challenge,Automatically identify the boundaries of the car in an image,https://www.kaggle.com/competitions/carvana-image-masking-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/6927/logos/header.png,"Automobiles and Vehicles,Image",734,874,6878,"<p>As with any big purchase, full information and transparency are key. While most everyone describes buying a used car as frustrating, it’s just as annoying to sell one, especially online. Shoppers want to know everything about the car but they must rely on often blurry pictures and little information, keeping used car sales a largely inefficient, local industry.</p>
<p><a href=""https://www.carvana.com/"" target=""_blank"">Carvana</a>, a successful online used car startup, has seen opportunity to build long term trust with consumers and streamline the online buying process.</p>
<p>An interesting part of their innovation is a custom rotating photo studio that automatically captures and processes 16 standard images of each vehicle in their inventory. While Carvana takes high quality photos,  bright reflections and cars with similar colors as the background cause automation errors, which requires a skilled photo editor to change.</p>
<p><br><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/carvana_graphics.png"" style=""display: block; margin-left: auto; margin-right: auto""><br></p>
<p>In this competition, you’re challenged to develop an algorithm that automatically removes the photo studio background. This will allow Carvana to superimpose cars on a variety of backgrounds. You’ll be analyzing a dataset of photos, covering different vehicles with a wide variety of year, make, and model combinations.</p>","<p>This competition is evaluated on the mean <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"">Dice coefficient</a>. The Dice coefficient can be used to compare the pixel-wise&nbsp;agreement&nbsp;between a predicted segmentation&nbsp;and its corresponding ground truth. The formula is given by:</p>
<p>$$&nbsp;\frac{2 * |X \cap Y|}{|X| + |Y|},$$</p>
<p>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.</p>
<h2>Submission File</h2>
<p><strong>In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.</strong>&nbsp; Instead of submitting an exhaustive list of indices for your&nbsp;segmentation, you&nbsp;will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and&nbsp;running a total of 3 pixels (1,2,3).</p>
<p>The competition format requires&nbsp;a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded&nbsp;pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.</p>
<p>The file should contain a header and have the following format:</p>
<pre>img,rle_mask<br>0004d4463b50_01,1 1 5 1<br>0004d4463b50_02,1 1<br>0004d4463b50_03,1 1<br>etc.</pre>
<p>Submission files may take several minutes to process due to the size.</p>"
New York City Taxi Trip Duration,Share code and data to improve ride time predictions,https://www.kaggle.com/competitions/nyc-taxi-trip-duration,https://storage.googleapis.com/kaggle-competitions/kaggle/6960/logos/header.png,"Tabular,Regression",1254,1358,11193,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/taxi_meter.png"" alt=""NYC taxi"" style=""float: right""></p>
<p>In this competition, Kaggle is challenging you to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.</p>

<p>Longtime Kagglers will recognize that this competition objective is similar to the <a href=""https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii"" target=""blank"">ECML/PKDD trip time challenge</a> we hosted in 2015. But, this challenge comes with a twist. Instead of awarding prizes to the top finishers on the leaderboard, this playground competition was created to reward collaboration and collective learning. </p>

<p>We are encouraging you (<a href=""https://www.kaggle.com/c/nyc-taxi-trip-duration#Prizes"" target=""_blank"">with cash prizes!</a>) to publish additional training data that other participants can use for their predictions. We also have designated bi-weekly and final prizes to reward authors of <a href=""https://www.kaggle.com/c/nyc-taxi-trip-duration/kernels"" target=""_blank"">kernels</a> that are particularly insightful or valuable to the community.</p>","<p>The evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">Root Mean Squared Logarithmic Error</a>.</p>
<p>The RMSLE is calculated as</p>
<p>$$<br />\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }<br />$$</p>
<p>Where:</p>
<p>\\(\epsilon\\) is the RMSLE value (score)<br />\\(n\\) is the total number of observations in the (public/private) data set,<br />\\(p_i\\) is your prediction of trip duration, and<br />\\(a_i\\) is the actual trip duration for \\(i\\). <br />\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<h2>Submission File</h2>
<p><strong>For every row in the dataset</strong>, submission files should contain two columns: id and trip_duration.  The id corresponds to the column of that id in the test.csv. The file should contain a header and have the following format:</p>
<pre>id,trip_duration<br />id00001,978<br />id00002,978<br />id00003,978<br />id00004,978<br />etc.</pre>"
Web Traffic Time Series Forecasting,Forecast future traffic to Wikipedia pages,https://www.kaggle.com/competitions/web-traffic-time-series-forecasting,https://storage.googleapis.com/kaggle-competitions/kaggle/6768/logos/header.png,"Internet,Tabular",1095,424,681,"<p>This competition focuses on the problem of forecasting
the future values of multiple time series, as it has always been one of the most challenging
problems in the field. More specifically, we aim the competition at testing state-of-the-art
methods designed by the participants, on the problem of forecasting future web traffic for
approximately 145,000 Wikipedia articles.</p>

<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/business-size_1x.png"" alt=""Planet Aerial Imagery"" style=""float: right"">

<p>Sequential or temporal observations emerge in many key real-world problems, ranging
from biological data, financial markets, weather forecasting, to audio and video processing.
The field of time series encapsulates many different problems, ranging from analysis and
inference to classification and forecast. What can you do to help predict future views? </p>

<p>This competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.
</p>

<p>
You have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.
</p>

<p>
We thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it.</p>
<p><br><i>Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.</i></p>
","<p>Submissions are evaluated on <a href=""https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error"">SMAPE</a>&nbsp;between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.</p>
<h2>Submission File</h2>
<p>For each article and day combination (see key.csv), you must predict the web traffic. The file should contain a header and have the following format:</p>
<pre>Id,Visits<br>bf4edcf969af,0<br>929ed2bf52b9,0<br>ff29d0f51d5c0<br>etc.</pre>
<p></p>
<p>Due to the large file size and number of rows, submissions may take a few minutes to score. Thank you for your patience.</p>"
NIPS 2017: Defense Against Adversarial Attack,Create an image classifier that is robust to adversarial attacks,https://www.kaggle.com/competitions/nips-2017-defense-against-adversarial-attack,https://storage.googleapis.com/kaggle-competitions/kaggle/6867/logos/header.png,"Adversarial Learning,Image",107,187,109,"***This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.***

Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.

Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.

To accelerate research on adversarial examples, [Google Brain](http://g.co/brain) is organizing **Competition on Adversarial Attacks and Defenses** within the [NIPS 2017 competition track](https://nips.cc/Conferences/2017/CompetitionTrack).

The competition on Adversarial Attacks and Defenses consist of three sub-competitions:

- [Non-targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack). The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
- [Targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack). The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
- [Defense Against Adversarial Attack](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack). The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.

In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.

*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.*","# Evaluation metric

Each defense classifier will be run on all adversarial images generated by all adversarial attacks (both targeted and non-targeted). For each correctly classified image defense classifier will get one point. In other words score for each defense will be computed using the following formula:

$$
score_{defense} = \sum_{attack \in A} \sum_{k=1}^{N} [defense(attack(Image_k)) = TrueLabel_k] 
$$

where:

* \\(A\\) is the set of all attacks, including targeted and non-targeted attacks
* \\(N\\) - number of images in the dataset
* \\(Image_{k}\\) - k-th image from the dataset
* \\(TrueLabel{k}\\) - ground truth label for image k
* \\([ ]\\) - indicator function which equals to 1 when Predicate is true, otherwise equals to 0

Higher score means better defense.
"
NIPS 2017: Targeted Adversarial Attack,Develop an adversarial attack that causes image classifiers to predict a specific target class,https://www.kaggle.com/competitions/nips-2017-targeted-adversarial-attack,https://storage.googleapis.com/kaggle-competitions/kaggle/6866/logos/header.png,"Adversarial Learning,Image",65,107,65,"***This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.***

Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.

Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.

To accelerate research on adversarial examples, [Google Brain](http://g.co/brain) is organizing **Competition on Adversarial Attacks and Defenses** within the [NIPS 2017 competition track](https://nips.cc/Conferences/2017/CompetitionTrack).

The competition on Adversarial Attacks and Defenses consist of three sub-competitions:

- [Non-targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack). The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
- [Targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack). The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
- [Defense Against Adversarial Attack](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack). The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.

In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.

*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.*","# Evaluation metric

Targeted adversarial attack will be provided set of images and one target class for each image.
All submitted targeted attacks will be provided the same set of images and target classes.

For each source image targeted attack is expected to produce adversarial image which is likely to be classified as desired target class by image classifier. All adversarial images generated by the targeted attack will be run through all submitted defense classifiers. Attack will get one point each time defense classifier outputs label which is equal to target label. In other words score for each targeted attack will be computed using the following formula:

$$
score_{TargetedAttack} = \sum_{defense \in D} \sum_{k=1}^{N} [defense(TargetedAttack(Image_k)) = TargetLabel_k] 
$$

where:

* \\(D\\) is the set of all defenses
* \\(N\\) - number of images in the dataset
* \\(Image_{k}\\) - k-th image from the dataset
* \\(TargetLabel_{k}\\) - target label for image k
* \\([ ]\\) - indicator function which equals to 1 when Predicate is true, otherwise equals to 0

Higher score means better targeted attack."
NIPS 2017: Non-targeted Adversarial Attack,Imperceptibly transform images in ways that fool classification models,https://www.kaggle.com/competitions/nips-2017-non-targeted-adversarial-attack,https://storage.googleapis.com/kaggle-competitions/kaggle/6864/logos/header.png,"Adversarial Learning,Image",91,153,91,"***This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.***

Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.

Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.

To accelerate research on adversarial examples, [Google Brain](http://g.co/brain) is organizing **Competition on Adversarial Attacks and Defenses** within the [NIPS 2017 competition track](https://nips.cc/Conferences/2017/CompetitionTrack).

The competition on Adversarial Attacks and Defenses consist of three sub-competitions:

- [Non-targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack). The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
- [Targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack). The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
- [Defense Against Adversarial Attack](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack). The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.

In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.

*Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.*","# Evaluation metric

Adversarial attack will be run on a given dataset (same for all attacks) and is expected to produce adversarial image for each image from the dataset. All adversarial images generated by the attack will be run through all submitted defense classifiers. Attack will get one point for each incorrect classification, i.e. for each pair of adversarial image and defense when adversarial image was misclassified by the defense. In other words score for each attack will be computed using the following formula:

$$
score_{attack} = \sum_{defense \in D} \sum_{k=1}^{N} [defense(attack(Image_k)) \ne TrueLabel_k] 
$$

where:

* \\(D\\) is the set of all defenses
* \\(N\\) - number of images in the dataset
* \\(Image_{k}\\) - k-th image from the dataset
* \\(TargetLabel_{k}\\) - target label for image k
* \\([ ]\\) - indicator function which equals to 1 when Predicate is true, otherwise equals to 0

Higher score means better attack."
Personalized Medicine: Redefining Cancer Treatment,Predict the effect of Genetic Variants to enable Personalized Medicine,https://www.kaggle.com/competitions/msk-redefining-cancer-treatment,,"Text,Multiclass Classification,Genetics",1386,445,3032,"<p>A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated.</p>
<p>But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the <a href=""https://nips.cc/Conferences/2017/CompetitionTrack"">NIPS 2017 Competition Track</a>, &nbsp;because we need your help to take personalized medicine to its full potential.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6841/media/79842_Web-hero-image_ALT-3.jpg"" alt=""""></p>
<p>Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers).&nbsp;</p>
<p>Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.</p>
<p>For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations.</p>
<p>We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.</p>
<p><br><i>Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.</i></p>
<h3></h3>","<p>Submissions are evaluated on <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">Multi Class Log Loss</a>&nbsp;between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each ID&nbsp;in the test set, you must predict a probability for each of the different classes a genetic mutation can be classified on. The file should contain a header and have the following format:</p>
<pre>ID,class1,class2,class3,class4,class5,class6,class7,class8,class9<br>0,0.1,0.7,0.05,0.05,0.1,0,0,0,0<br>1,0.7,0.1,0.05,0.05,0.1,0,0,0,0<br>2,0.05,0.05,0.1,0.7,0.1,0,0,0,0<br>3,0,0,0,0,0.05,0.05,0.1,0,7,0,1<br>etc.</pre>"
Passenger Screening Algorithm Challenge,Improve the accuracy of the Department of Homeland Security's threat recognition algorithms,https://www.kaggle.com/competitions/passenger-screening-algorithm-challenge,https://storage.googleapis.com/kaggle-competitions/kaggle/6775/logos/header.png,Image,518,217,396,"<p>While long lines and frantically shuffling luggage into plastic bins isn’t a fun experience, airport security is a critical and necessary requirement for safe travel.</p>

<p>No one understands the need for both thorough security screenings and short wait times more than U.S. Transportation Security Administration (TSA). They’re responsible for all U.S. airport security, screening more than two million passengers daily.</p>

<p>As part of their Apex Screening at Speed Program, DHS has identified high false alarm rates as creating significant bottlenecks at the airport checkpoints. Whenever TSA’s sensors and algorithms predict a potential threat, TSA staff needs to engage in a secondary, manual screening process that slows everything down. And as the number of travelers increase every year and new threats develop, their prediction algorithms need to continually improve to meet the increased demand.</p>

<p>Currently, TSA purchases updated algorithms exclusively from the manufacturers of the scanning equipment used. These algorithms are proprietary, expensive, and often released in long cycles. In this competition, TSA is stepping outside their established procurement process and is challenging the broader data science community to help improve the accuracy of their threat prediction algorithms. Using a dataset of images collected on the latest generation of scanners, participants are challenged to identify the presence of simulated threats under a variety of object types, clothing types, and body types. Even a modest decrease in false alarms will help TSA significantly improve the passenger experience while maintaining high levels of security.</p>

<p>This is a two-stage competition. Please read our <a href=""https://www.kaggle.com/two-stage-faq"">two-stage FAQs</a> to understand more about what this means.</p>

<p>All persons contained in the dataset are volunteers who have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.</p>","For every scan in the dataset, you will be predicting the probability that a threat is present in each of 17 body zones. A diagram of the body zone locations is available in the competition files section.

The description of each zone is as follows:

![Body zone descriptions][1]

If there are N images, you will be making 17N predictions. Submissions are scored on the log loss:

$$ - \frac{1}{N} \sum_{i=1}^N \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],$$

where:

 - N is the 17 * the number of scans in the test set
 - \\( \hat{y}_i \\) is the predicted probability of the scan having a threat in the given body zone
 - \\( y_i \\) is 1 if a threat is present, 0 otherwise
 - \\( log() \\) is the natural (base e) logarithm

Note: the actual submitted predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\). A smaller log loss is better.

## Submission File

You must predict a probability for each Id and body zone. The Id used for the submission is created by concatenating the image Id with the zone for which you are predicting ('_Zone1' through '_Zone17'). The file should have a header and be in the following format:

    Id,Probability  
    0397026df63bbc8fd88f9860c6e35b4a_Zone1,0.002  
    0397026df63bbc8fd88f9860c6e35b4a_Zone2,0.32  
    0397026df63bbc8fd88f9860c6e35b4a_Zone3,0.88  
    etc...


  [1]: https://storage.googleapis.com/kaggle-competitions/kaggle/6775/media/body_zone_descriptions.png"
iMaterialist Challenge at FGVC 2017,Can you assign accurate description labels to images of apparel products?,https://www.kaggle.com/competitions/imaterialist-challenge-FGVC2017,https://storage.googleapis.com/kaggle-competitions/kaggle/6539/logos/header.png,"Clothing and Accessories,Image",28,34,260,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6539/media/imaterialist.png"" style=""float: right; padding-left: 10px""></p>
<p>As shoppers move online, it’d be a dream come true to have product attributes in photos detected automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine grained attribute labels may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply can’t perceive such subtle differences between photos.</p>
<p>Tackling issues like this is why the <a href=""http://cvpr2017.thecvf.com/"">Conference on Computer Vision and Pattern Recognition (CVPR)</a> has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the <a href=""https://sites.google.com/corp/view/fgvc4/home"">FGVC4 workshop</a>. As part of this workshop, CVPR is partnering with Google to challenge the data science community to help push the state of the art in automatic image classification.</p>
<p>In this competition, FGVC workshop organizers and Google challenge you to develop algorithms that will help with the an important step towards automatic product detection–accurately assigning attribute labels for product images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC4 workshop.</p>
<p><i>Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.</i></p>","<p>Submissions are evaluated on top 1 error rate&nbsp;of the predictions. For each image \( i \) and task \( t\), there might be multiple labels \(g_{it,k}\)&nbsp;and the algorithm will produce a most confident label \(l_{it}\). As long as the label produced is the same as one of the ground truth labels, then the prediction is considered to be correct. Mathematically, the error rate for a single image and task is:</p>
<p>$$e_{it} = \min_{k}d(l_{it},g_{it,k})$$</p>
<p>Where</p>
<p>$$d(x,y) = \begin{cases} 0, &amp; \text{if}\ x=y \\ 1, &amp; \text{otherwise} \end{cases}$$</p>
<p>The overall error score is the average error over all test images and tasks</p>
<p>$$score = \frac{1}{N} \sum_{i,t}(e_{it})$$ where \( N \) is the total number of valid ground truth labels</p>
<h2>Submission File</h2>
<p>For each image_id and task_id&nbsp;in the test set, you must produce the most confident label.&nbsp;The file should contain a header and have the&nbsp;format below. Besides the header, each row has two columns. First column specifies the image and task ids, in the format of ""imageid_taskid"". Second column is the label id.</p>
<pre>id,expected<br>1_1,0<br>1_2,1<br>2_1,10<br>etc.</pre>
<p>Please note that, for each test image, participants need to produce labels for all the possible tasks specified in the task dictionary. So the submission file should have N rows, where:</p>
<p><em>N = number_test_image * number_tasks</em></p>"
iNaturalist Challenge at FGVC 2017,"Fine-grained classification challenge spanning 5,000 species.",https://www.kaggle.com/competitions/inaturalist-challenge-at-fgvc-2017,,"Animals,Plants,Image",50,60,618,"<p>With so much diversity, accurately classifying animals and plants is a tough challenge. Check out the photos below. Alpaca or Llama? Donkey or mule? Roses or kale?</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6722/media/Screen%20Shot%202017-05-16%20at%2011.07.20%20PM.png"" alt=""""></p>
<p>It’s estimated that our planet contains several million species of plants and animals–many that look really similar to each other. Because of this, a lot of species in the natural world are too hard to classify without an expert.</p>
<p>As part of the <a href=""https://sites.google.com/view/fgvc4/home"">FGVC4 workshop</a>&nbsp;at <a href=""http://cvpr2017.thecvf.com/"">CVPR 2017</a>&nbsp;we are conducting the iNat Challenge 2017 large scale species classification competition, sponsored by Google. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features fine-grained categories, big class imbalances, and large numbers of classes.</p>
<p><br>The iNat Challenge 2017 dataset contains 5,089&nbsp;species, with a combined training and validation set of 675,000 images that have been collected and verified by multiple users from <a href=""http://www.inaturalist.org/"">inaturalist.org</a>. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world.&nbsp; Example images, along with their unique <a href=""http://www.gbif.org/"">GBIF</a>&nbsp;ID numbers (where available), can be viewed <a href=""https://docs.google.com/spreadsheets/d/1JHn6J_9HBYyN5kaVrH1qcc3VMyxOsV2II8BvSwufM54"">here</a>.</p>
<p>Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the&nbsp;<a href=""https://sites.google.com/view/fgvc4/home"">FGVC4 workshop</a>.&nbsp;</p>
<p><br><i>Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.</i></p>","<p>We follow a similar metric to the classification tasks of the <a href=""http://image-net.org/challenges/LSVRC/2016/index#scene"">ILSVRC</a>. For each image, an algorithm will produce 5 labels. We allow 5 labels because some categories are disambiguated with additional data provided by the observer, such as latitude, longitude and date. It might also be the case that multiple categories occur in an image (e.g. a photo of a bee on a flower). For this competition each image has one ground truth label. For a given image, if the ground truth label is found among the 5 predicted labels, then the error for that image is 0, otherwise it is 1. The final score is the error averaged across all images. See <a href=""https://github.com/visipedia/inat_comp#evaluation"">here</a>&nbsp;for additional information.&nbsp;</p>
<h2>Submission File</h2>
<p>For each image&nbsp;in the test set, you must predict 5 class labels. The csv file should contain a header and have the following format:</p>
<pre>id,predicted<br>12345,0 78 23 3 42<br>67890,83 13 42 0 21</pre>"
Mercedes-Benz Greener Manufacturing,Can you cut the time a Mercedes-Benz spends on the test bench?,https://www.kaggle.com/competitions/mercedes-benz-greener-manufacturing,https://storage.googleapis.com/kaggle-competitions/kaggle/6565/logos/header.png,"Automobiles and Vehicles,Regression,Tabular",3823,4032,75157,"<p>Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. <a href=""https://www.kaggle.com/daimler"" target=""_blank"">Daimler’s</a> Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. .</p>
<p>To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6565/media/daimler-mercedes%20V02.jpg""></p>
<p>In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler’s standards.</p>","Submissions are evaluated on the R^2 value, also called the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination). 

## Submission File

For each 'ID' in the test set, you must predict the 'y' variable. The file should contain a header and have the following format:

    ID,y  
    1,100  
    2,100.33  
    3,105.81  
    ..."
Zillow Prize: Zillow’s Home Value Prediction (Zestimate),Can you improve the algorithm that changed the world of real estate?,https://www.kaggle.com/competitions/zillow-prize-1,https://storage.googleapis.com/kaggle-competitions/kaggle/6649/logos/header.png,"Housing,Real Estate",3770,4241,68053,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6649/media/_zillow_image_2.jpg"" alt=""Planet Aerial Imagery"" style=""float: right""></p>
<p>Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago.</p>
<p>A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost.</p>
<p>“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property.   And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning.</p>
<p>Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of  110M homes across the U.S.</p>

<p>In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens  on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition.</p>

<p>Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won’t be revealed until the close of the sales tracking period.</p>","<p>Submissions are evaluated on <a href=""https://www.kaggle.com/wiki/MeanAbsoluteError"">Mean Absolute Error</a>&nbsp;between the predicted log error and the actual log error. The log error is defined as $$logerror=log(Zestimate)-log(SalePrice)$$ and it is recorded in the transactions training data. If a transaction didn't happen for a property during that period of time, that row is ignored and not counted in the calculation of MAE.</p>

<h3>Submission File</h3>
<p>For each property (unique parcelid), you must predict a log error for each time point. You should be predicting 6 timepoints: <strong>October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712)</strong>.  The file should contain a header and have the following format:</p>
<pre>ParcelId,201610,201611,201612,201710,201711,201712
10754147,0.1234,1.2234,-1.3012,1.4012,0.8642-3.1412
10759547,0,0,0,0,0,0
etc.
</pre>
Note that the actual log errors are accurate the 4th decimal places, so you can adjust your decimal formats to limit the size of your submission file. "
Instacart Market Basket Analysis,Which products will an Instacart consumer purchase again?,https://www.kaggle.com/competitions/instacart-market-basket-analysis,https://storage.googleapis.com/kaggle-competitions/kaggle/6644/logos/header.png,Food,2621,2621,39863,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/instacart_sidebar.png"" alt=""instacart shopper"" style=""float: right; margin-left: 10px""></p>
<p>Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.</p>
<p>Instacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on <a href=""https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2"">3 Million Instacart Orders, Open Sourced.</a></p>
<p>In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. They’re not only looking for the best model, Instacart’s also looking for <a href=""https://boards.greenhouse.io/instacart/jobs/590555"">machine learning engineers</a> to grow their team.</p>
<p>Winners of this competition will receive both a cash prize and a fast track through the recruiting process. For more information about exciting opportunities at Instacart, check out their <a href=""https://careers.instacart.com/"">careers page here</a> or e-mail their recruiting team directly at <a>ml.jobs@instacart.com.</a></p>","Submissions will be evaluated based on their [mean F1 score](https://en.wikipedia.org/wiki/F1_score).

## Submission File

For each order_id in the test set, you should predict a space-delimited list of product_ids for that order. If you wish to predict an empty order, you should submit an explicit 'None' value. You may combine 'None' with product_ids. The spelling of 'None' is case sensitive in the scoring metric. The file should have a header and look like the following:

    order_id,products  
    17,1 2  
    34,None  
    137,1 2 3  
    etc."
Invasive Species Monitoring,Identify images of invasive hydrangea,https://www.kaggle.com/competitions/invasive-species-monitoring,https://storage.googleapis.com/kaggle-competitions/kaggle/6469/logos/header.png,"Image,Plants",511,526,4239,"<p>Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they’re difficult to undertake at scale.</p>
<p>Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/invasive-species-monitoring.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems.</p>
<p>In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable.</p>
<h3>Acknowledgments</h3>
<p>Data providers: <a href=""https://www.linkedin.com/in/requenac"">Christian Requena Mesa</a>, Thore Engel, <a href="" https://www.linkedin.com/in/amrita-menon-12b58947/"">Amrita Menon</a>, Emma Bradley.</p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each image&nbsp;in the test set, you must predict a probability for the target variable on whether the image contains invasive species or not. The file should contain a header and have the following format:</p>
<pre>name,invasive<br>2,0.5<br>5,0<br>6,0.2<br>etc.</pre>"
Sberbank Russian Housing Market,Can you predict realty price fluctuations in Russia’s volatile economy?,https://www.kaggle.com/competitions/sberbank-russian-housing-market,https://storage.googleapis.com/kaggle-competitions/kaggle/6392/logos/header.png,"Banking,Housing,Regression,Tabular",3264,3658,67581,"<p>Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget—whether personal or corporate—the last thing anyone needs is uncertainty about one of their biggets expenses. <a href=""https://www.kaggle.com/sberbank"" target=""_blank"">Sberbank</a>, Russia’s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building.</p>
<p>Although the housing market is relatively stable in Russia, the country’s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal.</p>
<p>In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.</p>","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">RMSLE</a> between their predicted prices and the actual data. The target variable, called <strong>price_doc</strong> in the training set, is the sale price of each property.</p>
<h2>Submission File</h2>
<p>For each id&nbsp;in the test set, you must predict the price that the property sold for. The file should contain a header and have the following format:</p>
<pre>id,price_doc<br>30474,7118500.44<br>30475,7118500.44<br>30476,7118500.44<br>etc.</pre>"
Planet: Understanding the Amazon from Space,Use satellite data to track the human footprint in the Amazon rainforest,https://www.kaggle.com/competitions/planet-understanding-the-amazon-from-space,https://storage.googleapis.com/kaggle-competitions/kaggle/6322/logos/header.png,"Image,Forestry",936,1181,20666,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6322/media/planet.png"" alt=""Planet Aerial Imagery"" style=""float: right""></p>
<p>Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively.</p>
 <p><a href=""https://www.planet.com/company/careers/"" target=""_blank"">Planet</a>, designer and builder of the world’s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate.</p>
<p>Furthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery.
</p>
<p>In this competition, Planet and its Brazilian partner <a href=""http://www.sccon.com.br/"" target=""_blank"">SCCON</a> are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond.</p>
<p>To dig into/explore more Planet data, <a href=""https://www.planet.com/explorer/?signup&amp;utm_source=kaggle"" target=""_blank"">sign up for a free account.</a></p>
<p>And if you're interested in building applications on Planet data, check out our <a href=""https://www.planet.com/products/developer-program/?utm_source=kaggle"" target=""_blank"">Application Developer Program.</a></p>
<h3>Getting Started</h3>
<ol>
<li> Review the <a href=""https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data"" target=""_blank"">data page</a>, which includes detailed information about the labels and the labeling process.</li>
<li>Download a subsample of the data to get familiar with how it looks.</li>
<li>Explore the subsample on Kernels. We’ve <a href=""https://www.kaggle.com/robinkraft/planet-understanding-the-amazon-from-space/getting-started-with-the-data-now-with-docs"" target=""_blank"">created a notebook</a> for you to get started.</li></ol>","<p>Submissions will be evaluated based on their mean (F_{2}) score.&nbsp;The F score, commonly used in information retrieval, measures accuracy&nbsp;using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all&nbsp;actual positives (tp + fn). The (F_{2})&nbsp;score is given by</p>
<p>$$<br>(1 + \beta^2)&nbsp;\frac{pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 2.<br>$$</p>
<p>Note that the (F_{2}) score&nbsp;weights recall&nbsp;higher than precision. The mean&nbsp;(F_{2}) score is formed by averaging the individual (F_{2}) scores for each row in the test set.</p>
<h2>Submission File</h2>
<p>For each image&nbsp;listed in the test set, predict a space-delimited list of tags&nbsp;which you believe are associated with the image. There are 17 possible tags: <strong>agriculture, artisinal_mine, bare_ground, blooming, blow_down, clear, cloudy, conventional_mine, cultivation, habitation, haze, partly_cloudy, primary, road, selective_logging, slash_burn, water</strong>. The file should contain a header and have the following format:</p>
<pre>image_name,tags<br>test_0,agriculture road water<br>test_1,primary clear<br>test_2,haze primary<br>etc.</pre>"
NOAA Fisheries Steller Sea Lion Population Count,How many sea lions do you see?,https://www.kaggle.com/competitions/noaa-fisheries-steller-sea-lion-population-count,https://storage.googleapis.com/kaggle-competitions/kaggle/6116/logos/header.png,"Image,Water Bodies,Animals",385,463,6015,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/noaa_sea_lions.png"" alt=""NOAA Stellar Sea Lions"" style=""float: right""></p>
<p>Steller sea lions in the western Aleutian Islands have declined 94 percent in the last 30 years. The endangered western population, found in the North Pacific, are the focus of conservation efforts which require annual population counts. Specially trained scientists at <a href=""https://www.afsc.noaa.gov/"" target=""_blank"">NOAA Fisheries Alaska Fisheries Science Center</a> conduct these surveys using airplanes and unoccupied aircraft systems to collect aerial images. Having accurate population estimates enables us to better understand factors that may be contributing to lack of recovery of Stellers in this area.</p>
<p>Currently, it takes biologists up to four months to count sea lions from the thousands of images NOAA Fisheries collects each year. Once individual counts are conducted, the tallies must be reconciled to confirm their reliability. The results of these counts are time-sensitive.</p>
<p>In this competition, Kagglers are invited to develop algorithms which accurately count the number of sea lions in aerial photographs. Automating the annual population count will free up critical resources allowing NOAA Fisheries to focus on ensuring we hear the sea lion’s roar for many years to come. Plus, advancements in computer vision applied to aerial population counts may also greatly benefit other endangered species.<br></p>
<h3>Resources</h3>
<p>Learn more about research being done to better understand what's going on with the endangered Steller sea lion populations by joining scientists on a research vessel to the western Aleutian Islands in the video below.</p>
<p></p>","<p>The aim of the competition is to count each type of steller sea lion in each photo. See the Data tab for more details.</p>
<p>Your submission file should have the following format:</p>
<pre>test_id,adult_males,subadult_males,adult_females,juveniles,pups<br>0,1,1,1,1,1<br>1,1,1,1,1,1<br>2,1,1,1,1,1<br>etc</pre>
<p><br>Your submissions will be evaluated by their <a href=""https://www.kaggle.com/wiki/RootMeanSquaredError"">RMSE</a>&nbsp;from the human-labelled ground truth, averaged over the columns.</p>"
Quora Question Pairs,Can you identify question pairs that have the same intent?,https://www.kaggle.com/competitions/quora-question-pairs,https://storage.googleapis.com/kaggle-competitions/kaggle/6277/logos/header.png,"Text,Tabular,Linguistics,Internet",3295,3850,53687,"<p>Where else but <a href=""https://www.kaggle.com/quora"" target=""_blank"">Quora</a>&nbsp;can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.</p>
<p>Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.</p>
<p>Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.</p>","<p>Submissions are evaluated on the&nbsp;<a href=""https://www.kaggle.com/wiki/LogarithmicLoss"">log loss</a> between the predicted values and the ground truth.</p>
<h2>Submission File</h2>
<p>For each ID&nbsp;in the test set, you must predict the probability that the questions are duplicates (a number between 0 and 1). The file should contain a header and have the following format:</p>
<pre>test_id,is_duplicate<br>0,0.5<br>1,0.4<br>2,0.9<br>etc.</pre>"
Intel & MobileODT Cervical Cancer Screening,Which cancer treatment will be most effective?,https://www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening,https://storage.googleapis.com/kaggle-competitions/kaggle/6243/logos/header.png,"Healthcare,Multiclass Classification,Image",848,386,1365,"<p>Cervical cancer is so easy to prevent if caught in its pre-cancerous stage that every woman should have access to effective, life-saving treatment no matter where they live. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patients’ physiological differences.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/unspecified898.jpg"" alt=""Intel MobileODT"" style=""float: right""></p>
<p>Especially in rural parts of the world, many women at high risk for cervical cancer are receiving treatment that will not work for them due to the position of their cervix. This is a tragedy: health providers are able to identify high risk patients, but may not have the skills to reliably discern which treatment which will prevent cancer in these women. Even worse, applying the wrong treatment has a high cost. A treatment which works effectively for one woman may obscure future cancerous growth in another woman, greatly increasing health risks.</p>
<p>Currently, MobileODT offers a Quality Assurance workflow to support remote supervision which helps healthcare providers make better treatment decisions in rural settings. However, their workflow would be greatly improved given the ability to make real-time determinations about patients’ treatment eligibility based on cervix type.</p>
<p>In this competition, <a href=""https://www.kaggle.com/intel"" target=""_blank"">Intel</a> is partnering with <a href=""http://www.mobileodt.com/"" target=""_blank"">MobileODT</a> to challenge Kagglers to develop an algorithm which accurately identifies a woman’s cervix type based on images. Doing so will prevent ineffectual treatments and allow healthcare providers to give proper referral for cases that require more advanced treatment.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/Icons-Comps.png"" alt=""BAH Lung"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<h3>Competition Partner</h3>
<p><a href=""http://www.mobileodt.com/"" target=""_blank"">MobileODT</a> has developed and sells the Enhanced Visual Assessment (EVA) System, a digital toolkit for health care workers of every level to provide expert services to patients, anchored at the point-of-care by an FDA-approved, intelligent, mobile-phone based medical device. Combining the algorithmic power of biomedical optics with the computational capabilities and connectivity of mobile phones, MobileODT's connected, intelligent medical systems can be used everywhere, under nearly any conditions. MobileODT's first product, the FDA approved EVA System for colposcopy, is in use by health providers in 31 hospital systems across the US, and in 22 countries, to better screen and treat women for cervical cancer and to conduct forensic colposcopy.</p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each image&nbsp;has been labeled with one type. For each image, you must submit a set of predicted probabilities (one for every category). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of images&nbsp;in the test set, M is the number of categories,&nbsp; \\(log\\) is the natural logarithm, (y_{ij}) is 1 if observation (i) belongs to class&nbsp;(j) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given image&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the image&nbsp;id, and a probability for each class.</p>
<p>The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>image_name,Type_1,Type_2,Type_3<br>0.jpg,0.1,0.3,0.6<br>1.jpg,0,0,1<br>...<br><br></pre>"
Google Cloud & YouTube-8M Video Understanding Challenge,Can you produce the best video tag predictions?,https://www.kaggle.com/competitions/youtube8m,https://storage.googleapis.com/kaggle-competitions/kaggle/6049/logos/header.png,"Image,Internet",655,821,6911,"<p>Video captures a cross-section of our society. And major advances in analyzing and understanding video have the potential to touch all aspects of life from learning and communication to entertainment and play. In this competition, Google is inviting the Kaggle community to join efforts to accelerate research in large-scale video understanding, while giving participants access to the <a href=""https://cloud.google.com/ml-engine/"" target=""_blank"">Google Cloud Machine Learning Engine</a>.</p>
<p>Today, one of the greatest obstacles to rapid improvements in video understanding research has been the lack of large-scale, labeled datasets open to the public. For example, the availability of large, labeled datasets such as <a href=""http://www.image-net.org/"" target=""_blank"">ImageNet</a>&nbsp;has enabled continued breakthroughs in machine learning and machine perception. To that end, <a href=""https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html"" target=""_blank"">Google’s recent release of the YouTube-8M (YT-8M) dataset</a>&nbsp;represents a significant step in this direction. Making this resource open to everyone from students and industry professionals is expected to kickstart innovation in areas such as representation learning and video modeling architectures.</p>
<p>In this competition, you are challenged to develop classification algorithms which accurately assign video-level labels using the new and improved YT-8M V2 dataset. The dataset was created from over 7 million YouTube videos (450,000 hours of video) and includes video labels from a vocabulary of 4716 classes (3.4 labels/video on average).  It also comes with pre-extracted audio &amp; visual features from every second of video (3.2B feature vectors in total). By taking part, Kagglers will not only play a pivotal role in setting state-of-the-art benchmarks, but also improve search and organization of video archives.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/Icons-Comps.png"" alt=""BAH Lung"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<h3>Getting Started</h3>
<ol>
<li>Review the <a href=""https://www.kaggle.com/c/youtube8m/data"">data</a>&nbsp;page for special instructions&nbsp;on how&nbsp;to access the competition's data.&nbsp;It will be hosted on Google Cloud. Participants have the option to download the data to work locally or work within the Google Cloud ML beta Platform.</li>
<li>Review the tutorial on <a href=""https://www.kaggle.com/c/youtube8m#getting-started-with-google-cloud"" target=""_blank"">Getting Started with Google Cloud</a>, and try the <a href=""https://github.com/google/youtube-8m/"" target=""_blank"">starter code</a>.
<ul>
<li>Sign up for a <a href=""https://cloud.google.com/ml/"" target=""_blank"">Google Cloud ML Platform free trial account</a>. The free trial account includes $300 in credits!</li>

</ul>
</li>
<li>We've also provided a subsample of the data to explore on Kernels. <a href=""https://www.kaggle.com/wendykan/youtube8m/starter-explore-youtube8m-sample-data/"" target=""_blank"">Take a look at this Python notebook</a> and create your own.</li>
<li>Don't forget to review the <a href=""https://www.kaggle.com/c/youtube8m#prizes"" target=""_blank"">prize eligibility</a> details, which includes requirements for code open-sourcing and a paper submission.</li>
</ol>
<p>Because Cloud ML is currently a beta product, Google&nbsp;welcomes the opportunity to hear your feedback about using the tool. Please share your questions and thoughts on the <a href=""https://www.kaggle.com/c/youtube8m/discussion"" target=""_blank"">competition's forums</a>. Additional resources specific to the YT-8M dataset and Google Cloud ML can be <a href=""https://www.kaggle.com/c/youtube8m#More-help"" target=""_blank"">found here</a>.</p>
<h3>Acknowledgements</h3>
<p><strong>Google Cloud Machine Learning</strong>, Competition Sponsor</p>
<p>Google Cloud Machine Learning is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Create your model with the powerful TensorFlow framework that powers many Google products, from GooglePhotos to Google Cloud Speech. Build models of any size with our managed scalable infrastructure. Your trained model is immediately available for use with our global prediction platform that can support thousands of users and TBs of data. The service is integrated with Google Cloud Dataflow for pre-processing, allowing you to access data from Google Cloud Storage, Google BigQuery, and others.</p>","<p>Submissions are evaluated the Global Average Precision (GAP) at k, where k=20. For each video, you will submit a list of predicted labels and their corresponding confidence scores. The evaluation takes the predicted labels that have the highest k confidence scores for each video, then treats each prediction and the confidence score as an individual data point in a long list of global predictions, to compute the Average Precision across all of the predictions and all the videos.&nbsp;</p>
<p>If a submission has N predictions (label/confidence pairs)&nbsp;sorted by its confidence score, then the Global Average Precision&nbsp;is computed as:</p>
<p>$$GAP = \sum_{i=1}^N p(i) \Delta r(i)$$</p>
<p>where N is the number of final predictions (if there are 20 predictions for each video, then N = 20 * #Videos ), p(i) is the precision, and r(i) is the recall. &nbsp;</p>
<p>A python implementation of GAP can be found at youtube-8m's <a href=""https://github.com/google/youtube-8m/blob/master/average_precision_calculator.py#L179"">github</a>. For other definitions of Average Precision, please refer to Kaggle's metric <a href=""https://www.kaggle.com/wiki/AveragePrecision"">page</a>.</p>
<h2>Submission File</h2>
<p>For each VideoId in the test set, you must predict a list of Labels and their corresponding confidence scores. The file should contain a header and have the following format:</p>
<pre>VideoId,LabelConfidencePairs<br>100000001,1 0.5 2 0.3 3 0.1 4 0.05 5 0.05<br>etc.</pre>"
Two Sigma Connect: Rental Listing Inquiries,How much interest will a new rental listing on RentHop receive?,https://www.kaggle.com/competitions/two-sigma-connect-rental-listing-inquiries,,"Tabular,Housing,Multiclass Classification,Text",2480,2480,46856,"<p>Finding the perfect place to call your new home should be more than browsing through endless listings. <a href=""https://www.renthop.com/"" target=""_blank"">RentHop</a> makes apartment search smarter by using data to sort rental listings by quality. But while looking for the perfect apartment is difficult enough, structuring and making sense of all available real estate data programmatically is even harder. <a href=""https://www.twosigma.com/"">Two Sigma</a> and RentHop, a portfolio company of Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition.
</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5590/media/twosigma-renthop-banner-250x250.png"" alt="""" style=""float: right""></p>
<p><br>Two Sigma invites you to apply your talents in this recruiting competition featuring rental listing data from RentHop. Kagglers will predict the number of inquiries a new listing receives based on the listing’s creation date and other features. Doing so will help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand renters’ needs and preferences.</p>
<p><br>Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. This challenge is an opportunity for competitors to gain a sneak peek into Two Sigma's data science work outside of finance.</p>
<h3>Acknowledgments</h3>
<p>This competition is co-hosted by Two Sigma and RentHop (a portfolio company of Two Sigma Ventures, which is a division of Two Sigma Investments) to encourage creativity in using real world data to solve everyday problems.
<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5590/media/twosigma-renthop.png"" alt=""""></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each listing&nbsp;has one true class. For each listing, you must submit a set of predicted probabilities (one for every listing). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of listings&nbsp;in the test set, M is the number of class labels (3 classes),&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given listing&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the listing_id, and a probability for each class.</p>
<p>The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>listing_id,high,medium,low<br>7065104,0.07743170693194379,0.2300252644876046,0.6925430285804516<br>7089035,0.0, 1.0, 0.0<br>...</pre>"
March Machine Learning Mania 2017,Predict the 2017 NCAA Basketball Tournament,https://www.kaggle.com/competitions/march-machine-learning-mania-2017,,"Basketball,Sports",441,491,771,"<p>Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our fourth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on national television.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt=""""></p>
<p>In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a <a href=""https://www.kaggle.com/datasets/about"">dataset</a>. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2017 tournament. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2017 results.</p>","<p>Submissions are scored on the log loss:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2017 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 &nbsp;= 2278 matchups.&nbsp;</p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104&nbsp;played team 1129&nbsp;in the year 2013. You must&nbsp;predict the probability that the team with the lower id beats the team with the higher id.</p>
<p>The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br>2013_1103_1107,0.5<br>2013_1103_1112,0.5<br>2013_1103_1125,0.5<br>...</pre>"
Data Science Bowl 2017,Can you improve lung cancer detection?,https://www.kaggle.com/competitions/data-science-bowl-2017,,"Image,Healthcare,Binary Classification",1972,742,1676,"<p>In the United States, lung cancer strikes 225,000 people every year, and accounts for $12 billion in health care costs. Early detection is critical to give patients the best chance at recovery and survival.</p>
<p>One year ago, the office of the U.S. Vice President spearheaded a bold new initiative, the Cancer Moonshot, to make a decade's worth of progress in cancer prevention, diagnosis, and treatment in just 5 years.</p>
<p>In 2017, the Data Science Bowl will be a critical milestone in support of the Cancer Moonshot by convening the data science and medical communities to develop lung cancer detection algorithms.</p>
<p>Using a data set of thousands of high-resolution lung scans provided by the National Cancer Institute, participants will develop algorithms that accurately determine when lesions in the lungs are cancerous. This will dramatically reduce the false positive rate that plagues the current detection technology, get patients earlier access to life-saving interventions, and give radiologists more time to spend with their patients.</p>
<p>This year, the Data Science Bowl will award $1 million in prizes to those who observe the right patterns, ask the right questions, and in turn, create unprecedented impact around cancer screening care and prevention. The funds for the prize purse will be provided by the Laura and John Arnold Foundation.</p>
<p>Visit DataScienceBowl.com to: <br>• Sign up to&nbsp;<a href=""http://www.datasciencebowl.com/contact/"">receive news</a> about the competition<br>• Learn about the&nbsp;<a href=""http://www.datasciencebowl.com/competitions/"">history of the Data Science Bowl</a> and past competitions<br>• Read our&nbsp;<a href=""http://www.datasciencebowl.com/data-science-insights/"">latest insights</a> on emerging analytics techniques</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/6004/media/illustration-lung-3.5.jpg"" alt=""BAH Lung"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<h3>Acknowledgments</h3>
<p>The Data Science Bowl is presented by</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/booz_kaggle.png"" alt=""BAH and Kaggle"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<h4>Competition Sponsors</h4>
<p>Laura and John Arnold Foundation<br>Cancer Imaging Program of the National Cancer Institute<br>American College of Radiology<br>Amazon Web Services<br>NVIDIA</p>
<h4>Data Support Providers</h4>
<p>National Lung Screening Trial<br>The Cancer Imaging Archive<br>Diagnostic Image Analysis Group, Radboud University<br>Lahey Hospital &amp; Medical Center<br>Copenhagen University Hospital</p>
<h4>Supporting Organizations&nbsp;</h4>
<p>Bayes Impact<br>Black Data Processng Associates<br>Code the Change<br>Data Community DC<br>DataKind<br>Galvanize<br>Great Minds in STEM<br>Hortonworks<br>INFORMS<br>Lesbians Who Tech<br>NSBE<br>Society of Asian Scientists &amp; Engineers<br>Society of Women Engineers<br>University of Texas Austin, Business Analytics Program,<br>McCombs School of Business<br>US Dept. of Health and Human Services<br>US Food and Drug Administration<br>Women in Technology<br>Women of Cyberjutsu</p>","<p>Submissions are scored on the log loss:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of patients&nbsp;in the test set</li>
<li>\\( \hat{y}_i \\) is the predicted probability of the image belonging to a patient with cancer</li>
<li>\\( y_i \\) is 1 if the diagnosis is cancer, 0 otherwise</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>Note: the actual submitted predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\). A smaller log loss is better.</p>
<h2>Submission File</h2>
<p>For each patient id&nbsp;in the test set, you must submit a probability. The file should have a header and be in the following format:</p>
<pre>id,cancer<br>01e349d34c02410e1da273add27be25c,0.5<br>05a20caf6ab6df4643644c923f06a5eb,0.5<br>0d12f1c627df49eb223771c28548350e,0.5<br>...</pre>"
Santa's Uncertain Bags,"♫ Bells are ringing, children singing, all is merry and bright. Santa's elves made a big mistake, now he needs your help tonight ♫",https://www.kaggle.com/competitions/santas-uncertain-bags,,Tabular,692,743,18383,"<p>All was well in Santa's workshop. The gifts were made, the route was planned, the naughty and nice list complete. Santa thought this would finally be the year he didn't need Kaggle's help with his combinatorial conundrums. At last, the Claus family could take the elves and reindeer on that&nbsp;well deserved vacation to the South Pole.</p>
<p>Then, with just days until the big night, Santa received an email from a panicked database admin elf. Attached was a server log with the&nbsp;six least&nbsp;jolly words a jolly old St. Nick could read:</p>
<pre>ALTER TABLE&nbsp;Gifts<br>DROP COLUMN&nbsp;Weight</pre>
<p>One of the North Pole elf&nbsp;interns had mistakenly deleted the weights for all of the inventory&nbsp;in the workshop! Santa didn't have a backup (remember, this is a guy who&nbsp;makes <strong>a</strong> list and checks it twice) and, without knowing each present's weight, he&nbsp;didn't know how he would safely pack his&nbsp;many gift bags. Gifts were already on their&nbsp;way to the sleigh packing facility and there&nbsp;wasn't time to re-weigh all the&nbsp;presents. It was once again necessary to summon the holiday talents of&nbsp;Kaggle's elite.</p>
<p>Can you help Santa fill his multiple bags with sets of uncertain gifts? Save the season by turning Santa's uncertain&nbsp;probabilities into presents for good little boys and girls.</p>
","<p>Submissions are evaluated on the total amount of weight you fit into Santa's 1000 bags. The rules that govern gift packing are as follows:</p>
<ul>
<li>Overfilling a bag above the 50lb limit will cause the entire bag&nbsp;to count for nothing, without warning!</li>
<li>No gift may be used more than once.</li>
<li>Every bag&nbsp;must have 3 or more gifts.</li>
</ul>
<h2>Submission File</h2>
<p>For each bag, you must provide&nbsp;a space-delimited list of GiftIds. Each line represents one bag. The file should contain a header and have the following format:</p>
<pre>Gifts<br>horse_0 book_345 gloves_48<br>doll_873 train_714 coal23 bike_85<br>etc.</pre>"
Dstl Satellite Imagery Feature Detection,Can you train an eye in the sky?,https://www.kaggle.com/competitions/dstl-satellite-imagery-feature-detection,,"Image,Multiclass Classification",419,519,5542,"<p>The proliferation of satellite imagery has given us a radically improved understanding of our planet. It has enabled us to better achieve everything from mobilizing resources during disasters to monitoring effects of global warming. What is often taken for granted is that advancements such as these have relied on labeling features&nbsp;of significance like building footprints and roadways fully by hand or through imperfect semi-automated methods.<br><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5916/media/_competition_dstl.png"" alt=""""></p>
<p>As these large, complex datasets continue to increase exponentially in number, the <a href=""https://www.gov.uk/government/organisations/defence-science-and-technology-laboratory"" target=""_blank"">Defence Science and Technology Laboratory (Dstl)</a>&nbsp;is seeking novel solutions to alleviate the burden on their image analysts. In this competition, Kagglers are challenged to accurately classify features&nbsp;in overhead imagery. Automating feature&nbsp;labeling will not only help Dstl make smart decisions more quickly around the defense and security of the UK, but also bring innovation to computer vision methodologies applied to satellite imagery.</p>","<p>Submissions are evaluated on <strong>Average Jaccard Index</strong> between the predicted multipolygons and the actual multipolygons. This is a vector-based metric where we use polygon geometries to evaluate&nbsp;how well your predictions are aligned with the answer.&nbsp;</p>
<p>The <a href=""https://en.wikipedia.org/wiki/Jaccard_index"">Jaccard Index</a>&nbsp;for two regions A and B, also known as the ""intersection over union"",&nbsp;is defined as:</p>
<p>$$Jaccard = {\frac{TP}{TP+FP+FN} }= {{|A \cap B|}\over{|A \cup B|}} = {{|A \cap B|}\over{|A| + |B| - |A \cap B|}}$$<a href=""https://commons.wikimedia.org/wiki/File:Intersection_of_sets_A_and_B.svg#/media/File:Intersection_of_sets_A_and_B.svg""><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Intersection_of_sets_A_and_B.svg/1200px-Intersection_of_sets_A_and_B.svg.png"" alt=""Intersection of sets A and B.svg"" style=""display: block; margin-left: auto; margin-right: auto""></a></p>
<p>where TP is the true positives area, FP is the false positives area, and FN is the false negatives area.&nbsp;</p>
<p>For each object class, of each image, we calculate the TP, FP, and FN areas.&nbsp;We then sum the total TP, total FP, and total FN across all the images, then the Jaccard is calculated for that class using total TP, total FP, and total FN. Then, we average all the Jaccard Indexes for all the 10 classes.&nbsp;</p>
<h2>Submission File</h2>
<p>For every row&nbsp;in the dataset, submission files should contain the following: ImageId (string), ClassType (int), and MultipolygonWKT (multipolygon coordinates in WKT format). You must submit a row for every&nbsp;ImageId and every&nbsp;ClassType. The complete format is in the sample_submission.csv file. If you want to predict an empty polygon for the class, put down MULTIPOLYGON EMPTY. Please submit all the rows in exactly the same order as sample_submission.csv, otherwise you might get a ""key not found error"".&nbsp;</p>
<p>The file should contain a header and have the following format:</p>
<pre>ImageId,ClassType,MultipolygonWKT<br>6020_0_1, 1, MULTIPOLYGON EMPTY<br>6120_2_4,1,""POLYGON ((0 0, 0.009188 0, 0.009188 -0.009039999999999999, 0 -0.009039999999999999, 0 0))""<br>etc.</pre>
<h2>Submission Time</h2>
<p>If you have complex polygons, it could take a longer time (4+ mins) to evaluate your submission. Please be patient when the submission is getting evaluated! If you find yourself getting timed out, please consider simplifying (smoothing) your polygons to reduce both the submission file size and the time it takes to evaluate the submission.&nbsp;</p>"
Two Sigma Financial Modeling Challenge,Can you uncover predictive value in an uncertain world?,https://www.kaggle.com/competitions/two-sigma-financial-modeling,https://storage.googleapis.com/kaggle-competitions/kaggle/5484/logos/header.png,Finance,2063,2317,24573,"<p>How can we use the world’s tools and intelligence to forecast economic outcomes that can never be entirely predictable? This question is at the core of countless economic activities around the world – including at <a href=""https://www.kaggle.com/two-sigma"" target=""_blank"">Two Sigma Investments</a>, who&nbsp;has been applying technology and systematic strategies to financial trading since 2001.</p>
<p>For over 15 years, Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. Through this exclusive partnership, Two Sigma is excited to explore what untapped value Kaggle's diverse&nbsp;data science community can discover in the financial markets.</p>
<p>Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty. By accurately predicting financial movements, Kagglers will learn about scientifically-driven approaches to&nbsp;unlocking&nbsp;significant predictive capability.&nbsp;Two Sigma is excited to find predictive value and gain a better understanding of the skills&nbsp;offered by the global data science crowd.</p>
<h2>What is a Code Competition?</h2>
<p>Welcome to Kaggle's very first Code Competition! In contrast to our traditional competitions, where competitors submit only prediction outputs, participants in&nbsp;Code Competitions&nbsp;will submit their code via <a href=""https://www.kaggle.com/c/two-sigma-financial-modeling/kernels"">Kaggle Kernels</a>. All kernels are private by default in Code Competitions. You&nbsp;can&nbsp;build your models in Kernels by running them on a training set and, once&nbsp;you're ready to submit your code, your&nbsp;model's performance will be evaluated against the test set and your score and public leaderboard position revealed. As with our traditional competitions, we&nbsp;still maintain a private leaderboard test set, which your code is also evaluated against for final scoring, but is&nbsp;not&nbsp;revealed until the competition closes.</p>
<p>Since Code Competitions are brand new, we ask for your patience if you encounter bugs or frustrating platform quirks. Please report any issues you find in the forums and we'll do our best to respond.</p>
<h2>Who owns my code?</h2>
<p>You do. Even though you are submitting code, the intellectual property exchange here works similarly to a standard prediction competition, whereby prize winners have the option to grant a non-exclusive license in exchange for a prize. There is a new addition to the terms for Code Competitions: Kaggle and the competition host reserve a right to review submissions ""for purposes related to evaluation and scoring in this Competition, including but not limited to the assessment of potential cheating behavior."" Please refer to the official competition rules for full details.</p>
<h2>Getting Started</h2>
<ol>
<li>Review the <a href=""https://www.kaggle.com/c/two-sigma-financial-modeling/data"">data page</a>&nbsp;for details about the data and the evaluation metric. You may&nbsp;download&nbsp;the train set for local training.</li>
<li>Take a look at the tutorial covering the&nbsp;new code submission process under&nbsp;the <a href=""https://www.kaggle.com/c/two-sigma-financial-modeling#submission-instructions"">submission instructions tab</a>. You'll find step-by-step instructions,&nbsp;some helpful pointers, plus details on environment constraints.</li>
<li>Get feedback on your&nbsp;benchmark code and share exploratory analyses with the community by making any of your kernels public.</li>
<li>Improve your score!</li>
</ol>
<p>Note: there is no cost of entry for participation.</p>","<p>Submissions will be evaluated on the R value between the predicted and actual values. The R value similar to the R squared value, also called the <a href=""https://en.wikipedia.org/wiki/Coefficient_of_determination"">coefficient of determination</a>. R squared can be calculated as:</p>
<p>$$R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \mu)^2}.$$</p>
<p>To calculate R, we then use:</p>
<p>$$R = sign\left( R^2 \right) \sqrt{\left|R^2\right|},$$</p>
<p>where \\(y\\) is the actual value, \\(\mu\\) is the mean of the actual values, and \\(\hat{y}\\) is the predicted value. Do not be discouraged by low R values; in finance, given the high ratio of signal-to-noise, even a small R can deliver meaningful value!</p>
<p><strong>Negative R values are clipped at -1, i.e. the score you see will be \\(max(-1,R)\\).</strong> Additionally, if a submission errors for any reason, you will receive a simple ""Error"" status.</p>"
The Nature Conservancy Fisheries Monitoring,Can you detect and classify species of fish?,https://www.kaggle.com/competitions/the-nature-conservancy-fisheries-monitoring,,"Image,Multiclass Classification",2293,554,2059,"<p>Nearly half of the world depends on seafood for their main source of protein. In the Western and Central Pacific, where 60% of the world’s tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods.&nbsp;<a href=""http://www.conserveca.org/tuna"" target=""_blank"">The Nature Conservancy</a>&nbsp;is working with local, regional and global partners to preserve this fishery for the future.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5568/media/TNC-FF-landing-page-banner.jpg"" alt=""The Nature Conservancy Competition"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Currently, the Conservancy is looking to the future by using cameras to dramatically scale the monitoring of fishing activities to fill critical science and compliance monitoring data gaps. Although these electronic monitoring systems work well and are ready for wider deployment, the amount of raw data produced is cumbersome and expensive to process manually.</p>
<p>The Conservancy is inviting the Kaggle community to develop algorithms to automatically detect and classify species of tunas, sharks and more&nbsp;that fishing boats catch, which will accelerate the video review process. Faster review and more reliable data will enable countries to reallocate human capital to management and enforcement activities which will have a positive impact on conservation and our planet.</p>
<p>Machine learning has the ability to transform what we know about our oceans and how we manage them. You can be part of the solution.</p>
<h2>Resources</h2>
<p>You can learn more about this competition and <a href=""http://www.thisisourfuture.org"" target=""_blank"">The Nature Conservancy</a>&nbsp;in the video below.</p>
<p></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each image&nbsp;has been labeled with one true class. For each image, you must submit a set of predicted probabilities (one for every image). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of images&nbsp;in the test set, M is the number of image&nbsp;class labels,&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given image&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the image file name, and a probability for each class.</p>
<p>The 8&nbsp;classes to predict are:&nbsp;'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK','YFT'</p>
<p>The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT<br>img_00001.jpg,1,0,0,0,0,...,0<br>img_00002.jpg,0.3,0.1,0.6,0,...,0<br>...</pre>"
"Ghouls, Goblins, and Ghosts... Boo! ",Can you classify monsters haunting Kaggle?,https://www.kaggle.com/competitions/ghouls-goblins-and-ghosts-boo,,"Tabular,Multiclass Classification",763,802,7522,"<p>Get out your dowsing rods, electromagnetic sensors, … and gradient boosting machines. Kaggle is haunted and we need your help. After a month of making scientific observations and taking careful measurements, we’ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our data scientists. When&nbsp;trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing&nbsp;our unwanted guests.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5708/media/halloween-660x.png"" alt=""""></p>
<p>So now the hour has come to put the data we’ve collected in your hands. We’ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?</p>","<p>Submissions are evaluated on the categorization accuracy (the percent of creatures&nbsp;that you correctly classify).</p>
<h2>Submission File</h2>
<p>Your submission file should predict the type&nbsp;for each creature&nbsp;in the test set. The file should contain a header and have the following format:</p>
<pre>id,type<br>0,Ghost<br>1,Goblin<br>2,Ghoul<br>etc.</pre>"
Transfer Learning on Stack Exchange Tags,Predict tags from models trained on unrelated topics,https://www.kaggle.com/competitions/transfer-learning-on-stack-exchange-tags,,"Multiclass Classification,Text,Tabular",380,600,3514,"<p>What does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered ""all pursuits are governed by the immutable laws of physics"" we'll begrudgingly give you partial&nbsp;credit. If you answered ""all were chosen randomly by a scheming&nbsp;Kaggle employee&nbsp;for a twisted&nbsp;transfer learning competition"", congratulations, we accept your answer and mark the question as solved.</p>
<p>In this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via&nbsp;a&nbsp;standard machine approach might&nbsp;involve training an algorithm on a corpus of related text. Here, you are challenged to train on material&nbsp;from outside the field. Can an&nbsp;algorithm learn appropriate physics tags from ""extreme-tourism Antarctica""? Let's find out.</p>
<p>Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the <a href=""https://archive.org/details/stackexchange"">Stack Exchange data dump</a>.</p>","<p>The evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/MeanFScore""> Mean F1-Score</a>. The F1 score measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:</p>
<p>\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]</p>
<p>The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.</p>
<h2>Submission File</h2>
<p>For every question&nbsp;in the dataset, submission files should contain two columns: id&nbsp;and tags. The predicted tags&nbsp;should be a space-delimited list. The file must have a header and should look like the following:</p>
<pre>id,tags<br>1,physics poetry<br>2,physics poetry chemistry<br>3,physics electrons<br>etc.</pre>"
Santander Product Recommendation,Can you pair products with people?,https://www.kaggle.com/competitions/santander-product-recommendation,,"Banking,Multiclass Classification,Tabular",1779,2051,28732,"<p>Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of&nbsp;financial decisions, <a href=""https://www.santanderbank.com/us/personal"" target=""_blank"">Santander Bank</a>&nbsp;offers a lending hand to their customers through personalized product recommendations.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5558/media/santander-banner-ts-660x.png"" alt=""""></p>
<p>Under their current system, a small number of Santander’s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers.</p>
<p>With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life.</p>
<p><strong>Disclaimer:</strong>&nbsp;This data set&nbsp;does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base.&nbsp;</p>","<p>Submissions are evaluated according to the Mean Average Precision @ 7 (MAP@7):</p>
<p>$$MAP@7 = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{min(m, 7)} \sum_{k=1}^{min(n,7)} P(k)$$</p>
<p>where |U| is the number of rows (users in two time points), P(k) is the precision at cutoff k, n is the number of predicted products, and m is the number of added&nbsp;products&nbsp;for the given user at that time point. If m = 0, the precision is defined to be 0.</p>
<h2>Submission File</h2>
<p>For every user at each time point, you must predict a space-delimited list of the products&nbsp;they added. The file should contain a header and have the following format:</p>
<pre>ncodpers,added_products<br>15889,ind_tjcr_fin_ult1<br>15890,ind_tjcr_fin_ult1 ind_recibo_ult1<br>15892,ind_nomina_ult1<br>15893,<br>etc.</pre>"
Allstate Claims Severity,How severe is an insurance claim?,https://www.kaggle.com/competitions/allstate-claims-severity,,"Regression,Tabular",3045,3045,52353,"<p>When you’ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why <a href=""https://www.allstate.com/"" target=""_blank"">Allstate</a>, a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5325/media/allstate_banner-660x120.png"" alt=""""></p>
<p>Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating&nbsp;an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate&nbsp;insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience.</p>
<p>New to Kaggle? This competition is a recruiting competition, your chance to get a foot in the door with the hiring team at Allstate.</p>","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/MeanAbsoluteError"">mean absolute error (MAE)</a>&nbsp;between the predicted loss and the actual loss.</p>
<h2>Submission File</h2>
<p>For every id in the test set, you should predict the loss value. The file should contain a header and have the following format:</p>
<pre>id,loss<br>4,0<br>6,1<br>9,99.3<br>etc.</pre>"
Outbrain Click Prediction,Can you predict which recommended content each user will click?,https://www.kaggle.com/competitions/outbrain-click-prediction,,"Tabular,Internet",978,1301,6642,"<p>The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. <a href=""http://www.outbrain.com/"" target=""_blank"">Outbrain</a>, the web’s leading content discovery platform, delivers these moments while we surf our favorite sites.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5497/media/outbrain-660x180.png"" alt=""Outbrain click prediction competition""></p>
<p>Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain’s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes.</p>","<p>Submissions are evaluated according to the <a href=""https://www.kaggle.com/wiki/MeanAveragePrecision"">Mean Average Precision @12&nbsp;</a>&nbsp;(MAP@12):</p>
<p>$$MAP@12 = \frac{1}{|U|} \sum_{u=1}^{|U|} \sum_{k=1}^{min(12, n)} P(k)$$&nbsp;</p>
<p>where |U| is the number of display_ids, P(k) is the precision at cutoff k, n is the number of predicted ad_ids.</p>
<h2>Submission File</h2>
<p>For each display_id&nbsp;in the test set, you must predict a space-delimited list of&nbsp;ad_ids,&nbsp;ordered by decreasing likelihood of&nbsp;being&nbsp;clicked. The candidate ad_ids for each display_id are provided in <strong>clicks_test.csv</strong>. Note that each display_id can have a different number of associated ads. The file should contain a header and have the following format:</p>
<pre>display_id,ad_id<br>16874594,66758 150083 162754 170392 172888 180797<br>16874595,8846 30609 143982<br>16874596,11430 57197 132820 153260 173005 288385 289122 289915<br>etc.</pre>"
Dogs vs. Cats Redux: Kernels Edition,Distinguish images of dogs from cats,https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition,,"Image,Animals,Binary Classification",1314,1334,9449,"<p>In 2013, we hosted one of our favorite for-fun competitions: &nbsp;<a href=""https://www.kaggle.com/c/dogs-vs-cats"">Dogs vs. Cats</a>. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a&nbsp;tensor flow was the&nbsp;diffusion of the creamer in a bored mathematician's cup of coffee. Now, <a href=""https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow"">even the cucumber farmers</a>&nbsp;are neural netting their way to a bounty.</p>
<p>Much has changed at Kaggle as well. Our online coding environment <a href=""http://blog.kaggle.com/category/kernels/"">Kernels</a>&nbsp;didn't exist in 2013, and so it was that we approached sharing&nbsp;by scratching primitive glpyhs&nbsp;on cave walls with sticks and sharp objects.&nbsp;No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and <a href=""https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/kernels/notebooks/new"">Jupyter Notebook</a>&nbsp;is in. We even <a href=""https://github.com/Kaggle/docker-python/blob/76efa2051a7835311f369cedc25193f729b57a2e/Dockerfile#L37"">have TensorFlow</a>.&nbsp;What more could a data scientist ask for? But seriously, what more? Pull requests welcome.</p>
<p><img style=""display: block; margin: auto"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3362/media/woof_meow.jpg"" alt=""Woof Meow""></p>
<p>We are excited to bring back the infamous Dogs vs. Cats classification problem&nbsp;as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult&nbsp;problem, it is through practice of&nbsp;new techniques on old datasets that we will make light of machine learning's future challenges.</p>","<p>Submissions are scored on the log loss:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of&nbsp;images in the test set</li>
<li>\\( \hat{y}_i \\) is the predicted probability of the image being a dog</li>
<li>\\( y_i \\) is 1 if the image is a dog, 0 if cat</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p></p>
<h2>Submission File</h2>
<p>For each image in the test set, you must submit a probability that image is a dog. The file should have a header and be in the following format:</p>
<pre>id,label<br>1,0.5<br>2,0.5<br>3,0.5<br>...</pre>"
Melbourne University AES/MathWorks/NIH Seizure Prediction,Predict seizures in long-term human intracranial EEG recordings ,https://www.kaggle.com/competitions/melbourne-university-seizure-prediction,,"Healthcare,Diseases",477,645,10047,"<p>Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.</p>
<p>Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/Seizure%20Prediction%20Graphic.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<h3>The Competition</h3>
<p>Transitioning from the Kaggle contests held on seizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contest focuses on seizure prediction using long-term electrical brain activity recordings from humans obtained from the world-first clinical trial of the implantable NeuroVista Seizure Advisory System.</p>
<p>Human brain activity was recorded in the form of intracranial EEG (iEEG), which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system.&nbsp;These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans.&nbsp;The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity.&nbsp;</p>
<h3>Acknowledgments&nbsp;</h3>
<p>This competition is sponsored by MathWorks, the National Institutes of Health (NINDS), the American Epilepsy Society and the University of Melbourne, and organised in partnership with the Alliance for Epilepsy Research, the University of Pennsylvania and the Mayo Clinic.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/AES.png"" alt=""""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/Mathwork.png"" alt="""">&nbsp;<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/NINDS.png"" alt=""""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/UMel.png"" alt="""">&nbsp;&nbsp;&nbsp;<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/Mayo.png"" alt=""""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/AFER.png"" alt="""">&nbsp;<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5390/media/UPenn.png"" alt=""UPenn""></p>
<h3>References</h3>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each File&nbsp;in the test set, you must predict a probability for the Class&nbsp;variable. A probability of 1 indicates a prediction of the preictal class.&nbsp;The file should contain a header and have the following format:</p>
<pre>File,Class<br>1_1.mat,0.1<br>1_2.mat,0.00<br>1_3.mat,1<br>etc.</pre>"
House Prices - Advanced Regression Techniques,"Predict sales prices and practice feature engineering, RFs, and gradient boosting",https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques,https://storage.googleapis.com/kaggle-competitions/kaggle/5407/logos/header.png?t=2021-04-23-18-16-53,"Regression,Tabular",4619,4872,25373,"<h3>Start here if...</h3>
<p>You have some experience&nbsp;with R or Python and&nbsp;machine learning basics. This is a perfect competition for data science students&nbsp;who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.&nbsp;</p>
<h3>Competition Description</h3>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.</p>
<p>With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.</p>
<h3>Practice Skills</h3>
<ul>
<li>Creative feature engineering&nbsp;</li>
<li>Advanced regression techniques like random forest and gradient boosting</li>
</ul>
<h2>Acknowledgments</h2>
<p>The <a href=""http://www.amstat.org/publications/jse/v19n3/decock.pdf"" target=""_blank"">Ames Housing&nbsp;dataset</a>&nbsp;was compiled&nbsp;by Dean De Cock for use in data science education. It's an incredible alternative&nbsp;for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.&nbsp;</p>

<p>Photo by <a href=""https://unsplash.com/@tthfilms"">Tom Thain</a> on Unsplash.</p>","<h3>Goal</h3>
<p>It is your job to predict the sales price for each house.&nbsp;For each Id in the test set, you must predict the value of the SalePrice variable.&nbsp;</p>
<h3>Metric</h3>
<p>Submissions are evaluated on <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" target=""_blank"">Root-Mean-Squared-Error (RMSE)</a>&nbsp;between the logarithm of the predicted value&nbsp;and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)</p>
<h3>Submission File Format</h3>
<p>The file should contain a header and have the following format:</p>
<pre>Id,SalePrice<br>1461,169000.1<br>1462,187724.1233<br>1463,175221<br>etc.</pre>
<p>You can download an example submission file (sample_submission.csv) on the&nbsp;<a href=""https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"" target=""_blank"">Data page.</a></p>"
Leaf Classification,Can you see the random forest for the leaves?,https://www.kaggle.com/competitions/leaf-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/5408/logos/header.png,"Image,Multiclass Classification",1595,1684,13060,"<p>There are estimated to be nearly half a million species of plant in the world. Classification of species has been historically problematic and often results in duplicate identifications.
</p><p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5408/media/bigleaves.jpg"" alt=""Leaf Classification""></p>
<p>The objective of this playground competition is to use binary leaf images and extracted&nbsp;features, including shape, margin &amp; texture, to accurately identify 99&nbsp;species of plants. Leaves, due to their volume, prevalence, and unique characteristics, are an effective means of differentiating plant species. They also provide a fun introduction to applying techniques that involve image-based features.</p>
<p>As a first step, try building a classifier that uses the provided pre-extracted features. Next, try creating a set of your own features. Finally, examine the errors you're making and see what you can do to improve.</p>
<h3>Acknowledgments</h3>
<p>Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from leaf images collected by &nbsp;<br>James Cope, Thibaut Beghin, Paolo Remagnino, &amp; Sarah Barman of the Royal Botanic Gardens, Kew, UK.</p>
<p>Charles Mallah, James Cope, James Orwell. <em>Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features</em>. Signal Processing, Pattern Recognition and Applications, in press. 2013.</p>
<p>We thank the UCI machine learning repository for hosting the dataset.</p>","<p>Submissions are evaluated using the multi-class logarithmic loss. Each image&nbsp;has been labeled with one true species. For each image, you must submit a set of predicted probabilities (one for every species).&nbsp;The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of images&nbsp;in the test set, M is the number of species&nbsp;labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).</p>
<p>The submitted probabilities for a given device are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum), but they need to be in the range of [0, 1]. In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the image&nbsp;id, all candidate species&nbsp;names, and a probability for each species. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>id,Acer_Capillipes,Acer_Circinatum,Acer_Mono,...<br>2,0.1,0.5,0,0.2,...<br>5,0,0.3,0,0.4,...<br>6,0,0,0,0.7,...<br>etc.</pre>"
Bosch Production Line Performance,Reduce manufacturing failures,https://www.kaggle.com/competitions/bosch-production-line-performance,,"Manufacturing,Tabular,Binary Classification",1370,1599,26126,"<p>A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. <a href=""http://www.bosch.com/en/com/home/index.php"" target=""_blank"">Bosch</a>, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts&nbsp;as they progress through the manufacturing processes.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5357/media/BoschManufacturingKaggleImage.jpg"" alt=""Bosch production line""></p>
<p>Because&nbsp;Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the&nbsp;intricacies of the data and complexities of the production line pose problems for current methods.</p>
<p>In this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.</p>","<p>Submissions are evaluated on the <a href=""https://en.wikipedia.org/wiki/Matthews_correlation_coefficient"">Matthews correlation coefficient</a>&nbsp;(MCC)&nbsp;between the predicted and the observed response. The MCC is given by:</p>
<p>$$&nbsp;MCC = \frac{(TP*TN) - (FP * FN)}{\sqrt{(TP+FP)(TP+FN)(TN + FP)(TN+FN)}},&nbsp;$$</p>
<p>where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.</p>
<h2>Submission File</h2>
<p>For each Id&nbsp;in the test set, you must predict a binary prediction&nbsp;for the Response&nbsp;variable. The file should contain a header and have the following format:</p>
<pre>Id,Response<br>1,0<br>2,1<br>3,0<br>etc.</pre>"
Predicting Red Hat Business Value,Classify customer potential,https://www.kaggle.com/competitions/predicting-red-hat-business-value,,"Tabular,Business",2260,2398,33554,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5261/media/kaggle-image_072516_1269x351.jpg"" alt=""""></p>
<p>Like most companies, Red Hat is able to gather a great deal of information over time about the behavior of individuals who interact with them. They’re in search of better methods of using this behavioral data to predict which individuals they should approach—and even when and how to approach them.</p>
<p>In this competition, Kagglers are challenged to create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities.</p>
<p>With an improved prediction model in place, Red Hat will be able to more efficiently prioritize resources to generate more business and better serve their customers.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5261/media/Red_Hat_Kaggle_Co-brand_rgb_logo.png"" alt=""Red Hat"" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted&nbsp;and the observed outcome.</p>
<h2>Submission File</h2>
<p>For each activity_id in the test set, you must predict a probability for the 'outcome'&nbsp;variable, represented by a number between 0 and 1. The file should contain a header and have the following format:</p>
<pre>activity_id,outcome<br>act1_1,0<br>act1_100006,0<br>act1_100050,0<br>etc.</pre>"
TalkingData Mobile User Demographics,Get to know millions of mobile device users,https://www.kaggle.com/competitions/talkingdata-mobile-user-demographics,,"Multiclass Classification,Demographics,Mobile and Wireless,Tabular",1680,1952,24460,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5340/media/TalkingData_banner-265x200.png"" alt="""" style=""float: right""></p>
<p>Nothing is more comforting than being greeted by your favorite drink just as you walk through the door of the corner café. While a thoughtful barista knows you take a macchiato every Wednesday morning at 8:15, it’s much more difficult in a digital space for your preferred brands to personalize your experience.</p>
<p><a href=""https://www.talkingdata.com/"" target=""_blank"">TalkingData</a>, China’s largest third-party mobile data platform, understands that everyday choices and behaviors paint a picture of who we are and what we value. Currently, TalkingData is seeking to leverage behavioral data from more than 70% of the 500 million mobile devices active daily in China to help its clients better understand and interact with their audiences.</p>
<p>In this competition, Kagglers are challenged to build a model predicting users’ demographic characteristics based on their app usage, geolocation, and mobile device properties. Doing so will help millions of developers and brand advertisers around the world pursue&nbsp;data-driven marketing efforts which are relevant to their users and catered to their preferences.</p>
<h2>Acknowledgements</h2>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5340/media/turi.jpg"" alt=""Turi"" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each device&nbsp;has been labeled with one true class. For each device, you must submit a set of predicted probabilities (one for each class). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of devices&nbsp;in the test set, M is the number of class labels,&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if device&nbsp;\\(i\\) belongs to class&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given device&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum), but they need to be in the range of [0, 1]. In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the device id, and a probability for each class.</p>
<p>The 12 classes to predict are:</p>
<p>'F23-', 'F24-26','F27-28','F29-32', 'F33-42', 'F43+', <br> 'M22-', 'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+'</p>
<p>The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+<br>1234,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833<br>5678,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833<br>...</pre>"
Grupo Bimbo Inventory Demand,Maximize sales and minimize returns of bakery goods,https://www.kaggle.com/competitions/grupo-bimbo-inventory-demand,,"Tabular,Food",1963,2263,21739,"<p>Planning a celebration is a balancing&nbsp;act of preparing just enough food to go around&nbsp;without being stuck eating&nbsp;the same leftovers for the next week. The key is anticipating how many guests will come. Grupo Bimbo must weigh similar considerations as it strives to meet daily consumer demand for fresh bakery products on the shelves of over 1 million stores along its 45,000 routes across Mexico.</p>
<p><img alt="""" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5260/media/grupobimbo_660x_crop.png""></p>
<p>Currently, daily inventory calculations are performed by direct delivery sales employees who must single-handedly predict the forces of supply, demand, and hunger based on their personal experiences with each store. With some breads carrying a one week shelf life, the acceptable margin for error is small.</p>
<p>In this competition, Grupo Bimbo invites&nbsp;Kagglers to develop a model to accurately forecast inventory demand based on historical sales data. Doing so will make sure consumers of its over 100 bakery products aren’t staring at empty shelves, while also reducing the amount spent on refunds to store owners with surplus product unfit for sale.</p>","<p>The&nbsp;evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError"">Root Mean Squared Logarithmic Error</a>.</p>
<p>The RMSLE is calculated as</p>
<p>$$<br>\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }<br>$$</p>
<p>Where:</p>
<p>\\(\epsilon\\) is the RMSLE value (score)<br>\\(n\\) is the total number of observations in the (public/private) data set,<br>\\(p_i\\) is your prediction of demand, and<br>\\(a_i\\) is the actual demand&nbsp;for \\(i\\). <br>\\(\log(x)\\) is the natural logarithm of \\(x\\)</p>
<h2>Submission File</h2>
<p><strong>For every row&nbsp;in the dataset</strong>, submission files should contain two columns: id&nbsp;and Demanda_uni_equi. &nbsp;The id corresponds to the column of that id&nbsp;in the test.csv. The file should contain a header and have the following format:</p>
<pre>id,Demanda_uni_equil<br>0,1<br>1,0<br>2,500<br>3,100<br>etc.</pre>"
Integer Sequence Learning,"1, 2, 3, 4, 5, 7?!",https://www.kaggle.com/competitions/integer-sequence-learning,,Tabular,284,308,1368,"<p>7. You read that correctly. That's the start to a real integer sequence, the <a href=""https://oeis.org/A000961"">powers of primes</a>. Want something easier? How about the next number in&nbsp;0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55? If you answered 89, you may enjoy this challenge. Your computer may find it considerably&nbsp;less enjoyable.</p>
<p>The <a href=""https://oeis.org/"">On-Line Encyclopedia of Integer Sequences</a>&nbsp;is a 50+ year effort by mathematicians the world over to catalog sequences of integers. If it has a pattern, it's probably in the OEIS, and probably described with amazing detail. This competition&nbsp;challenges you create a machine learning algorithm capable of guessing&nbsp;the next number in an integer sequence. While this sounds like pattern recognition&nbsp;in its most basic form, a quick look at the data will&nbsp;convince you this is anything&nbsp;but&nbsp;basic!</p>
<h3>Acknowledgments</h3>
<p>Kaggle is hosting this competition for the data science community to use for fun and education. We thank the OEIS and its contributors for cataloging this data.</p>","<p>This competition is evaluated on accuracy of your predictions (the percentage of sequences where you predict the next number correctly).</p>
<h2>Submission File</h2>
<p>For every sequence Id in the test set, you should predict the next integer. The file should contain a header and have the following format:</p>
<pre>Id,Last<br>1,0<br>2,22<br>4,9378<br>etc.</pre>"
Ultrasound Nerve Segmentation,Identify nerve structures in ultrasound images of the neck,https://www.kaggle.com/competitions/ultrasound-nerve-segmentation,https://storage.googleapis.com/kaggle-competitions/kaggle/5144/logos/header.png?t=2020-10-24-03-08-54,"Image,Healthcare",922,1022,14316,"<p>Even the bravest patient cringes at the mention of a surgical procedure. Surgery inevitably brings discomfort, and oftentimes involves significant post-surgical&nbsp;pain.&nbsp;Currently, patient&nbsp;pain is frequently managed through the use of narcotics that bring a bevy of unwanted side effects.</p>
<p>This competition's sponsor&nbsp;is working to improve pain management through the use of indwelling catheters that block or mitigate pain at the source. Pain management catheters reduce dependence on narcotics and speed up patient recovery.</p>
<p>Accurately identifying nerve structures in ultrasound images is a critical step in effectively inserting a patient’s pain management catheter. In this competition, Kagglers are challenged to build a model that can identify nerve structures in&nbsp;a dataset of ultrasound images of the neck. Doing so would improve catheter placement and contribute to a more pain free future.&nbsp;</p>","<p>This competition is evaluated on the mean <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"">Dice coefficient</a>. The Dice coefficient can be used to compare the pixel-wise&nbsp;agreement&nbsp;between a predicted segmentation&nbsp;and its corresponding ground truth. The formula is given by:</p>
<p>$$&nbsp;\frac{2 * |X \cap Y|}{|X| + |Y|},$$</p>
<p>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.</p>
<h2>Submission File</h2>
<p><strong>In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.</strong>&nbsp; Instead of submitting an exhaustive list of indices for your&nbsp;segmentation, you&nbsp;will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and&nbsp;running a total of 3 pixels (1,2,3).</p>
<p>The competition format requires&nbsp;a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded&nbsp;pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.</p>
<p>The file should contain a header and have the following format:</p>
<pre>img,pixels<br>1,1 1 5 1<br>2,1 1<br>3,1 1<br>etc.</pre>"
Facebook V: Predicting Check Ins,Identify the correct place for check ins,https://www.kaggle.com/competitions/facebook-v-predicting-check-ins,,"Multiclass Classification,Geography,Internet,Tabular",1209,1209,15035,"<p><a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins/details/work-at-facebook"" target=""_blank"">Ever wonder what it's like to work at Facebook?</a>&nbsp;Facebook and Kaggle are launching a machine learning engineering competition for 2016. Trail blaze your way to the top of the leaderboard to earn an opportunity at interviewing for one of the 10+ open roles&nbsp;as a software engineer, working on world class machine learning problems.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5186/media/FB5_banner.png"" alt=""""></p>
<p>The goal of this competition is to predict which place a person would like to check in to. For the purposes of this competition, Facebook created an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square. For a given set of coordinates, your task is to return a ranked list of the most likely places. Data was fabricated to resemble location signals coming from mobile devices, giving you a flavor of what it takes to work with real data complicated by inaccurate and noisy values. Inconsistent and erroneous location data can disrupt experience for services like Facebook Check In.</p>
<p>We highly encourage competitors to be active on <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins/scripts"" target=""_blank"">Kaggle Scripts</a>. Your work there will be thoughtfully included in the decision making process.</p>
<p><strong>Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.</strong></p>","<p>Submissions are evaluated according to the <a href=""https://www.kaggle.com/wiki/MeanAveragePrecision"">Mean Average Precision @3&nbsp;</a>&nbsp;(MAP@3):</p>
<p>$$MAP@3 = \frac{1}{|U|} \sum_{u=1}^{|U|} \sum_{k=1}^{min(3, n)} P(k)$$&nbsp;</p>
<p>where |U| is the number of check in events, P(k) is the precision at cutoff k, n is the number of predicted businesses.&nbsp;</p>
<h2>Submission File</h2>
<p>For every user check in, you must predict a space-delimited list of the businesses&nbsp;they check into. <strong>You may&nbsp;submit up to 3 predictions for each check in.</strong>&nbsp;The file should contain a header and have the following format:</p>
<pre>row_id,place_id<br>0,2083653582 1476539553 1000015801<br>1,6147316961 6147316961 8999137193<br>etc...</pre>"
Avito Duplicate Ads Detection,Can you detect duplicitous duplicate ads?,https://www.kaggle.com/competitions/avito-duplicate-ads-detection,,Internet,546,624,8147,"<p>Online marketplaces make&nbsp;it a breeze for users to both find and buy unique treasures or unload their dusty record collections in the spirit of spring cleaning. As one of the world's largest and fastest growing online classifieds, <a href=""https://www.avito.ru/"" target=""_blank"">Avito</a>&nbsp;hosts high volumes of listings and competitive sellers often go to great lengths to get their wares noticed.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5174/media/avito_duplicates_large1.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>For some sellers,&nbsp;this means posting the same ad several times with slightly altered text or photos taken from different angles.&nbsp;To ensure that buyers can easily find what they're looking for without sifting through dozens of deceptively identical ads, Avito is asking Kagglers to develop a model that can&nbsp;automatically spot duplicate ads.&nbsp;With more accurate duplicate&nbsp;ad detection, Avito will make it&nbsp;much easier for buyers to find and make their next purchase with an honest seller.</p>","<p>The goal of this competition is to predict if a pair of ads are duplicates. The evaluation metric for this competition is the <a href=""https://www.kaggle.com/wiki/AreaUnderCurve"" target=""_blank"">AUC (area under the curve)</a>.</p>
<h2>Submission File</h2>
<p>Each line of your submission should contain an id and a probability&nbsp;prediction. The prediction column should have values between 0 and 1, representing the probability of the pair of ads being duplicates.&nbsp;</p>
<p>Your submission file must have a header row and&nbsp;should&nbsp;have the following format:</p>
<pre>id,probability<br>0,0.543404941791<br>1,0.278369385094<br>2,1.0<br>3,0.5<br>etc.</pre>"
Painter by Numbers,Does every painter leave a fingerprint? ,https://www.kaggle.com/competitions/painter-by-numbers,,Image,41,50,177,"<p>With an original Picasso carrying a 106 million dollar price tag, identifying an authentic work of art from a forgery is a high-stakes industry. While algorithms have gotten good at telling us if a still life is of a basket of apples or a sunflower bouquet, they aren't yet able to tell us with certainty if both paintings are by van Gogh.&nbsp;&nbsp;</p>
<p>In this&nbsp;playground competition, we're challenging Kagglers to examine pairs of paintings and determine if they are by the same artist. This is an excellent opportunity to improve your&nbsp;computer vision skills and engage with a unique dataset of art.&nbsp;From the movement of brushstrokes to the use of light and dark, successful algorithms will likely incorporate many aspects of a painter's unique style.&nbsp;</p>
<h3>Resources</h3>
<ul>
<li><a href=""http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style"" target=""_blank"">neural algorithm</a></li>
<li><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3170918/"" target=""_blank"" style=""font-size: 1em; line-height: 1.5em"">How Do We See Art: An Eye-Tracker Study</a></li>
</ul>
<h3>Acknowledgments</h3>
<p>Many of the images in this dataset&nbsp;were obtained from <a href=""http://www.wikiart.org/"" target=""_blank"">wikiart.org</a>.&nbsp;Additional paintings were provided by artists whose contributions will be acknowledged at the close of the competition.</p>
<p>This playground competition and its datasets were prepared by&nbsp;<a href=""https://www.kaggle.com/smallyellowduck"" target=""_blank"">Small Yellow Duck</a> (Kiri Nichol). This includes the design of the&nbsp;pairwise-evaluation scheme.</p>","<p>The goal of this competition is to predict if a pair of images are artworks made by the same artist (or not). The evaluation metric for this competition is the <a href=""https://www.kaggle.com/wiki/AreaUnderCurve"" target=""_blank"">AUC (area under the curve)</a>.</p>
<h2>Submission File</h2>
<p>The<strong> submission_info.csv </strong>file contains three columns: index, image1 and image2. For each row in the submission_info file, you will need to predict the probability that image1 and image2 are by the same artist.</p>
<p><strong>Every row in the</strong> submission file should contain two columns: index and sameArtist. index is the row number from <strong>submission_info.csv</strong> and sameArtist is the probability that image1 and image2 are by the same artist.</p>
<p>The file should contain a header and have the following format:</p>
<pre>index,sameArtist<br>1,0.5<br>2,0.55<br>3,1.0<br>4,0.0<br>etc.</pre>"
Draper Satellite Image Chronology,Can you put order to space and time? ,https://www.kaggle.com/competitions/draper-satellite-image-chronology,,Image,399,422,2676,"<p>Imagine a world where we can use satellite images to help find better access to clean water, prevent poaching of wildlife, predict storms more efficiently, optimize traffic patterns more readily, and inform human behaviors to mitigate the spread of disease.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5229/media/draper_lg.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Thanks to a marked increase of satellites in orbit, we will be able to capture images – and the information contained within – of nearly every place on Earth, every day by 2017. However, our ability to analyze datasets of these images has not advanced as quickly.&nbsp;Changes from day to day in images of the same location are subtle, can be hard to detect, and are difficult to understand in terms of their significance.</p>
<p>In this competition, <a href=""http://www.draper.com/"" target=""_blank"">Draper</a>&nbsp;provides a unique dataset of&nbsp;images taken at the same locations over 5 days.&nbsp;Kagglers are challenged to predict the chronological order of the photos taken at each location. Accurately doing so could uncover approaches that&nbsp;have a global impact on commerce, science, and humanitarian works.</p>","<p>Submissions are evaluated on the mean <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"">Spearman's correlation coefficient</a>. To calculate this, the Spearman's rank correlation is computed for each set of 5 images.&nbsp;Identical predicted values are assigned fractional ranks equal to the average of their positions&nbsp;in the sorted&nbsp;values.&nbsp;These correlations are then averaged to create the score for&nbsp;the leaderboard.</p>
<h2>Submission File</h2>
<p>For each set of 5 images, you should predict a space-delimited list of the correct&nbsp;day for each respective image. For example, ""1,3 1 2 5 4"" implies that for setId = 1, the image set1_1 occurred third, set1_2 occurred first, set1_3 occurred second, set1_4 occurred fifth, and set1_5 occurred fourth.</p>
<p>The file should contain a header and have the following format:</p>
<pre>setId,day<br>1,1 2 3 4 5<br>2,1 2 3 4 5<br>3,1 2 3 4 5<br>etc.</pre>"
Expedia Hotel Recommendations,Which hotel type will an Expedia customer book?,https://www.kaggle.com/competitions/expedia-hotel-recommendations,,"Tabular,Hotels and Accommodations,Recommender Systems",1971,2176,22632,"<p>Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination,&nbsp;it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a&nbsp;trendy pool bar?&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5056/media/expedia_icons.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with&nbsp;hundreds of millions of visitors every month!</p>
<p>Currently,&nbsp;Expedia uses&nbsp;search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them&nbsp;for each&nbsp;user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will&nbsp;stay at 100 different hotel groups.</p>
<p><strong>The data in this competition&nbsp;is a random selection from Expedia and is not representative of the overall statistics.&nbsp;</strong></p>","<p>Submissions are evaluated according to the <a href=""https://www.kaggle.com/wiki/MeanAveragePrecision"">Mean Average Precision @ 5</a>&nbsp;(MAP@5):</p>
<p>$$MAP@5 = \frac{1}{|U|} \sum_{u=1}^{|U|} \sum_{k=1}^{min(5, n)} P(k)$$&nbsp;</p>
<p>where |U| is the number of user events, P(k) is the precision at cutoff k, n is the number of predicted hotel clusters.&nbsp;</p>
<h2>Submission File</h2>
<p>For every user event, you must predict a space-delimited list of the hotel clusters&nbsp;they booked. <strong>You may&nbsp;submit up to 5 predictions for each user event.</strong>&nbsp;The file should contain a header and have the following format:</p>
<pre>id,hotel_cluster<br>0,99 3 1 75 20<br>1,2 50 30 23 9<br>etc...</pre>"
Kobe Bryant Shot Selection,Which shots did Kobe sink?,https://www.kaggle.com/competitions/kobe-bryant-shot-selection,,"Basketball,Binary Classification,Tabular",1117,1200,10389,"
<p>Kobe Bryant marked his retirement from the NBA&nbsp;by scoring 60 points in his final game as a Los Angeles Laker on Wednesday, April 12, 2016. Drafted into the NBA at the age of 17, Kobe earned the sport’s highest accolades throughout his <a href=""http://www.npr.org/sections/thetwo-way/2016/04/13/474107238/kobe-bryants-life-and-career-by-the-numbers"" target=""_blank"">long career</a>.</p>
<p>Using 20 years of data on Kobe's swishes and misses, can you predict which shots will find the bottom of the net?&nbsp;This competition&nbsp;is well&nbsp;suited&nbsp;for practicing classification basics, feature engineering, and time series analysis. Practice got Kobe an eight-figure contract and 5 championship rings. What will it get you?</p>
<h3>Acknowledgements</h3>
<p>Kaggle is hosting this competition for the data science community to use for fun and education.&nbsp;For more data on Kobe and other NBA greats, visit&nbsp;stats.nba.com.</p>","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/LogarithmicLoss"">log loss</a>.</p>
<h2>Submission File</h2>
<p>For each missing shot_made_flag in the data set, you should predict a probability that Kobe made the field goal. The file should have a header and the&nbsp;following format:</p>
<pre>shot_id,shot_made_flag<br>1,0.5<br>8,0.5<br>17,0.5<br>etc.</pre>"
State Farm Distracted Driver Detection,Can computer vision spot distracted drivers?,https://www.kaggle.com/competitions/state-farm-distracted-driver-detection,,"Image,Automobiles and Vehicles",1438,1679,25571,"<p>We've all been there: a light turns green and the car&nbsp;in front of you doesn't&nbsp;budge. Or, a previously unremarkable vehicle suddenly&nbsp;slows&nbsp;and starts swerving from side-to-side.</p>
<p>When you pass the offending driver, what do you expect to see?&nbsp;You certainly aren't&nbsp;surprised when you spot a&nbsp;driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation&nbsp;on their phone.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5048/media/drivers_statefarm.png"" alt=""""></p>
<p>According to the CDC motor vehicle safety division, <a href=""http://www.cdc.gov/motorvehiclesafety/distracted_driving/"" target=""_blank"">one in five car accidents</a>&nbsp;is caused by a distracted driver. Sadly, this translates to&nbsp;425,000 people injured and 3,000 people killed by distracted driving every year.</p>
<p><a href=""https://www.statefarm.com/"" target=""_blank"">State Farm</a>&nbsp;hopes to improve these alarming statistics, and better insure their customers, by testing whether&nbsp;dashboard cameras can&nbsp;automatically detect drivers engaging in distracted behaviors.&nbsp;Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver's&nbsp;behavior. Are they driving attentively, wearing their&nbsp;seatbelt, or taking a selfie with their friends in the backseat?</p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each image&nbsp;has been labeled with one true class. For each image, you must submit a set of predicted probabilities (one for every image). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of images&nbsp;in the test set, M is the number of image&nbsp;class labels,&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given image&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the image file name, and a probability for each class.</p>
<p>The 10 classes to predict are:</p>
<p>c0: normal driving<br>c1: texting - right<br>c2: talking on the phone - right<br>c3: texting - left<br>c4: talking on the phone - left<br>c5: operating the radio<br>c6: drinking<br>c7: reaching behind<br>c8: hair and makeup<br>c9: talking to passenger</p>
<p>The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9<br>img_0.jpg,1,0,0,0,0,...,0<br>img_1.jpg,0.3,0.1,0.6,0,...,0<br>...</pre>"
Shelter Animal Outcomes,Help improve outcomes for shelter animals,https://www.kaggle.com/competitions/shelter-animal-outcomes,,"Animals,Multiclass Classification,Tabular",1599,1755,15596,"<p>Every year, approximately 7.6 million companion animals end up in US&nbsp;shelters. Many animals are&nbsp;given up as unwanted by&nbsp;their owners, while others are picked up after getting&nbsp;lost or taken out&nbsp;of cruelty situations. Many of these animals&nbsp;find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/5039/media/kaggle_pets2.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Using a dataset of intake information including breed, color, sex, and age from the <a href=""http://www.austintexas.gov/department/animal-services"" target=""_blank"">Austin Animal Center</a>, we're asking Kagglers to predict the outcome for each animal.</p>
<p>We also believe this dataset can help us understand trends in animal outcomes. These insights&nbsp;could help shelters focus their energy on specific animals who need a little extra help finding a&nbsp;new home. We encourage you to publish your insights on <a href=""https://www.kaggle.com/c/shelter-animal-outcomes/scripts"">Scripts</a>&nbsp;so they are publicly accessible.</p>
<h3>Acknowledgements</h3>
<p>Kaggle is hosting this competition for the machine learning community to use for data science practice and social good. The&nbsp;dataset is brought to you by <a href=""http://www.austintexas.gov/department/animal-services"" target=""_blank"">Austin Animal Center</a>. Shelter animal statistics were taken from the <a href=""http://www.aspca.org/animal-homelessness/shelter-intake-and-surrender/pet-statistics"" target=""_blank"">ASPCA</a>.</p>
<p><em>Glamour shots of Kaggle's&nbsp;shelter pets are pictured above. From left to right: Shelby, Bailey, Hazel, Daisy, and Yeti.</em></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each incident&nbsp;has been labeled with one true class. For each animal, you must submit a set of predicted probabilities (one for every class). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of animals&nbsp;in the test set, M is the number of outcomes, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in outcome&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to outcome&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given animal&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission Format</h2>
<p>You must submit a csv file with the animal&nbsp;id, all candidate outcome&nbsp;names, and a probability for each outcome. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>AnimalID,Adoption,Died,Euthanasia,Return_to_owner,Transfer<br>A715022,1,0,0,0,0<br>A677429,0.5,0.3,0.2,0,0<br>...<br>etc.</pre>"
Santander Customer Satisfaction,Which customers are happy customers?,https://www.kaggle.com/competitions/santander-customer-satisfaction,,"Tabular,Binary Classification,Banking",5115,5688,93333,"<p>From frontline support&nbsp;teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more,&nbsp;unhappy customers rarely voice their dissatisfaction&nbsp;before leaving.</p>
<p><a href=""https://www.santanderbank.com/us/personal"" target=""_blank"">Santander&nbsp;Bank</a>&nbsp;is asking Kagglers to help them identify dissatisfied&nbsp;customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a&nbsp;customer's happiness before it's too late.</p>
<p>In this competition, you'll work&nbsp;with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.<br><br><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4986/media/santander_custsat_red.png""></p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each ID&nbsp;in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:</p>
<pre>ID,TARGET<br>2,0<br>5,0<br>6,0<br>etc.</pre>"
March Machine Learning Mania 2016,Predict the 2016 NCAA Basketball Tournament,https://www.kaggle.com/competitions/march-machine-learning-mania-2016,,"Basketball,Tabular,Sports",596,653,1045,"<p><strong>Update</strong>: although&nbsp;the tournament is over, we're continuing our&nbsp;analysis under the <a href=""https://www.kaggle.com/wcukierski/2016-march-ml-mania"">predictions dataset page</a>.</p>
<p>Back for its third year, March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. You're provided&nbsp;data covering three decades of historical NCAA games&nbsp;and&nbsp;freely&nbsp;encouraged to use other sources of&nbsp;data to gain a winning&nbsp;edge.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/bball-logo.png"" alt=""""></p>
<p>In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2016 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2016 results.</p>
<h3>Acknowledgments</h3>
<p>SAP is the presenting sponsor of March Machine Learning Mania 2016. Please see <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2016/details/about-the-sponsor"">About the Sponsor</a>&nbsp;to read more.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4862/media/SAP%20Logo.png"" alt=""SAP Logo"" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are scored on the log loss, also called the predictive binomial deviance:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2016 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 &nbsp;= 2278 matchups.&nbsp;</p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104&nbsp;played team 1129&nbsp;in the year 2013. You must&nbsp;predict the probability that the team with the lower id beats the team with the higher id.</p>
<p>The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br>2012_1104_1124,0.5<br>2012_1104_1125,0.5<br>2012_1104_1140,0.5<br>...</pre>"
BNP Paribas Cardif Claims Management,Can you accelerate BNP Paribas Cardif's claims management process?,https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management,,"Banking,Tabular,Binary Classification",2920,3283,54337,"<p>As a global specialist in personal insurance, <a href=""https://www.bnpparibascardif.com/"" target=""_blank"">BNP Paribas Cardif</a>&nbsp;serves 90 million clients in 36 countries across Europe, Asia and Latin America.</p>
<p>In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science&nbsp;to&nbsp;meet the new needs and expectations of customers.</p>
<p><img style=""margin-right: auto; margin-left: auto; display: block"" alt="""" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4852/media/Kaggle_pic_1.PNG""></p>
<p>In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims:</p>
<ol>
<li></li>
<li></li>
</ol>
<p>Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers.</p>","<p>The evaluation metric for this competition is&nbsp;<strong>Log Loss</strong></p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N {(y_i\log(p_i) + (1 - y_i)\log(1 - p_i))}$$</p>
<p>where N is the number of observations, \\(log\\) is the natural logarithm, \\(y_{i}\\) is the binary target, and \\(p_{i}\\) is the predicted probability that \\(y_i\\) equals 1.</p>
<p>Note: the actual submitted predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p><strong>For every observation in the test dataset</strong>, submission files should contain two columns: ID and PredictedProb.&nbsp;The file should contain a header and have the following format:</p>
<pre>ID,PredictedProb<br>0,0.5<br>1,0.5<br>2,0.5<br>7,0.5<br>etc.</pre>"
Home Depot Product Search Relevance,Predict the relevance of search results on homedepot.com,https://www.kaggle.com/competitions/home-depot-product-search-relevance,,Tabular,2123,2551,35492,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4853/media/home_depot_tools.jpg""></p>
<p>Shoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. Speed, accuracy and delivering a frictionless customer experience are essential.</p>
<p>In this competition, Home Depot is asking Kagglers to help them improve their&nbsp;customers' shopping experience by developing a model that can accurately predict the relevance of search results.</p>
<p>Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.</p>","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/RootMeanSquaredError"">root mean squared error (RMSE)</a>.</p>
<h2>Submission File</h2>
<p>For each id in the test set, you must predict a relevance. This is a real number in [1,3]. The file should contain a header and have the following format:</p>
<pre>id,relevance<br>1,1<br>4,2<br>5,3<br>etc.</pre>"
Yelp Restaurant Photo Classification,Predict attribute labels for restaurants using user-submitted photos,https://www.kaggle.com/competitions/yelp-restaurant-photo-classification,,"Image,Food,Internet",355,355,4509,"<p>Does your favorite Ethiopian restaurant&nbsp;take reservations? Will a&nbsp;first date at that authentic&nbsp;looking bistro break your wallet?&nbsp;Is the&nbsp;diner down the street a good call for breakfast?&nbsp;Restaurant&nbsp;labels&nbsp;help Yelp users quickly answer questions like these, narrowing down their results to only restaurants&nbsp;that fit their nuanced needs.</p>
<p>In this competition, Yelp is challenging Kagglers to build a model that automatically tags restaurants with multiple labels&nbsp;using&nbsp;a dataset of user-submitted photos. Currently, restaurant labels are manually selected&nbsp;by Yelp users&nbsp;when they submit a review. Selecting the labels is optional, leaving some restaurants un- or only partially-categorized.&nbsp;</p>
<p>In an age of food selfies and photo-centric social storytelling, it may be no surprise to hear that&nbsp;Yelp's users&nbsp;upload an enormous amount&nbsp;of photos every day alongside their written reviews. Can you turn their pictures into (less than a thousand) words?</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4829/media/yelp_5starteam_trans.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Yelp isn’t only looking for your best model; we’re looking for data mining engineers that can help us use our data in novel ways while pushing code to production. The prize for this competition is a fast track through the&nbsp;recruiting process and an opportunity to show our data mining teams just what you’ve got!&nbsp;For more information about exciting opportunities at Yelp, check out the <a href=""https://www.kaggle.com/c/yelp-restaurant-photo-classification/details/jobs-at-yelp"" target=""_blank"">Jobs at Yelp</a>&nbsp;competition page and Yelp's own&nbsp;<a href=""http://www.yelp.com/careers"" target=""_blank"">careers page</a>.</p>","<p>The evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/MeanFScore""> Mean F1-Score</a>&nbsp;also known as example-based F-measure in the multi-label learning literature. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:</p>
<p>\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]</p>
<p>The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.</p>
<h2>Submission File</h2>
<p>For every business&nbsp;in the dataset, submission files should contain two columns: business_id&nbsp;and labels. Labels&nbsp;should be a space-delimited list. The file must have a header and should look like the following:</p>
<pre>business_id,labels<br>003sg,1 2 3<br>00er5,0 7<br>00kad,1 5 8<br>etc.</pre>"
Second Annual Data Science Bowl,Transforming How We Diagnose Heart Disease,https://www.kaggle.com/competitions/second-annual-data-science-bowl,,"Image,Healthcare",192,293,619,"<p>We all have a heart. Although we often take it for granted, it's our heart that gives us the moments in life to imagine, create, and discover. Yet cardiovascular disease threatens to take away these moments. Each day, 1,500 people in the U.S. alone are diagnosed with heart failure—but together, we can help. We can use data science to transform how we diagnose heart disease. By putting data science to work in the cardiology field, we can empower doctors to help more people live longer lives and spend more time with those that they love.</p>
<p>Declining cardiac function is a key indicator of heart disease. Doctors determine cardiac function by measuring end-systolic and end-diastolic volumes (i.e., the size of one chamber of the heart at the beginning and middle of each heartbeat), which are then used to derive the&nbsp;ejection fraction (EF). EF is the percentage of blood ejected from the left ventricle with each heartbeat. Both the volumes and the ejection fraction are predictive of heart disease. While a number of technologies can measure volumes or EF, Magnetic Resonance Imaging (MRI) is considered the gold standard test to accurately assess the heart's squeezing ability.</p>
<p><img alt=""Illustrated represetation of the DSB challenge"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4729/media/heart-illustration2-resized.jpg""></p>
<p>The challenge with using MRI to measure cardiac volumes and derive ejection fraction, however, is that the process is manual and slow. A skilled cardiologist must analyze MRI scans to determine EF. The process can take up to 20 minutes to complete—time the cardiologist could be spending with his or her patients. Making this measurement process more efficient will enhance doctors' ability to diagnose heart conditions early, and carries broad implications for advancing the science of heart disease treatment.</p>
<p>The 2015 Data Science Bowl challenges you to create an algorithm to automatically measure end-systolic and end-diastolic volumes in cardiac MRIs. You will examine MRI images from more than 1,000 patients. This data set was compiled by the National Institutes of Health and Children's National Medical Center and is an order of magnitude larger than any cardiac MRI data set released previously. With it comes the opportunity for the data science community to take action to transform how we diagnose heart disease.</p>
<p>This is not an easy task, but together we can push the limits of what's possible. We can give people the opportunity to spend more time with the ones they love, for longer than ever before.</p>
<h3>Acknowledgments</h3>
<p>The Data Science Bowl is presented by:</p>
<p><img style=""display: block; margin-left: auto; margin-right: auto"" alt=""Data Science Bowl"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/booz_kaggle.png""></p>
<p>The National Heart, Lung, and Blood Institute (NHLBI) provided the MRI images for this competition. Special thanks to NHLBI Intramural Investigators Dr. Michael Hansen and Dr. Andrew Arai.</p>
<p>Additional support for the Data Science Bowl was provided by NVIDIA:</p>
<p><img style=""display: block; margin-left: auto; margin-right: auto"" alt=""nVidia"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4729/media/NVLogo_2D.jpg""></p>","<p>Submissions will be evaluated on the&nbsp;Continuous Ranked Probability Score (CRPS). For each MRI, you must&nbsp;predict a cumulative probability distribution for both the systolic and diastolic volumes (two separate distributions per case). The CRPS is computed as follows:</p>
<p>\[&nbsp;C = \frac{1}{600N} \sum_{m=1}^{N} \sum_{n=0}^{599} (P(y \le n) -H(n -&nbsp;V_m))^2, \]&nbsp;</p>
<p>where P is the predicted distribution, N is the number of rows in the test set (equal to twice the number of cases), V&nbsp;is the actual&nbsp;volume (in mL) and&nbsp;H(x) is the Heaviside step function (\\(H(x) = 1\\) for \\(x \ge 0\\)&nbsp;and zero otherwise). While it is not simple to&nbsp;visualize the CRPS, the shaded area on the figure below&nbsp;may be a helpful guide for understanding the error term between the predicted distribution and actual volume:</p>
<p><img alt="""" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4729/media/crps.png""></p>
<p>The entry will&nbsp;not score&nbsp;if any of the predicted values&nbsp;has \[P(y \le k) &gt; P(y \le k+1)\] for any k (i.e., the CDF must&nbsp;be non-decreasing).</p>
<h2>Submission&nbsp;File</h2>
<p>For each Id, you must predict 600 values that represent its cumulative distribution from 0 to 599 mL. P0 represents the probability the volume is less than or equal to 0 mL,&nbsp;P1&nbsp;represents the probability the volume is less than or equal to 1&nbsp;mL, etc. The file must have a header and contain all 600 values in the following format:</p>
<pre style=""white-space: pre-wrap"">Id,P0,P1,P2,P3,...,P599<br>1_systolic,0.1,0.3,0.33,0.4,...,1.0<br>1_diastolic,0.1,0.24,0.25,0.35,...,1.0<br>...<br>etc</pre>"
Santa's Stolen Sleigh,"♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫",https://www.kaggle.com/competitions/santas-stolen-sleigh,,Tabular,1126,1213,10540,"<p><a href=""https://www.kaggle.com/the1owl/santas-stolen-sleigh/you-ll-shoot-your-eye-out-kid"">Fork this script and get started on the problem</a></p>
<p>The North Pole&nbsp;is in an uproar over&nbsp;news that Santa's magic sleigh has been stolen. Able to carry all the world's&nbsp;presents in one trip, it was&nbsp;considered crucial to successfully delivering holiday goodies across the globe&nbsp;in one night.</p>
<p>Unwilling to cancel&nbsp;Christmas,&nbsp;Santa is determined to deliver toys to all the good girls and boys using his&nbsp;day-to-day, magic-less sleigh. With so little time to pull off this&nbsp;plan, Santa is once again counting on Kagglers to&nbsp;help.</p>
<p>Given the sleigh's antiquated, weight-limited specifications, your challenge is to&nbsp;optimize the routes and loads&nbsp;Santa will take to and from the North Pole. And don't forget about Dasher, Dancer, Prancer, and Vixen; Santa is adamant that the best solutions will minimize the toll of this hectic night on his reindeer&nbsp;friends.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4704/media/santabanner2015.png"" alt=""""></p>
<h3>Acknowledgements</h3>
<p>This competition is brought to you by <a href=""http://www.fico.com/"" target=""_blank"">FICO</a>.</p>
","<p>Your goal is to&nbsp;minimize total weighted&nbsp;reindeer weariness:</p>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<p>Mathematically, weighted reindeer weariness&nbsp;is calculated by:</p>
<p>$$WRW = \sum\limits_{j=1}^{m} \sum\limits_{i=1}^{n} \Big[&nbsp;\big( \sum\limits_{k=1}^{n} w_{kj} - \sum\limits_{k=1}^{i} w_{kj} \big) \cdot Dist(Loc_i, Loc_{i-1}) \Big]_j ,$$</p>
<p>where m is the number of trips, n is the number of gifts for each trip \\(j\\), \\(w_{ij}\\) is the weight of the \\(i^{th}\\)&nbsp;gift at trip \\(j\\), \\(Dist()\\) is calculated with <a href=""https://en.wikipedia.org/wiki/Haversine_formula"">Haversine Distance</a>&nbsp;between two locations, and \\(Loc_i\\) is the location of gift \\(i\\).&nbsp;\\(Loc_0\\) and&nbsp;\\(Loc_n\\) are North Pole, and&nbsp;\\(w_{nj}\\), a.k.a. the last leg of each trip, is always the base weight of the sleigh.</p>
<p>For example, if you have 2 gifts A, B to deliver in the trip, then the WRW is calculated as:</p>
<p>dist( North pole to A ) * ( weight A + weight B + base_weight ) +</p>
<p>dist( A to B ) * ( weight B + base_weight ) +</p>
<p>dist( B to North pole ) * ( base_weight )</p>
<h2>Submission File</h2>
<p>Submission files should contain two columns: GiftId and TripId.&nbsp;GiftId should be ordered by the order of delivery, and different trips should have different&nbsp;TripIds.&nbsp;</p>
<p>The file should contain a header and have the following format:</p>
<pre>GiftId,TripId<br>1,1<br>5,1<br>3,2<br>4,2<br>2,1<br>etc.</pre>"
Airbnb New User Bookings,Where will a new guest book their first travel experience?,https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings,,"Hotels and Accommodations,Recommender Systems,Tabular",1458,1458,20753,"<p>Instead of waking to overlooked&nbsp;""Do not disturb"" signs, <a href=""https://www.airbnb.com/"" target=""_blank"">Airbnb</a>&nbsp;travelers find themselves rising with the birds in a&nbsp;whimsical treehouse, having their morning coffee on the deck of a houseboat,&nbsp;or cooking a shared regional breakfast with their hosts.</p>
<p>New users on Airbnb can book a place to stay in 34,000+ cities across 190+ countries. By accurately predicting&nbsp;where&nbsp;a new user will book their first travel experience, Airbnb can share more personalized content with their community, decrease the average time to first booking,&nbsp;and better forecast demand.</p>
<p>In this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking. Kagglers who impress with their answer (and an explanation of how they got there) will be considered for an interview for the opportunity to join Airbnb's <a href=""https://www.airbnb.com/careers/departments/data-science-analytics"" target=""_blank"">Data Science and Analytics team</a>.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4651/media/airbnb_banner.png"" alt=""""></p>
<p><em>Wondering if you're a good fit?&nbsp;</em><em>Check out <a href=""http://venturebeat.com/2015/06/30/how-we-scaled-data-science-to-all-sides-of-airbnb-over-5-years-of-hypergrowth/"" target=""_blank"">this article</a>&nbsp;on how Airbnb scaled data science to all sides of their&nbsp;organization, and visit their&nbsp;<a href=""https://www.airbnb.com/careers"" target=""_blank"">careers page</a>&nbsp;for more on Airbnb's&nbsp;mission to create a world that inspires human connection.</em></p>","<p>The evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain"">NDCG (Normalized discounted cumulative gain)</a>&nbsp;@k where k=5.&nbsp;NDCG is calculated as:</p>
<p>\[&nbsp;DCG_k=\sum_{i=1}^k\frac{2^{rel_i}-1}{\log_2{\left(i+1\right)}}, \]</p>
<p>\[ nDCG_k=\frac{DCG_k}{IDCG_k}, \]</p>
<p>where \\(rel_i\\) is the&nbsp;relevance of the result at position&nbsp;\\(i\\).</p>
<p>\\(IDCG_k\\) is the maximum possible (ideal) \\(DCG\\) for a given set of queries.&nbsp;All NDCG calculations are relative values on the interval 0.0 to 1.0.</p>
<p>For each new user, you are to make a maximum of 5 predictions on the country of the first booking. The ground truth country is marked with relevance = 1, while the rest have relevance = 0.</p>
<p>For example, if for a particular user the destination is FR, then the predictions become:</p>
<p>[ FR ] &nbsp;gives a \\(NDCG=\frac{2^{1}-1}{log_{2}(1+1)}=1.0\\)</p>
<p>[ US, FR ] gives a \\(DCG=\frac{2^{0}-1}{log_{2}(1+1)}+\frac{2^{1}-1}{log_{2}(2+1)}=\frac{1}{1.58496}=0.6309\\)&nbsp;</p>
<h2>Submission File</h2>
<p>For every user&nbsp;in the dataset, submission files should contain two columns: id&nbsp;and country. <strong>The destination country predictions must be ordered such that the most probable&nbsp;destination country goes first.</strong></p>
<p>The file should contain a header and have the following format:</p>
<pre>id,country<br>000am9932b,NDF<br>000am9932b,US<br>000am9932b,IT<br>01wi37r0hw,FR<br>etc.</pre>"
Telstra Network Disruptions,Predict service faults on Australia's largest telecommunications network ,https://www.kaggle.com/competitions/telstra-recruiting-network,,"Multiclass Classification,Internet,Tabular",971,971,19548,"<p>In their first recruiting competition, <a href=""https://www.telstra.com.au/"" target=""_blank"">Telstra</a>&nbsp;is<img style=""float: right"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4726/media/telstra_300.png"">&nbsp;challenging&nbsp;Kagglers to predict the severity of service disruptions&nbsp;on their network. Using a dataset of features from their service logs, you're tasked&nbsp;with predicting if a disruption is a momentary glitch or a total interruption of connectivity.</p>
<p>Telstra is on a journey to enhance the customer experience - ensuring everyone in the company is putting customers first. In terms of its expansive network, this means continuously advancing how it predicts the scope and timing of service disruptions. Telstra wants to see how you would help it drive customer advocacy by developing a more advanced predictive model for service disruptions and to help it better serve its customers.</p>
<p>This challenge&nbsp;was crafted as a simulation of the type of problem&nbsp;you might tackle&nbsp;as a member of the team at&nbsp;Telstra.</p>
<p><em>Kagglers who stand out will be considered for data science roles in Telstra's Big Data team in Telstra’s absolute discretion. Highly-ranked participants will combine technical expertise and intuition in data science problems with a keen business sense and an effortless ability to work with technical and non-technical staff to turn data into real changes that impact customers. Highly-ranked participants will be considered by Telstra for interviews for employment, based on their work in the Competition and ability to meet the selection criteria for any suitable open job vacancy in Melbourne and Sydney, Australia.&nbsp;Participation in this Competition is not a recruitment process and Kaggle does not provide Telstra with recruitment services.</em></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each data row&nbsp;has been labeled with one true class. For each row, you must submit a set of predicted probabilities (one for every fault severity). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of rows&nbsp;in the test set, M is the number of fault severity&nbsp;classes,&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class&nbsp;\\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given row&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the row id, all 3 classes (0,1,2), and a probability for each fault severity. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>id,predict_0,predict_1,predict_2<br>11066,0,1,0<br>18000,0,1,0<br>etc.</pre>"
Prudential Life Insurance Assessment,Can you make buying life insurance easier?,https://www.kaggle.com/competitions/prudential-life-insurance-assessment,,Tabular,2610,2610,45348,"<p>Picture this. You are a data scientist in a start-up culture with the potential to have a very large impact on the business. Oh, and you are backed up by a company with 140 years' business experience.</p>
<p>Curious? Great! You are the kind of person we are looking for.</p>
<p><a href=""https://www.kaggle.com/c/prudential-life-insurance-assessment/details/about-prudential"">Prudential</a>, one of the largest issuers&nbsp;of life insurance in the USA, is hiring passionate data scientists to join a newly-formed Data Science group solving complex challenges and identifying opportunities. The results have been impressive so far but we want more.&nbsp;</p>
<h4>The Challenge</h4>
<p>In a one-click shopping world with on-demand <img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4699/media/iStock_insurancehands300.png"" alt="""" style=""float: right"">everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days.</p>
<p>The result? People are turned off. That’s why only 40% of U.S. households own individual life insurance.&nbsp;Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries.</p>
<p>By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry.</p>
<p>The results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process.</p>","<p>Submissions are scored based on the&nbsp;quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0.</p>
<p>The response variable has 8&nbsp;possible ratings.&nbsp; Each application&nbsp;is characterized by a tuple <em>(e</em>,<em>e)</em>, which corresponds to its scores by <em>Rater A</em> (actual risk) and <em>Rater B</em> (predicted risk).&nbsp; The quadratic weighted kappa is calculated as follows.</p>
<p>First, an N&nbsp;x&nbsp;N histogram matrix <em>O</em> is constructed, such that <em>O</em> corresponds to the number of applications&nbsp;that received a rating <em>i</em> by<em>&nbsp;A</em> and a rating <em>j</em> by<em>&nbsp;B</em>.&nbsp;An <em>N-by-N </em>matrix of weights, <em>w</em>, is calculated based on the difference between raters' scores:</p>
<p></p>
<p>An <em>N-by-N</em> histogram matrix of expected ratings, <em>E</em>, is calculated, assuming that there is no correlation between rating scores.&nbsp; This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that <em>E</em> and <em>O</em> have the same sum.</p>
<p>From these three matrices, the quadratic weighted kappa is calculated as:&nbsp;</p>
<p></p>
<h2>Submission File</h2>
<p>For each Id in the test set, you must predict the Response variable. The file should contain a header and have the following format:</p>
<pre>Id,Response<br>1,4<br>3,8<br>4,3<br>etc.</pre>"
Homesite Quote Conversion,Which customers will purchase a quoted insurance plan?,https://www.kaggle.com/competitions/homesite-quote-conversion,,"Binary Classification,Tabular",1755,1916,36264,"<p>Before&nbsp;asking someone on a date or skydiving, it's<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4657/media/homesite.png"" alt="""" style=""float: right""> important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. <a href=""https://homesite.com/"" target=""_blank"">Homesite</a>,&nbsp;a leading provider of homeowners insurance, does not currently&nbsp;have a dynamic&nbsp;conversion rate model that can give them confidence a quoted price will lead to a purchase.&nbsp;</p>
<p>Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you&nbsp;to predict which&nbsp;customers will purchase a given quote.&nbsp;Accurately predicting conversion&nbsp;would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments.&nbsp;</p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each QuoteNumber in the test set, you must predict a probability for QuoteConversion_Flag. The file should contain a header and have the following format:</p>
<pre>QuoteNumber,QuoteConversion_Flag<br>3,0<br>5,0.3<br>7,0<br>etc.</pre>"
The Winton Stock Market Challenge,Join a multi-disciplinary team of research scientists,https://www.kaggle.com/competitions/the-winton-stock-market-challenge,,"Finance,Tabular",829,829,10717,"<p>Do you laugh (and then get down to work) in the face of&nbsp;terabytes of noisy, non-stationary data? Winton Capital is looking for data scientists&nbsp;who excel at finding the hidden signal in the proverbial haystack, and who are excited&nbsp;by creating&nbsp;novel statistical modelling and data mining techniques.&nbsp;</p>
<p>In this recruiting competition, Winton challenges you to take on the very difficult&nbsp;task of predicting the future (stock returns). Given historical stock performance and a host of&nbsp;masked features, can you predict&nbsp;intra and end of day returns without being deceived by all the noise?&nbsp;</p>
<p>Research&nbsp;scientists at Winton have crafted this competition to be&nbsp;challenging and fun for the community while providing a taste of the types of problems they work on everyday. They're excited to connect with Kagglers who bring a&nbsp;unique background and&nbsp;creative approach to&nbsp;the&nbsp;competition.</p>
<p>Winton is offering cash prizes to winning teams as a reward for their work, but the intent of the competition is not commercial. The intellectual property you create remains your own and will be evaluated in the context of suitability for employment.&nbsp;</p>
<p></p>
<p><em>For more on the culture at Winton, check out the <a href=""https://www.kaggle.com/c/the-winton-stock-market-challenge/details/about-winton"" target=""_blank"">About Winton</a>&nbsp;page or their&nbsp;<a href=""https://www.wintoncapital.com/Careers/"" target=""_blank"">careers page</a>.</em></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/WeightedMeanAbsoluteError"" target=""_blank"">Weighted Mean Absolute Error</a>. Each return&nbsp;you predicted is compared with the actual return. The formula is then</p>
<p>$$WMAE = \frac{1}{n}\sum\limits_{i=1}^{n} w_i \cdot \left|y_i - \hat{y_i}\right| ,$$</p>
<p>where \\(w_i\\) is the weight associated with the return, Weight_Intraday, Weight_Daily for intraday and daily returns, \\(i\\), \\(y_i\\) is the predicted return, \\(\hat{y_i}\\) is the actual return, \\(n\\) is the number of predictions.&nbsp;</p>
<p>The weights for the training set are given in the training data. The weights for the test set are unknown.</p>
<h2>Submission File</h2>
<p>The submission file&nbsp;should contain two columns: Id&nbsp;and Predicted. <strong>For each 5-day window</strong>, you need to predict 62 returns. For example, for the first time window, you will predict 1_1, 1_2, to 1_62. 1_1 to 1_60 are predicting&nbsp;Ret_121 through Ret_180, 1_61 the prediction for Ret_PlusOne, and 1_62 the prediction for Ret_PlusTwo.</p>
<p>The file should contain a header and have the following format:</p>
<pre>Id,Predicted<br>1_1,0<br>1_2,0<br>1_3,0<br>1_4,0<br>...<br>1_60,0<br>1_61,0<br>1_62,0<br>2_1,0<br>2_2,0<br>etc.</pre>"
Walmart Recruiting: Trip Type Classification,Use market basket analysis to classify shopping trips,https://www.kaggle.com/competitions/walmart-recruiting-trip-type-classification,,"Multiclass Classification,Tabular",1043,1043,13058,"<p>Walmart uses both art and science to continually make progress on their core mission of better understanding and serving their customers. One way Walmart is able to improve customers' shopping experiences is by segmenting their store visits into different trip types.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4654/media/walmart_triptypes640.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Whether they're on a last minute run for new puppy supplies or leisurely making their way through a weekly grocery list, classifying trip types&nbsp;enables Walmart to create the best shopping experience for every customer.</p>
<p>Currently, Walmart's trip types are created from&nbsp;a combination of existing customer insights (""art"") and purchase history&nbsp;data (""science"").&nbsp;In their third recruiting competition, Walmart&nbsp;is challenging Kagglers to focus on the (data) science and classify&nbsp;customer trips using only a transactional dataset&nbsp;of the items they've purchased. Improving the science behind trip type&nbsp;classification will help Walmart refine their segmentation process.</p>
<p>Walmart&nbsp;is hosting this competition to connect with data scientists who break the mold.</p>","<p>Submissions are evaluated using&nbsp;the multi-class logarithmic loss. For each visit, you must submit a set of predicted probabilities (one for every&nbsp;TripType). The formula is:</p>
<p>$$-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of visits&nbsp;in the test set, M is the number of trip types, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is of&nbsp;class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to&nbsp;class \\(j\\).</p>
<p>The submitted probabilities for a given visit&nbsp;are not required to sum to one&nbsp;because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes&nbsp;of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission Format</h2>
<p>You must&nbsp;submit a csv file with the VisitNumber, all candidate TripTypes, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>""VisitNumber"",""TripType_3"",""TripType_4"",...<br>1,0,0.1,...<br>2,1,0,...<br>etc.</pre>"
The Allen AI Science Challenge,Is your model smarter than an 8th grader?,https://www.kaggle.com/competitions/the-allen-ai-science-challenge,,"Artificial Intelligence,Text,Tabular,Multiclass Classification",170,302,679,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4571/media/AI_banner.png"" alt=""""></p>
<p>The&nbsp;<a href=""http://allenai.org/index.html"" target=""_blank"">Allen Institute for Artificial Intelligence (AI2)</a>&nbsp;is working to improve&nbsp;humanity through fundamental advances in artificial intelligence.&nbsp;One critical&nbsp;but challenging problem in&nbsp;AI&nbsp;is to demonstrate the ability to consistently&nbsp;understand and correctly answer general questions about the world.&nbsp;</p>
<p>The&nbsp;<a href=""http://allenai.org/aristo.html"" target=""_blank"">Aristo project</a>&nbsp;at AI2 is focused on building such a system. One way Aristo ""learns"" is by extracting facts from various sources and processing them into a structured knowledge base. When taking an exam, questions are parsed and processed along with any accompanying diagrams to determine a strategy for answering. Aristo then uses&nbsp;entailment, statistical analysis, and inference methods to&nbsp;select a final answer.</p>
<p>While Aristo's abilities have improved significantly&nbsp;in the last two years, it&nbsp;still doesn't have&nbsp;perfect, reliable methods of gathering knowledge, understanding questions, or reasoning through answers.</p>
<p>Using a dataset of multiple choice question and answers from a standardized 8th grade science exam, AI2 is challenging&nbsp;you to&nbsp;create a model that&nbsp;gets to the head of the class.</p>","<p>Submissions are evaluated by categorization accuracy, i.e., the fraction of multiple choice questions answered correctly. Random guessing should produce an evaluation score of around 0.25.</p>
<h2>Submission file</h2>
<p><strong>For every question&nbsp;in the dataset</strong>, submission files should contain two columns: id&nbsp;and correctAnswer, where the correctAnswer is one of A, B, C or D just like in the training set and the id is the question id from the dataset.</p>
<p>The file should contain a header and have the following format:</p>
<pre>id,correctAnswer<br>102501,A<br>102502,B<br>102503,B<br>102504,D<br>etc.</pre>
<h2>Model&nbsp;submission&nbsp;and final test set</h2>
<p>During the last two weeks of the model training period, you will be able to upload your models to Kaggle. This model submission must contain all data, code, and parameter settings necessary to evaluate your models on new questions, and include a README file with instructions on how to do so. The purpose of this is to ensure a fair competition and that no manual answering&nbsp;of the test set questions&nbsp;has been done. You must also make at least one submission on the validation set with this model before the model submission deadline.</p>
<p>If you would like, you may submit your model as an encrypted archive, and you will only be asked to provide the decryption key if you are one of the preliminary winners. <strong>The model submission is required to be eligible to win prize money.</strong></p>
<p>When the final test set is released, the same model should&nbsp;be used to submit answers to the test set, and the prize winning models will be verified&nbsp;manually as to fulfilling these requirements. To avoid random discrepancies, make sure to seed any random number generators used in the model.</p>"
Rossmann Store Sales,"Forecast sales using store, promotion, and competitor data",https://www.kaggle.com/competitions/rossmann-store-sales,,"Tabular,Time Series Analysis",3298,3735,70114,"<p>Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to&nbsp;six weeks in advance. Store sales are influenced by many factors, including&nbsp;promotions, competition, school and state holidays, seasonality, and locality. With&nbsp;thousands of individual managers predicting sales based on their unique circumstances,&nbsp;the accuracy of results can be quite varied.</p>
<p></p>
<p></p>
<p><em></em></p>","<p>Submissions are evaluated on the Root Mean Square Percentage Error (RMSPE). The RMSPE is calculated as</p>
<p>$$<br>\textrm{RMSPE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(\frac{y_i - \hat{y}_i}{y_i}\right)^2},<br>$$</p>
<p>where y_i denotes the sales of a single store on a single day and yhat_i&nbsp;denotes the corresponding prediction. Any day and store with 0 sales is ignored in scoring.</p>
<h2>Submission File</h2>
<p>The file should contain a header and have the following format:</p>
<pre>Id,Sales<br>1,0<br>2,0<br>3,0<br>etc.</pre>"
How Much Did It Rain? II,Predict hourly rainfall using data from polarimetric radars,https://www.kaggle.com/competitions/how-much-did-it-rain-ii,,"Regression,Tabular",587,691,8105,"<p>After incorporating&nbsp;feedback from the Kaggle community, as well as scientific and educational partners, the&nbsp;Artificial Intelligence Committee of the American Meteorological Society&nbsp;is&nbsp;excited to be running a second&nbsp;iteration of the <a href=""https://www.kaggle.com/c/how-much-did-it-rain"" target=""_blank"">How Much Did It Rain?</a>&nbsp;competition.</p>
<p>How Much Did It Rain? II is focused on solving the same core rain measurement prediction problem, but approaches it with a new and improved dataset and evaluation metric. This competition will go even further towards building a&nbsp;useful educational tool for universities, as well as making a meaningful contribution to continued meteorological research.</p>
<h3>Competition&nbsp;Description</h3>
<p>Rainfall is highly variable across space and time, making it notoriously tricky to measure. Rain gauges can be an&nbsp;effective measurement tool for a specific location, but it is impossible to have them everywhere. In order to have widespread coverage, data from weather radars is&nbsp;used to estimate rainfall nationwide.&nbsp;Unfortunately, these predictions never exactly match the measurements taken&nbsp;using rain gauges.</p>
<p>Recently, in an effort to improve their rainfall predictors, the U.S. National Weather Service upgraded their radar network to be polarimetric. These polarimetric radars are able to provide higher quality&nbsp;data than conventional <a href=""https://en.wikipedia.org/wiki/Doppler_radar"" target=""_blank"">Doppler radars</a>&nbsp;because they&nbsp;transmit radio wave pulses with both horizontal and vertical orientations.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4195/media/dual_pol2.jpg"" alt=""Polarimetric radar. Image courtesy NOAA"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Dual pulses make it easier to infer the size and type of precipitation because rain drops become flatter as they increase in size,&nbsp;whereas&nbsp;ice crystals tend to be elongated vertically.</p>
<p>In this competition, you are given snapshots of polarimetric radar values and asked to predict the hourly rain gauge total. A word of caution:&nbsp;many of the gauge values in the training dataset are implausible (gauges may get clogged, for example). More details are on the <a href=""https://www.kaggle.com/c/how-much-did-it-rain-ii/data"" target=""_blank"">data page</a>.</p>
<h2>Acknowledgements</h2>
<p>This competition is sponsored by the <a href=""http://www2.ametsoc.org/stac/index.cfm/committees/committee-on-artificial-intelligence-applications-to-environmental-science/"">Artificial Intelligence Committee</a>&nbsp;of the American Meteorological Society.&nbsp;<a href=""http://www.climate.com/"">Climate Corporation</a>&nbsp;is providing&nbsp;the prize pool.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4495/media/rainsponsors.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>The&nbsp;evaluation metric for this competition is <a href=""https://www.kaggle.com/wiki/MeanAbsoluteError"" target=""_blank"">Mean Absolute Error (MAE)</a>. Please refer to this <a href=""http://climate.geog.udel.edu/~climate/publication_html/Pdf/WM_CR_05.pdf"">paper</a>&nbsp;about&nbsp;the choice of MAE over RMSE.&nbsp;&nbsp;</p>
<h2>Submission File</h2>
<p><strong>For every gauge-hour&nbsp;in the dataset</strong>, submission files should contain two columns: Id and Expected. &nbsp;The Id corresponds to the column of that name in the test.csv. The Expected is the predicted gauge value.&nbsp;The file should contain a header and have the following format:</p>
<pre>Id,Expected<br>1,0.73<br>2,1.23<br>3,0.49<br>etc.</pre>"
What's Cooking?,Use recipe ingredients to categorize the cuisine,https://www.kaggle.com/competitions/whats-cooking,,"Multiclass Classification,Text,Food",1387,1539,14294,"<p><em>Picture yourself strolling&nbsp;through your local, open-air market...&nbsp;</em><em>What do you see? What do you smell? What will you make for dinner tonight?</em></p>
<p>If you're in Northern California, you'll be&nbsp;walking past the inevitable&nbsp;bushels of leafy greens, spiked with dark purple kale&nbsp;and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.</p>
<p>Some of our strongest geographic and cultural associations are tied to a&nbsp;region's local foods. This playground competitions&nbsp;asks you to predict the category of a dish's cuisine given a&nbsp;list of its ingredients.&nbsp;</p>
<h3>Acknowledgements</h3>
<p>We&nbsp;want to thank <a href=""http://www.yummly.com/"" target=""_blank"">Yummly</a>&nbsp;for&nbsp;providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4526/media/Yummly_logo.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are evaluated on the categorization accuracy (the percent of dishes&nbsp;that you correctly classify).</p>
<h2>Submission File</h2>
<p>Your submission file should predict the cuisine&nbsp;for each recipe in the test set. The file should contain a header and have the following format:</p>
<pre>id,cuisine<br>35203,italian<br>17600,italian<br>35200,italian<br>17602,italian<br>...<br>etc.</pre>"
Right Whale Recognition,Identify endangered right whales in aerial photographs ,https://www.kaggle.com/competitions/noaa-right-whale-recognition,,"Image,Animals,Water Bodies",364,470,4788,"<p>With fewer&nbsp;than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each&nbsp;whale is integral to&nbsp;the efforts of researchers working to protect&nbsp;the species from extinction.</p>
<p>Currently, only a handful of very experienced researchers can&nbsp;identify individual whales on sight while out on the water. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively&nbsp;target whales for biological samples, acoustic recordings, and necessary health assessments.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4521/media/ChristinKhan_RightWhaleMomCalf_640.png""></p>
<p>To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an&nbsp;online photo-identification catalog. Customized software has been developed to aid in this process (<a href=""http://www.neaq.org/conservation_and_research/projects/endangered_species_habitats/right_whale_research/right_whale_projects/monitoring_individuals_and_family_trees/identifying_with_photographs/digits.php"" target=""_blank"">DIGITS</a>), but this still relies on a manual inspection of the potential comparisons, and there is a lag time for those images to be incorporated into the database. The current identification process is extremely time consuming and requires special training. This&nbsp;constrains&nbsp;marine biologists, who work under tight deadlines with&nbsp;limited budgets.</p>
<p>This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales.&nbsp;Automating the identification of right whales would&nbsp;allow researchers to better focus on their&nbsp;conservation efforts.&nbsp;Recognizing a whale in real-time would also give researchers on the water access to potentially life-saving historical health and entanglement records as they struggle to free a whale that has been accidentally caught up in fishing gear.</p>
<h3>Acknowledgements</h3>
<p><a href=""http://www.mathworks.com/"" target=""_blank"">MathWorks</a>&nbsp;is sponsoring the competition prize pool. If your team is participating in this competition MathWorks is also providing complimentary&nbsp;software.&nbsp;<a href=""http://www.mathworks.com/academia/student-competitions/kaggle/"" target=""_blank"">Click here</a> for more details on how to request your copy.</p>
<p><a href=""http://www.mathworks.com/academia/student-competitions/kaggle/""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4521/media/mathworks_logo.jpg"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></a></p>
<p>Thanks to Christin Khan and Leah Crowe from <a href=""http://www.nefsc.noaa.gov/psb/"" target=""_blank"">NOAA</a>&nbsp;for hand labeling the images to create this one of a kind dataset and to the right whale research team at <a href=""http://www.neaq.org/index.php"" target=""_blank"">New England Aquarium</a>&nbsp;for maintaining the photo-identification <a href=""http://rwcatalog.neaq.org/"" target=""_blank"">catalog</a>. Without their continued efforts, none of this would be possible.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4521/media/New_England_Aquarium.svg.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each image&nbsp;has been labeled with one true class. For each image, you must submit a set of predicted probabilities (one for every whale). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of images&nbsp;in the test set, M is the number of whale&nbsp;labels,&nbsp; \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to whale \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to whale&nbsp;\\(j\\).</p>
<p>The submitted probabilities for a given image&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission File</h2>
<p>You must submit a csv file with the image file name, all candidate whale ids, and a probability for each whale id. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>Image,whale_00195,whale_00442,whale_02411,whale_02608,whale_02839,...,whale_99573<br><br>w_1947.jpg,1,0,0,0,0,...,0<br>...</pre>"
Springleaf Marketing Response,Determine whether to send a direct mail piece to a customer ,https://www.kaggle.com/competitions/springleaf-marketing-response,,"Binary Classification,Tabular,Marketing",2221,2482,39297,"<p><a href=""https://www.springleaf.com/"" target=""_blank"">Springleaf</a>&nbsp;puts&nbsp;the humanity back into lending by offering their customers personal and auto loans that help them take control of their lives and their finances. Direct mail is one important way Springleaf's team can connect with customers whom may be in need of a loan.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4487/media/springleafbanner_650crop.png"" alt=""""></p>
<p>Direct offers provide huge value to customers who need them, and are a fundamental&nbsp;part&nbsp;of Springleaf's marketing strategy. In order to improve their targeted&nbsp;efforts,&nbsp;Springleaf must be sure they&nbsp;are focusing on the customers who are likely to respond and be good candidates for their services.</p>
<p>Using a large set&nbsp;of anonymized features, Springleaf is asking you&nbsp;to predict which customers will respond to a direct mail offer. You&nbsp;are&nbsp;challenged to construct new meta-variables and employ&nbsp;feature-selection methods to approach this dauntingly wide dataset.</p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability and the observed target.</p>
<h2>Submission File</h2>
<p>For each ID in the test set, you should predict a probability. The file should contain a header and have the following format:</p>
<pre>ID,target<br>1,0.35<br>3,0.001<br>6,0.9333<br>etc.</pre>"
Truly Native? ,Predict which web pages served by StumbleUpon are sponsored,https://www.kaggle.com/competitions/dato-native,,"Binary Classification,Marketing,Tabular",273,339,3220,"<p>Online media companies rely more and more on paid advertising to keep their lights on and their content engines humming. ""<a href=""https://en.wikipedia.org/wiki/Native_advertising"" target=""_blank"">Native advertising</a>"" is a popular alternative to the unsightly banner ads and infuriating pop-ups of Internet Advertising 1.0.&nbsp;Native ads mimic the core&nbsp;content of the site they're advertising on, ideally avoiding any interruption of&nbsp;the&nbsp;user's experience.</p>
<h2><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4493/media/dato_banner3.png"" style=""display: block; margin-left: auto; margin-right: auto""></h2>
<p>When native advertising is done right, users aren't desperately scanning an ad for a hidden ""x"". In fact,&nbsp;they don't even know they're viewing one.&nbsp;To pull this off, native ads need to be just as&nbsp;interesting, fun, and informative as the unpaid content on a&nbsp;site.</p>
<p><a href=""https://dato.com/"" target=""_blank"">Dato</a>&nbsp;is sponsoring this competition with the noble goal of making native advertising live up to its name.&nbsp;With a dataset of over 300,000 raw HTML files containing text, links, and downloadable images, they&nbsp;also want to give Kagglers a challenge&nbsp;that encourages creativity. Given the HTML of websites&nbsp;served to users of&nbsp;StumbleUpon, your challenge is to identify the&nbsp;paid content disguised as just another internet gem&nbsp;you've stumbled upon.</p>
<p>If media companies can&nbsp;better&nbsp;identify poorly designed native ads, they can keep them off your feed and out of your user experience.&nbsp;</p>
<p>For details on using GraphLab Create for the competition, check out <a href=""http://blog.dato.com/kaggle-competition-stumbleupon-and-dato-ask-whats-truly-native"" target=""_blank"">Dato's post</a>.</p>
<h2>Acknowledgements&nbsp;&nbsp;</h2>
<p>The dataset for this competition was generously provided by <a href=""http://www.stumbleupon.com/"" target=""_blank"">StumbleUpon</a>.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4493/media/stumbleupon_logo.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are judged on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"" target=""_blank"">area under the ROC curve</a>.</p>
<h2>Submission File</h2>
<p>Each line of your submission should contain an Id and a prediction of the probability that this file is sponsored. Your submission file must have a header row. The file should have the following format:</p>
<pre>file,sponsored<br>1923087_raw_html.txt,0.1<br>2216383_raw_html.txt,0.7<br>...<br>etc</pre>"
Flavours of Physics: Finding τ  →  μμμ,Identify a rare decay phenomenon,https://www.kaggle.com/competitions/flavours-of-physics,,"Physics,Binary Classification,Tabular",673,706,10124,"<p>Like last year's&nbsp;<a href=""https://www.kaggle.com/c/higgs-boson"" target=""_blank"">Higgs Boson Machine Learning Challenge</a>,&nbsp;this competition&nbsp;deals with the &nbsp;physics at the <a href=""http://home.web.cern.ch/topics/large-hadron-collider"" target=""_blank"">Large Hadron Collider (LHC)</a>.&nbsp;However, the subject of last year's challenge, the&nbsp;Higgs Boson, was already known to exist. The aim of this year's challenge is to find a phenomenon that is not already known to exist – charged lepton flavour violation – thereby helping to establish ""<a href=""https://en.wikipedia.org/wiki/Physics_beyond_the_Standard_Model"" target=""_blank"">new physics</a>"".&nbsp;</p>
<h3>Flavours of Physics 101</h3>
<p>The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From <a href=""https://en.wikipedia.org/wiki/Noether%27s_theorem"">Noether’s theorem</a>, we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum.</p>
<p>Symmetries are also crucial to the structure of the <a href=""https://en.wikipedia.org/wiki/Standard_Model"" target=""_blank"">Standard Model</a>&nbsp;of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity.</p>
<p><a href=""https://en.wikipedia.org/wiki/Standard_Model"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/standardmodel2.png"" style=""display: block; margin-left: auto; margin-right: auto""></a></p>
<p>Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn’t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is <em>τ → μμμ</em> (or τ → 3μ). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics.</p>
<h3>Competition Design</h3>
<p>You will be working with&nbsp;real data from the <a href=""http://lhcb-public.web.cern.ch/lhcb-public/"" target=""_blank"">LHCb experiment</a>&nbsp;at the LHC, mixed with simulated datasets of the decay. The metric used&nbsp;in this challenge includes checks that physicists do in their analysis to make sure the&nbsp;results are unbiased. These&nbsp;checks have been built into the&nbsp;competition design to help ensure that&nbsp;the results will be useful for physicists in&nbsp;future studies.&nbsp;</p>
<p>To get started, review the&nbsp;<a href=""https://www.kaggle.com/c/flavours-of-physics/data"" target=""_blank"">Data Page</a>, and be sure to download&nbsp;the <a href=""https://www.kaggle.com/c/flavours-of-physics/details/starter-kit"" target=""_blank"">Starter Kit</a>. The Starter Kit&nbsp;will help you to get used to the unique submission procedure for this competition.</p>
<h3>Competition Video Tutorial</h3>
<p>You've got lots of questions.&nbsp;Researchers at CERN &amp; LCHb have the answers. </p>
<p>- What is the goal of this competition? (1:56)<br> - Why is finding τ → μμμ exciting? (2:18)<br> - What are flavours? (4:10)<br> - Why use machine learning to find τ → μμμ? (4:57)<br> - How did you decide on the size of the dataset? (5:31)<br> - Why is weighted AUC the evaluation metric? (6:09)<br> - Why use Ds → φπ data for the Agreement Test? (7:53)<br> - Why do we need a Correlation Check? (8:44)<br> - How will the competition results impact what you do? (11:38)<br> - How will the competition results be used at CERN? (12:17)</p>
<h3>Resources</h3>
<p><a href=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/lhcb_description_official.pdf"" target=""_blank"">Flavour of Physics, Research Documentation</a></p>
<p><a href=""http://arxiv.org/pdf/1409.8548.pdf"" target=""_blank"">Roel Aaij et al., Search for the lepton flavour violating decay τ → µµµ, 2015, JHEP, 1502:121, 2015</a></p>
<p><a href=""http://iopscience.iop.org/1748-0221/10/03/T03002/pdf/1748-0221_10_03_T03002.pdf"" target=""_blank"">New approaches for boosting to uniformity</a></p>
<h3>Acknowledgements</h3>
<p>This competition is brought to you by:&nbsp;</p>
<p><a href=""http://home.web.cern.ch/"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/CERN_logo156.png""></a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=""http://lhcb-public.web.cern.ch/lhcb-public/"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/lhcb_logo150.png""></a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<a href=""https://yandexdataschool.com/"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/schoolofdata_logo.png""></a></p>
<p><a href=""https://yandexdatafactory.com"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/datafactory_logo.png"" alt=""""></a></p>
<p>Co-sponsored by:</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/intel_logo.png"" alt=""""></p>
<p>Additional support from:&nbsp;</p>
<p><a href=""https://www.uzh.ch/index_en.html"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/zurich_logo.png"" alt=""""></a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=""http://www2.warwick.ac.uk/"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/univwarwick_logo.png"" alt=""""></a>&nbsp; &nbsp;</p>
<p><a href=""http://www.ifj.edu.pl/"" target=""_blank"">&nbsp;<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/ifj_logo75.png"" alt=""""></a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=""http://www.hse.ru/en/"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/higherschoolec_logo75.png"" alt=""""></a></p>


","<p>The evaluation metric for this competition is Weighted Area Under the ROC Curve.&nbsp;The&nbsp;ROC curve is divided into sections based on the True Positive Rate (TPR).&nbsp;To calculate the total area, multiply&nbsp;the area with TPR&nbsp;in [0., 0.2] by&nbsp;weight 2.0, the area with TPR&nbsp;in [0.2, 0.4] by 1.5, the area with TPR&nbsp;[0.4, 0.6] with weight 1.0, and the area with TPR [0.6, 0.8] with weight 0.5. Anything above a TPR of 0.8&nbsp;has weight 0.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/roc_optimistic.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>These weights were chosen to match the evaluation methodology used by CERN scientists.&nbsp;Note that the weighted AUC is calculated only for events (simulated signal events for tau-&gt;µµµ and real background events for tau-&gt;µµµ) with min_ANNmuon &gt; 0.4 (see details in <a href=""https://storage.googleapis.com/kaggle-competitions/kaggle/4488/media/lhcb_description_official.pdf"">section 2.2 Physics background</a>).</p>
<p>Before your predictions are scored with weighted AUC, they also must pass two addition checks: first an <a href=""https://www.kaggle.com/c/flavours-of-physics/details/agreement-test"">agreement test</a>&nbsp;and then the <a href=""https://www.kaggle.com/c/flavours-of-physics/details/correlation-test"">correlation test</a>. Please refer to their respective pages to learn about these tests and what is needed to pass them.</p>
<h2>Submission File</h2>
<p><strong>For every event&nbsp;in the dataset</strong>, submission files should contain two columns: <code>id&nbsp;and prediction</code>. The <code>prediction</code> should be a floating point value between 0 and 1.0, indicating the probability that this event is&nbsp;τ → 3μ decay.&nbsp;</p>
<p>The file should contain a header and have the following format:</p>
<pre>id,prediction<br>14711831,0.3<br>16316387,0.3<br>6771382,0.3<br>686045,0.3<br>8755882,0.3<br>10247299,0.3<br>etc.</pre>"
Coupon Purchase Prediction,Predict which coupons a customer will buy,https://www.kaggle.com/competitions/coupon-purchase-prediction,,"Multiclass Classification,Marketing,Tabular",1072,1188,18390,"<p>Recruit&nbsp;<a href=""http://ponpare.jp/"" target=""_blank"">Ponpare</a>&nbsp;is Japan's leading joint&nbsp;coupon site, offering huge discounts on&nbsp;everything from hot yoga, to gourmet sushi, to a summer concert bonanza. Ponpare's coupons open doors for customers they've only dreamed of stepping through. They can learn difficult to acquire&nbsp;skills, go on unheard of adventures, and dine like (and with) the stars.</p>
<p>Investing in a new experience is not cheap. We fear wasting our time and money on a product or service that we may not enjoy or fully understand. Ponpare takes the high price out of this equation, making it easier for you to take the leap towards your first sky-dive or&nbsp;diamond engagement ring.</p>
<p>Using past purchase and browsing behavior, this competition asks you to predict which coupons a customer will buy in a given period of time. The resulting models will be used to improve Ponpare's recommendation system, so they can make sure their customers don't miss out on their next favorite thing.</p>
<p><img style=""display: block; margin-left: auto; margin-right: auto"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4481/media/recruit_image.png"" alt=""Ponpare""></p>","<p>Submissions are evaluated according to the Mean Average Precision @ 10 (MAP@10):</p>
<p>$$MAP@10 = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{min(m, 10)} \sum_{k=1}^{min(n,10)} P(k)$$&nbsp;</p>
<p>where |U| is the number of users, P(k) is the precision at cutoff k, n is the number of predicted coupons, and m is the number of purchased coupons for the given user. If m = 0, the precision is defined to be 0.</p>
<h2>Submission File</h2>
<p>For every user, you must predict a space-delimited list of the coupons they purchased. The file should contain a header and have the following format (we have substituted&nbsp;the coupon hashes with dummy values to fit below, but in your prediction file you should use the real hash values):</p>
<pre>USER_ID_hash,PURCHASED_COUPONS<br>0004901ba699a49fd93a3c6bb1768b8f,hash4<br>0006d6ac7c6ef3fc0ab0dc40deb3c960,hash1 hash2<br>00078d03b4dda619293c1793c251f783,<br>etc...</pre>"
Liberty Mutual Group: Property Inspection Prediction,Quantify property hazards before time of inspection,https://www.kaggle.com/competitions/liberty-mutual-group-property-inspection-prediction,,Housing,2232,2358,45786,"<p>A Fortune 100 company, <a href=""https://www.libertymutual.com/"" target=""_blank"">Liberty Mutual Insurance</a>&nbsp;has provided a wide range of insurance products and services designed to meet their&nbsp;customers' ever-changing needs for over 100 years.</p>
<p>To ensure that Liberty Mutual’s portfolio of home insurance policies aligns with their&nbsp;business goals, many newly insured properties receive a home inspection. These inspections review the condition of key attributes of the property, including things like the foundation, roof, windows and siding.&nbsp;The results of&nbsp;an inspection help Liberty Mutual determine&nbsp;if the property is one they want to insure.</p>
<p>In this challenge, your task is to predict a transformed count of hazards or pre-existing damages using a dataset of property information. This will enable Liberty Mutual to more accurately identify high risk homes&nbsp;that&nbsp;require additional examination&nbsp;to confirm their insurability.</p>
<p><img style=""display: block; margin-left: auto; margin-right: auto"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4471/media/houses.png""></p>
<p>Liberty Mutual is interested in hiring predictive modelers like you to work on one of many growing analytics teams within our company. As a member of Liberty Mutual’s advanced analytics community, you will have the opportunity to apply sophisticated, cutting-edge techniques, similar to those used in this competition, to large data sets in departments such as Actuarial, Product, Claims, Marketing, Distribution, Human Resources, and Finance.&nbsp;<a href=""http://www.jobs.libertymutualgroup.com/search/predictive-model/?extcmp=JobPost%7CKaggle%7C%7C%7CPredictiveModel"" target=""_blank"">Click to view available positions.</a></p>
<p><em>Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the <a href=""https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction/rules"" target=""_blank"">competition rules</a>&nbsp;for the full details. </em></p>","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/Gini"">normalized Gini coefficient</a>.</p>
<p>To calculate the normalized Gini, your predictions are sorted from largest to smallest. This is the only step where the explicit prediction values are used (i.e. only the order of your predictions matters). We then move from largest to smallest, asking ""In the leftmost x% of the data, how much of the observed loss have you accumulated?"" With no model, you expect to accumulate 10% of the loss in 10% of the predictions, so no model (or a ""null"" model) achieves a straight line. The area between your curve and this straight line is the Gini coefficient.</p>
<p>There is a maximum achievable area for a perfect model. The normalized Gini is obtained by dividing the Gini coefficient of your model by the Gini coefficient of a perfect model.</p>
<h2>Submission File</h2>
<p>The file should contain a header and have the following format:</p>
<pre>Id,Hazard<br>6,0<br>7,0<br>8,0<br>etc.</pre>"
Machinery Tube Pricing,Model quoted prices for industrial tube assemblies,https://www.kaggle.com/competitions/machinery-tube-pricing,,"Regression,Manufacturing,Tabular",1320,1448,26239,"Construction machines rely on a complex set of tubes to keep the forklift lifting, the loader loading, and the bulldozer from dozing off. Tubes can vary across a number of dimensions, including base materials, number of bends, bend radius, bolt patterns, and end types.

Tubes come from a variety manufacturers, each having their own unique pricing model. This competition provides detailed tube, component, and annual volume datasets, and challenges you to predict the price a supplier will quote for a given tube assembly.","<p>Submissions are evaluated one the&nbsp;Root Mean Squared Logarithmic Error (RMSLE).&nbsp;The RMSLE is calculated as</p>
<p>\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]</p>
<p>Where:</p>
<ul>
<ul>
<li>\\(n\\) is&nbsp;the number of price quotes&nbsp;in the test set</li>
<li>\\(p_i\\) is your predicted&nbsp;price</li>
<li>\\(a_i\\) is the actual price</li>
<li>\\(\log(x)\\) is the natural&nbsp;logarithm</li>
</ul>
</ul>
<h2>Submission File</h2>
<p>The file should contain a header and have the following format:</p>
<pre>id,cost<br>1,0<br>2,0<br>3,0<br>...</pre>"
Grasp-and-Lift EEG Detection,Identify hand motions from EEG recordings,https://www.kaggle.com/competitions/grasp-and-lift-eeg-detection,,Multiclass Classification,378,451,4399,"<p>Think back to this morning: turning off the alarm, getting dressed, brushing your teeth, making coffee, drinking coffee, and locking the door as you left&nbsp;for work. Now imagine doing all those things again, without the use of your hands.&nbsp;</p>
<p>Patients who have lost hand function due to amputation or&nbsp;neurological disabilities wake up to&nbsp;this reality everyday. Restoring&nbsp;a patient's&nbsp;ability to perform these&nbsp;basic activities of daily life with a <a href=""https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface"" target=""_blank"">brain-computer&nbsp;interface&nbsp;(BCI)</a>&nbsp;prosthetic device would greatly increase their independence and&nbsp;quality of life. Currently, there are no realistic, affordable, or low-risk options for neurologically disabled patients to directly control external prosthetics with their brain activity.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4477/media/eeg_banner650.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Recorded from the human scalp, EEG signals are evoked by brain activity.&nbsp;The relationship between brain activity and EEG signals is complex and poorly understood outside of specific laboratory tests. Providing affordable, low-risk, non-invasive BCI devices is dependent on further advancements in interpreting EEG signals.&nbsp;</p>
<p>This competition challenges you to identify when a hand is grasping, lifting, and replacing an object using&nbsp;EEG data that was taken from healthy subjects as they performed&nbsp;these activities.&nbsp;Better understanding the relationship between EEG signals and hand movements is critical to developing a BCI device that would&nbsp;give patients with neurological disabilities the ability to move through the world with greater autonomy.&nbsp;</p>
<h3>Acknowledgements</h3>
<p>This competition is sponsored by the <a href=""http://www.wayproject.eu/"" target=""_blank"">WAY Consortium</a>&nbsp;(Wearable interfaces for hAnd function recoverY; FP7-ICT-288551).</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4477/media/logo_WAY_trasp.png"" style=""display: block; margin-left: auto; margin-right: auto""></p>","<p>Submissions are evaluated on the mean column-wise AUC. That is, the mean of the individual&nbsp;<a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">areas under the ROC curve</a>&nbsp;for each predicted column.</p>
<p>Since the columns span multiple subjects and series, you should submit calibrated probabilities that fall on the same scale.</p>
<h2>Submission File</h2>
<p>You must predict six&nbsp;probabilities for each id in the test set, where each id corresponds to a single time frame. The id is formed by concatenating (subject_series_frame).</p>
<p>The file should contain a header and have the following format:</p>
<pre>id,HandStart,FirstDigitTouch,BothStartLoadPhase,LiftOff,Replace,BothReleased<br>subj1_series9_0,0,0,0,0,0,0<br>subj1_series9_1,0,0,0,0,0,0<br>subj1_series9_2,0,0,0,0,0,0<br>etc.</pre>"
San Francisco Crime Classification,Predict the category of crimes that occurred in the city by the bay ,https://www.kaggle.com/competitions/sf-crime,,"Multiclass Classification,Tabular,Crime",2331,2859,18370,"<p>From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of <a href=""http://en.wikipedia.org/wiki/Alcatraz_Island"" target=""_blank"">Alcatraz</a>.</p>
<p>Today, the city is known more for its tech scene than its criminal past. But,&nbsp;with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity&nbsp;of crime in the city by the bay.</p>
<p>From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods.&nbsp;Given time and location, you must predict the category of crime that occurred.</p>
<p>We're also encouraging you to explore the dataset visually. What can we learn about the city through visualizations like this <a href=""https://www.kaggle.com/benhamner/sf-crime/san-francisco-top-crimes-map"" target=""_blank"">Top Crimes Map</a>? The top most up-voted <a href=""https://www.kaggle.com/c/sf-crime/scripts"" target=""_blank"">scripts</a>&nbsp;from this competition will receive official Kaggle swag as prizes.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4458/media/sfcrime_banner.png"" alt=""""></p>
<h2>Acknowledgements</h2>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This&nbsp;dataset is brought to you by <a href=""https://data.sfgov.org/"" target=""_blank"">SF OpenData</a>,&nbsp;the central clearinghouse for data published by the City and County of San Francisco.</p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"">multi-class logarithmic loss</a>. Each incident&nbsp;has been labeled with one true class. For each incident, you must submit a set of predicted probabilities (one for every class). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of cases&nbsp;in the test set, M is the number of class labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).</p>
<p>The submitted probabilities for a given incident&nbsp;are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission Format</h2>
<p>You must submit a csv file with the incident&nbsp;id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>Id,ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS<br>0,0.9,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0<br>1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1<br>...<br>etc.</pre>"
Avito Context Ad Clicks,Predict if context ads will earn a user's click ,https://www.kaggle.com/competitions/avito-context-ad-clicks,,"Tabular,Marketing",413,455,5939,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4438/media/target_avito300.png"" alt="""" style=""float: right""></p>
<p>In Russia, if you're looking to sell a tractor, a designer dress, a vintage lunchbox,&nbsp;or even a house, your first stop will likely&nbsp;be <a href=""https://www.avito.ru/"" target=""_blank"">Avito.ru</a>. As the largest general classified website in Russia, Avito connects buyers and sellers across the world's biggest country.</p>
<p>Sellers&nbsp;are highly motivated to place ads on Avito, hoping to gain attention from&nbsp;the site's 70 million unique monthly visitors.&nbsp;There are three different types of ads available to sellers on Avito: regular, highlighted, and context.&nbsp;</p>
<p>Context ads are seen as the best way to target users with goods and services.&nbsp;Currently, Avito uses general statistics on ad performance to drive the placement of context ads. Their existing&nbsp;model ignores individual user behavior, making it difficult to&nbsp;predict which ad will&nbsp;be the most relevant&nbsp;for (and earn the most clicks from) each potential buyer.&nbsp;</p>
<p>In this competition, Avito is challenging you to improve on their model by&nbsp;predicting if&nbsp;individual users will click a given context ad. <strong>To create&nbsp;the most robust model and fun competition possible, Avito has provided&nbsp;eight comprehensive relational datasets for you to explore.&nbsp;</strong>This competition will help Avito more accurately predict click-through rates for their ads, creating a world where both buyers and sellers win.</p>","<p>Submissions are scored on the log loss, also called the predictive binomial deviance:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<p>n is the number of rows in sampleSubmissions, each row represents one impression, which is a unique combination of an ad and a search&nbsp;<br>\\( \hat{y}_i \\) is the predicted probability of this ad being clicked<br>\\( y_i \\) is 1 if the actual ad's IsClick = 1, 0 if&nbsp;IsClick = 0<br>\\( log() \\) is the natural (base e) logarithm<br>A smaller log loss is better.</p>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value of \\(1.0 * 10 ^ {-15} \\).</p>
<h2>Submission File</h2>
<p>The file should contain a header and have the following format:</p>
<pre>TestId,IsClick<br>0,0<br>1,0<br>5,0<br>7,0<br>9,0<br>etc.</pre>"
Denoising Dirty Documents,Remove noise from printed text,https://www.kaggle.com/competitions/denoising-dirty-documents,,Image,161,167,798,"<div style=""background-image: url(&quot;https://storage.googleapis.com/kaggle-competitions/kaggle/4406/media/coffeestain50.png&quot;); background-repeat: no-repeat; background-position: top"">
<p><a href=""http://en.wikipedia.org/wiki/Optical_character_recognition"" target=""_blank"">Optical Character Recognition</a>&nbsp;(OCR) is the process of getting type or handwritten documents&nbsp;into&nbsp;a digitized format. If you've read a classic novel on&nbsp;a digital reading device or had your doctor pull up old healthcare records via the hospital computer system, you've probably benefited from OCR.</p>
<p>OCR makes previously static content editable, searchable, and much easier to share. But, a lot of documents eager for digitization&nbsp;are being held&nbsp;back. Coffee stains, faded sun spots, dog-eared pages, and lot of wrinkles are keeping some printed documents offline and in the past.&nbsp;</p>
<p>This competition challenges you to give these documents a machine learning makeover. Given a dataset of images of scanned&nbsp;text that has&nbsp;seen better days, you're challenged&nbsp;to remove the noise. Improving the ease of&nbsp;document enhancement&nbsp;will help us get that&nbsp;rare mathematics book on our e-reader before the&nbsp;next beach vacation.</p>
<p>We've kicked off&nbsp;the fun with a few&nbsp;<a href=""https://www.kaggle.com/c/denoising-dirty-documents/scripts"">handy scripts to get you started on the dataset</a>.</p>
<h3>Acknowledgements</h3>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by RM.J. Castro-Bleda, S. España-Boquera, J. Pastor-Pellicer, F. Zamora-Martinez. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:</p>
<p><em>Bache, K. &amp; Lichman, M. (2013). <a href=""http://archive.ics.uci.edu/ml/citation_policy.html"" target=""_blank"">UCI Machine Learning Repository</a>. Irvine, CA: University of California, School of Information and Computer Science</em></p>
</div>","<p>Submissions are evaluated on the <a href=""https://www.kaggle.com/wiki/RootMeanSquaredError"">root mean squared error</a>&nbsp;between the cleaned pixel intensities&nbsp;and the actual grayscale pixel intensities.</p>
<h2>Submission File</h2>
<p>Form the submission file by melting each images into a set of pixels, assigning each pixel an id of&nbsp;image_row_col (e.g. 1_2_1 is image 1, row 2, column 1). Intensity values range from 0 (black) to 1 (white). The file should contain a header and have the following format:</p>
<pre>id,value<br>1_1_1,1<br>1_2_1,1<br>1_3_1,1<br>etc.</pre>"
ICDM 2015: Drawbridge Cross-Device Connections,Identify individual users across their digital devices,https://www.kaggle.com/competitions/icdm-2015-drawbridge-cross-device-connections,,"Multiclass Classification,Tabular",338,405,2353,"<p>Imagine you're planning a summer holiday to Iceland: you read a travel blog on your smartphone on the subway to work, search for hotels on your laptop during lunch, browse Reykjavik restaurants on a&nbsp;tablet while half-watching TV after dinner, and then download a travel book to&nbsp;your e-reader to skim before bed.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4453/media/multichannel_banner.png"" alt=""""></p>
<p>As consumers move across devices to complete online tasks, their identity becomes fragmented. Marketers, hoping to target them&nbsp;with meaningful messages, recommendations, and customized experiences, aren't always able to discern&nbsp;when activity on different devices is tied to one user vs. many users.</p>
<p>Given usage&nbsp;data and a set of fabricated non-personally-identifiable&nbsp;IDs, this competition tasks you with making&nbsp;individual user connections across a variety of&nbsp;digital devices. Improving marketers' ability to identify individual users&nbsp;as they switch between&nbsp;devices means you'll see relevant messages wherever you go, making it easy for you to plan the&nbsp;best, most fjord-filled&nbsp;trip ever.&nbsp;</p>
<h2>Acknowledgements</h2>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4453/media/ICMD_sponsors.png"" alt=""""></p>
<p>The competition dataset and prize pool have been generously provided by <a href=""http://www.drawbrid.ge/"" target=""_blank"">Drawbridge</a>&nbsp;in sponsorship&nbsp;of the <a href=""http://icdm2015.stonybrook.edu/"" target=""_blank"">ICDM 2015</a>&nbsp;conference.&nbsp;</p>","<p>Submissions will be evaluated based on their mean \\(F_{0.5}\\) score.&nbsp;The F score, commonly used in information retrieval, measures accuracy&nbsp;using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all&nbsp;actual positives (tp + fn). The \\(F_{0.5}\\)&nbsp;score is given by</p>
<p>$$<br>(1 + \beta^2)&nbsp;\frac{pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 0.5.<br>$$</p>
<p>Note that the \\(F_{0.5}\\) score&nbsp;weights precision higher than recall. The mean&nbsp;\\(F_{0.5}\\) score is formed by averaging the individual \\(F_{0.5}\\) scores for each row in the test set.</p>
<h2>Submission File</h2>
<p>For each device listed in the test set, predict a space-delimited list of cookie_ids which you believe are associated with the device. The file should contain a header and have the following format:</p>
<pre>device_id,cookie_id<br>id_1,id_10<br>id_100002,id_10 id_20 id_30<br>id_1000035,id_10 id_20<br>etc.</pre>"
Crowdflower Search Results Relevance,Predict the relevance of search results from eCommerce sites,https://www.kaggle.com/competitions/crowdflower-search-relevance,,"Tabular,Internet",1324,1422,23177,"<p>So many of our favorite daily activities are mediated by proprietary search algorithms.&nbsp;Whether you're trying to find a stream of&nbsp;that reality&nbsp;TV show on cat herding or&nbsp;shopping an eCommerce site for a new set of Japanese sushi knives, the relevance of search results is often responsible for your (un)happiness.&nbsp;Currently, small online businesses have no good way of evaluating the performance of their search algorithms, making it difficult for them to provide an exceptional customer experience.</p>
<p>The goal of this competition is to create an open-source model&nbsp;that can be used to measure&nbsp;the relevance of search results. In doing so, you'll be helping enable&nbsp;small business owners to match the experience provided by&nbsp;more resource rich competitors. It will also&nbsp;provide more established&nbsp;businesses a model&nbsp;to test against.&nbsp;Given the queries and resulting&nbsp;product descriptions&nbsp;from leading&nbsp;eCommerce&nbsp;sites, this competition asks you to evaluate the&nbsp;accuracy of their search algorithms.</p>
<p><em>Make a first submission with this <a href=""https://www.kaggle.com/users/993/ben-hamner/crowdflower-search-relevance/python-benchmark"" target=""_blank"">Python benchmark</a>&nbsp;on <a href=""https://www.kaggle.com/c/crowdflower-search-relevance/scripts"" target=""_blank"">Kaggle scripts</a>.&nbsp;</em></p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4407/media/cfbanner.png""></p>
<p><em>The dataset for this competition was created using query-result pairings enriched on the&nbsp;</em><em><a href=""http://www.crowdflower.com/"" target=""_blank"">CrowdFlower</a>&nbsp;platform. They are sponsoring this competition as an investment in the open-source data science community. A dataset collected, cleaned, and labeled by CrowdFlower can make your supervised machine learning dreams come true.</em></p>","<p>Submissions are scored based on the&nbsp;quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the&nbsp;metric may go below 0.&nbsp;The quadratic weighted kappa is calculated between the scores assigned by the human rater and the&nbsp;predicted scores.</p>
<p>Results&nbsp;have 4&nbsp;possible ratings, 1,2,3,4.&nbsp; Each search record&nbsp;is characterized by a tuple <em>(e</em>,<em>e)</em>, which corresponds to its scores by <em>Rater A</em> (human) and <em>Rater B</em> (predicted).&nbsp; The quadratic weighted kappa is calculated as follows. First, an N&nbsp;x&nbsp;N histogram matrix <em>O</em> is constructed, such that <em>O</em> corresponds to the number of search records&nbsp;that received a rating <em>i</em> by<em>&nbsp;A</em> and a rating <em>j</em> by<em>&nbsp;B</em>.&nbsp;An <em>N-by-N </em>matrix of weights, <em>w</em>, is calculated based on the difference between raters' scores:</p>
<p></p>
<p>An <em>N-by-N</em> histogram matrix of expected ratings, <em>E</em>, is calculated, assuming that there is no correlation between rating scores.&nbsp; This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that <em>E</em> and <em>O</em> have the same sum.</p>
<p>From these three matrices, the quadratic weighted kappa is calculated as:&nbsp;</p>
<p></p>
<h2></h2>
<p></p>
<pre>id,prediction<br>1,3<br>3,2<br>4,1<br>5,4<br>etc..</pre>"
Facebook Recruiting IV: Human or Robot?,Predict if an online bid is made by a machine or a human,https://www.kaggle.com/competitions/facebook-recruiting-iv-human-or-bot,,"Binary Classification,Tabular,Internet",983,983,13546,"<p><a href=""https://www.facebook.com/careers/"">Ever wonder what it's&nbsp;like to work at Facebook?</a>&nbsp;Facebook and Kaggle are launching an Engineering competition for 2015. Trail blaze your way to the top of the leader board to earn an opportunity at&nbsp;interviewing for <a href=""https://www.facebook.com/careers/department?req=a0IA000000G34SMMAZ&amp;dept=engineering&amp;q=machine%20learning"">a role</a>&nbsp;as a&nbsp;software engineer, working on world class Machine Learning problems.&nbsp;</p>
<p>In this competition, you'll be chasing down robots for an online auction site. Human bidders on the site are becoming increasingly frustrated with their inability to win auctions vs. their software-controlled counterparts. As a result, usage from the site's core customer base is plummeting.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4294/media/robot_banner@.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>In order to&nbsp;rebuild customer happiness, the site owners need&nbsp;to eliminate&nbsp;computer generated bidding&nbsp;from their auctions. Their attempt at building a&nbsp;model to identify these&nbsp;bids&nbsp;using behavioral data, including bid frequency over short periods of time, has proven insufficient.&nbsp;</p>
<p>The goal of this competition is to identify online auction bids that are placed by ""robots"", helping the site owners easily flag these users for removal from their site to prevent unfair auction activity.&nbsp;</p>
<p><em><strong>The data in this competition comes from an online platform, not from Facebook.</strong></em></p>
<p><em> Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.&nbsp;</em></p>","<p>Submissions are judged on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"" target=""_blank"">area under the ROC curve</a>.</p>
<h2>Submission File</h2>
<p>Each line of your submission should contain an Id and a prediction of the probability that this bidder is a robot. Your submission file must have a header row. The file should have the following format:</p>
<pre>bidder_id,prediction<br>38d9e2e83f25229bd75bfcdc39d776bajysie,0.3<br>9744d8ea513490911a671959c4a530d8mp2qm,0.0<br>dda14384d59bf0b3cb883a7065311dac3toxe,0.9<br>...<br>etc</pre>"
ECML/PKDD 15: Taxi Trip Time Prediction (II),Predict the total travel time of taxi trips based on their initial partial trajectories,https://www.kaggle.com/competitions/pkdd-15-taxi-trip-time-prediction-ii,,Tabular,345,418,3297,"<p style=""color: rgba(0, 0, 0, 1)"">This is the second of two data science challenges that share the same dataset. The <a href=""https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i"" target=""_blank"">Taxi Service Trajectory competition</a>&nbsp;predicts the final destination of taxi trips.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4383/media/taxibanner7.png""></p>
<p>To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict how long a driver will have his taxi occupied. If a dispatcher knew approximately when a&nbsp;taxi driver would be ending their current ride, they would be better able to identify which driver&nbsp;to assign to each pickup request.&nbsp;</p>
<p>In this challenge, we ask you to build a predictive framework that is able to infer the&nbsp;trip time&nbsp;of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the travel time of a particular taxi trip.</p>
<p><em>This competition&nbsp;is affiliated&nbsp;with&nbsp;the organization of <a href=""http://www.ecmlpkdd2015.org/"">ECML/PKDD 2015</a>.</em></p>
<p><strong><em><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4378/media/Screen%20Shot%202015-04-16%20at%203.43.05%20PM.png"" alt=""""></em></strong></p>","<p>Submissions are evaluated one the&nbsp;Root Mean Squared Logarithmic Error (RMSLE).&nbsp;The RMSLE is calculated as</p>
<p>\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]</p>
<p>Where:</p>
<ul>
<li></li>
<li></li>
<li></li>
<li>logarithm</li>
</ul>
<h2 style=""text-align: justify"">Submission Format</h2>
<p><strong>For every trip in the dataset</strong>, submission files should contain two columns: TRIP_ID and TRAVEL_TIME. TRIP_ID represents the ID of the trip for which you are predicting the total travel time (i.e. a string), while the TRAVEL_TIME column contains your prediction (i.e. a positive integer value containing the travel time in seconds).</p>
<p>The file should contain a header and have the following format:</p>
<pre>TRIP_ID,TRAVEL_TIME<br>T1,60<br>T2,90<br>T3,122<br>etc.</pre>"
West Nile Virus Prediction,Predict West Nile virus in mosquitos across the city of Chicago,https://www.kaggle.com/competitions/predict-west-nile-virus,,"Binary Classification,Tabular",1304,1445,29882,"<p><a href=""http://www.cdc.gov/westnile/"" target=""_blank"">West Nile virus</a>&nbsp;is most commonly spread to humans through infected mosquitos. Around&nbsp;20% of&nbsp;people who become infected&nbsp;with the virus develop symptoms&nbsp;ranging from a&nbsp;persistent fever, to serious neurological illnesses that can&nbsp;result in&nbsp;death.</p>
<p><a href=""https://storage.googleapis.com/kaggle-competitions/kaggle/4366/media/moggie2.png""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4366/media/moggie2.png"" style=""float: right""></a></p>
<p>In 2002,&nbsp;the first human cases of West Nile virus were reported in Chicago.&nbsp;By&nbsp;2004 the City of Chicago and the Chicago Department of Public Health (CDPH) had established a&nbsp;comprehensive surveillance and control program that is still in effect&nbsp;today.</p>
<p>Every week from&nbsp;late spring through the fall, mosquitos in traps across the city are tested for the virus. The results of these tests&nbsp;influence when and where the city will spray airborne pesticides to control adult mosquito populations.</p>
<p>Given weather, location, testing, and spraying data, this competition&nbsp;asks you to predict&nbsp;when and where different species of mosquitos will test positive for West Nile virus.&nbsp;A more accurate method of predicting outbreaks of West Nile virus in mosquitos will help the City of Chicago and CPHD more efficiently and effectively allocate resources&nbsp;towards preventing transmission of this potentially deadly virus.&nbsp;</p>
<p><em>We've jump-started your analysis with some <a href=""https://www.kaggle.com/users/3716/davidchudzicki/predict-west-nile-virus/map-of-mosquito-counts-on-one-day"" target=""_blank"">visualizations</a>&nbsp;and <a href=""https://www.kaggle.com/users/3716/davidchudzicki/predict-west-nile-virus/when-are-there-records-at-each-site"" target=""_blank"">starter code</a>&nbsp;in&nbsp;R and Python on <a href=""https://www.kaggle.com/c/predict-west-nile-virus/scripts"" target=""_blank"">Kaggle Scripts</a>. No data download or local environment setup needed!</em></p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4366/media/chiskyline.png"" alt=""""></p>
<h3>Acknowledgements&nbsp;</h3>
<p>This competition is sponsored by the <a href=""http://www.rwjf.org/"" target=""_blank"">Robert Wood Johnson Foundation</a>.&nbsp;Data is provided by the <a href=""http://www.cityofchicago.org/city/en/depts/cdph.html"" target=""_blank"">Chicago Department of Public Health</a>.</p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability that West Nile Virus&nbsp;is present&nbsp;and the observed outcomes.</p>
<h2>Submission File</h2>
<p>For each record&nbsp;in the test set, you should predict a real-valued probability that WNV is present. The file should contain a header and have the following format:</p>
<pre>Id,WnvPresent<br>1,0<br>2,1<br>3,0.9<br>4,0.2<br>etc.</pre>"
ECML/PKDD 15: Taxi Trajectory Prediction (I),Predict the destination of taxi trips based on initial partial trajectories,https://www.kaggle.com/competitions/pkdd-15-predict-taxi-service-trajectory-i,,Tabular,381,459,3031,"<p>The taxi industry is evolving rapidly. New competitors and technologies are changing the way traditional taxi services do business. While this evolution has created new efficiencies, it has also created new problems.&nbsp;</p>
<p>One major shift is the widespread adoption of electronic dispatch systems that have replaced the VHF-radio dispatch systems of times past.&nbsp;These mobile data terminals are installed in each vehicle and typically provide information on&nbsp;GPS localization and taximeter state. Electronic dispatch systems make it easy to see where a taxi has been, but not necessarily where it is going. In most cases, taxi drivers operating with&nbsp;an electronic dispatch system do not indicate the final destination of their current ride.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4378/media/portugal_map4.png"" alt=""""></p>
<p>Another recent change is the switch from broadcast-based (one to many) radio messages for service dispatching to unicast-based (one to one) messages. With unicast-messages, the dispatcher needs to correctly identify which taxi they should dispatch to a pick up location.&nbsp;Since taxis using electronic dispatch systems do not usually enter their drop off location, it is extremely difficult for dispatchers to know which taxi to contact.&nbsp;</p>
<p>To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service.&nbsp;Particularly during periods of high demand, there is often a taxi whose current ride will end near or exactly at a requested pick up location from a new rider. If a dispatcher knew approximately where their taxi drivers would be ending their current rides, they would be able to identify which taxi to assign to each pickup request.</p>
<p>The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately 25%), the taxi has been called through the taxi call-center, and the passenger’s telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id.</p>
<p>In this challenge, we ask you to build a predictive framework that is able to infer the final destination of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the final trip's destination (WGS84 coordinates).</p>
<p><strong>This is the first&nbsp;of two data science challenges that share the same dataset. The <a href=""https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii"" target=""_blank"">Taxi Service Trip Time&nbsp;competition</a>&nbsp;predicts the total time of taxi rides.</strong></p>
<p><em>This competition&nbsp;is affiliated&nbsp;with&nbsp;the organization of <a href=""http://www.ecmlpkdd2015.org/"" target=""_blank"">ECML/PKDD 2015</a>.</em></p>
<p><strong><em><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4378/media/Screen%20Shot%202015-04-16%20at%203.43.05%20PM.png"" alt=""""></em></strong></p>","<p>The evaluation metric for this competition is the <strong>Mean Haversine Distance</strong>. The <a href=""http://en.wikipedia.org/wiki/Haversine_formula"">Haversine Distance</a>&nbsp;is commonly used in navigation. It measures&nbsp;distances between two points on a sphere based on their latitude and longitude.</p>
<p>The Harvesine Distance between the two locations can be computed as follows</p>
<p>\[a=sin^2 \left( \frac{\phi_2-\phi_1}{2} \right)+cos \left( \phi_1 \right)cos \left(\phi_2 \right)sin^2 \left(\frac{\lambda_2-\lambda_1}{2} \right)\]</p>
<p>\[d=2 \cdot r \cdot atan \left( \sqrt{ \frac{a}{1-a}} \right) \]</p>
<p>where \\( \phi \\) is the&nbsp;latitude, \\( \lambda \\) is the longitude,&nbsp;</p>
<p>\\( d\\) is the distance between two points, and \\( r&nbsp;\\) is the sphere's radius,&nbsp;</p>
<p>In our case, it should be replaced by the Earth's radius in the desired metric (e.g., 6371 kilometers).</p>
<h2 style=""text-align: justify"">Submission Format</h2>
<p><strong>For every trip&nbsp;in the dataset</strong>, submission files should contain three&nbsp;columns: TRIP_ID, LATITUDE, and LONGITUDE. TRIP_ID represents the ID of the trip for which you are predicting the destination&nbsp;(i.e. a string). The LATITUDE/LONGITUDE represent the location's coordinates (using <a href=""http://en.wikipedia.org/wiki/World_Geodetic_System"">WGS84 </a>format) of your predicted destination</p>
<p>The file should contain a header and have the following format:</p>
<pre>TRIP_ID, LATITUDE, LONGITUDE<br>T1, 41.146504,-8.611317<br></pre>"
Walmart Recruiting II: Sales in Stormy Weather,Predict how sales of weather-sensitive products are affected by snow and rain,https://www.kaggle.com/competitions/walmart-recruiting-sales-in-stormy-weather,,"Regression,Tabular",484,484,8243,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4332/media/walmartrecruit_banner.png"" alt=""""></p>
<p>Walmart operates&nbsp;11,450 stores in 27 countries, managing inventory across varying climates and cultures. Extreme weather events, like hurricanes, blizzards, and floods, can have a huge impact on sales at the store and product level.&nbsp;</p>
<p>In their second Kaggle recruiting competition, Walmart challenges participants to accurately&nbsp;predict the sales of 111 potentially weather-sensitive&nbsp;products (like umbrellas, bread, and milk) around the time of major weather events at 45 of their retail locations.&nbsp;</p>
<p>Intuitively, we may expect an uptick in the sales of umbrellas before a big thunderstorm, but it's difficult&nbsp;for replenishment managers to correctly predict the level of inventory needed to avoid being out-of-stock or&nbsp;overstock during and after that storm.&nbsp;Walmart relies on a variety of vendor tools to predict sales around extreme weather events, but it's an ad-hoc and time-consuming&nbsp;process that lacks a systematic measure of effectiveness.&nbsp;</p>
<p>Helping Walmart better predict sales of weather-sensitive products&nbsp;will keep valued customers out of the rain. It could also earn you a position at&nbsp;one of the most data-driven retailers in the world!&nbsp;</p>
<p><em>Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.</em></p>","<p>Submissions are evaluated one the&nbsp;Root Mean Squared Logarithmic Error (RMSLE).&nbsp;The RMSLE is calculated as</p>
<p>\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]</p>
<p>Where:</p>
<ul>
<li>\\(n\\) is&nbsp;the number of&nbsp;rows in the test set</li>
<li>\\(p_i\\) is your predicted units sold</li>
<li>\\(a_i\\) is the actual&nbsp;units sold</li>
<li>\\(\log(x)\\) is the natural&nbsp;logarithm</li>
</ul>
<h2>Submission Format</h2>
<p>Your submission file must have a header and should be structured in the following format. The id column is&nbsp;a triplet representing a store_nbr, item_nbr, and date. Form the id by concatenating these (in that order) with an underscore. E.g. ""2_1_2013-04-01"" represents store 2, item 1, sold on 2013-04-01.</p>
<pre>id,units<br>2_1_2013-04-01,0<br>2_2_2013-04-01,0<br>2_3_2013-04-01,0
...
</pre>"
Restaurant Revenue Prediction,Predict annual restaurant sales based on objective measurements,https://www.kaggle.com/competitions/restaurant-revenue-prediction,,"Regression,Tabular",2257,2459,32745,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4272/media/TAB_banner2.png"" alt=""""></p>
<p>With over 1,200 quick service restaurants across the globe, TFI&nbsp;is the company behind some of the world's most well-known brands: Burger King, Sbarro, Popeyes, Usta Donerci, and Arby’s. They employ over 20,000 people in&nbsp;Europe and Asia and make significant daily investments in developing new restaurant sites.</p>
<p>Right now, deciding when and where to open new restaurants is largely a subjective process based on the personal judgement and experience&nbsp;of&nbsp;development teams. This subjective data&nbsp;is&nbsp;difficult to accurately extrapolate across geographies and cultures.&nbsp;</p>
<p>New restaurant sites take large investments of time and capital to get up and running. When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred.&nbsp;</p>
<p>Finding a mathematical model to increase the effectiveness of investments in new restaurant sites would allow TFI&nbsp;to invest more in other important business areas, like sustainability, innovation, and training for&nbsp;new employees.&nbsp;Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant&nbsp;sales of 100,000 regional locations.</p>
<p><em>TFI&nbsp;would love to hire an expert Kaggler like you to head&nbsp;up their growing data science team in Istanbul or Shanghai. You'd be tackling problems like the one featured in this competition&nbsp;on a global scale.&nbsp;<a href=""https://www.kaggle.com/jobs/12949/tab-food-investments-head-of-data-science-istanbul-or-shanghai"">See the job description here &gt;&gt;</a></em></p>","<h3>Root Mean Squared Error (RMSE)</h3>
<p>Submissions are scored on the root mean squared error. RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:</p>
<p>\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]</p>
<p>where y hat is the predicted value and y is the original value.</p>
<h3>Submission File</h3>
<p><strong>For every restaurant&nbsp;in the dataset</strong>, submission files should contain two columns: Id&nbsp;and Prediction.&nbsp;</p>
<p>The file should contain a header and have the following format:</p>
<pre>Id,Prediction<br>0,1.0<br>1,1.0<br>2,1.0<br>etc.</pre>"
Otto Group Product Classification Challenge,Classify products into the correct category,https://www.kaggle.com/competitions/otto-group-product-classification-challenge,,"Tabular,Internet",3507,3841,43398,"<p><a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13585/share-your-scripts-visualizations/73073#post73073"">Get started on this competition through Kaggle Scripts</a></p>
<p>The Otto Group is one of the world’s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate &amp; Barrel (USA), Otto.de (Germany) and&nbsp;3 Suisses (France). We are selling millions of products worldwide every day, with several thousand products being added to our product line.</p>
<p>A consistent analysis of the performance of our products is crucial. However, due to our diverse global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights we can generate about our product range.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4280/media/Grafik.jpg"" alt=""2nd iteration""></p>
<p>For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. The winning models will be open sourced.</p>","<p>Submissions are evaluated using&nbsp;the multi-class logarithmic loss. Each product&nbsp;has been labeled with one&nbsp;true category. For each product, you must submit a set of predicted probabilities (one for every category). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of products&nbsp;in the test set, M is the number of class labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to&nbsp;class \\(j\\).</p>
<p>The submitted probabilities for a given product&nbsp;are not required to sum to one&nbsp;because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes&nbsp;of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission Format</h2>
<p>You must&nbsp;submit a csv file with the product&nbsp;id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9<br>1,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0<br>2,0.0,0.2,0.3,0.3,0.0,0.0,0.1,0.1,0.0<br>...<br>etc.</pre>"
Diabetic Retinopathy Detection,Identify signs of diabetic retinopathy in eye images,https://www.kaggle.com/competitions/diabetic-retinopathy-detection,,"Binary Classification,Image",660,853,6993,"<p><strong><a href=""http://en.wikipedia.org/wiki/Diabetic_retinopathy"">Diabetic retinopathy</a>&nbsp;is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.</strong></p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4104/media/retina.jpg"" alt=""retina"" style=""display: block; margin-left: 0; margin-right: 15px; float: left""></p>
<p>The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment.</p>
<p>Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment.</p>
<p>Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease.&nbsp;While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed.&nbsp;As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient.</p>
<p>The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models&nbsp;will be open sourced to maximize the impact such a model can&nbsp;have on improving DR detection.</p>
<h2>Acknowledgements</h2>
<p>This competition is sponsored by the <a href=""http://www.chcf.org/"">California Healthcare Foundation</a>.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4104/media/chcf.gif"" alt=""""></p>
<p>Retinal images were&nbsp;provided by <a href=""http://eyepacs.com/"">EyePACS</a>, a free platform for retinopathy screening.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4104/media/eyepacs.png"" alt=""""></p>","<p>Submissions are scored based on the&nbsp;quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0.&nbsp;The quadratic weighted kappa is calculated between the scores assigned by the human rater and the&nbsp;predicted scores.</p>
<p>Images&nbsp;have five&nbsp;possible ratings, 0,1,2,3,4.&nbsp; Each image&nbsp;is characterized by a tuple <em>(e</em>,<em>e)</em>, which corresponds to its scores by <em>Rater A</em> (human) and <em>Rater B</em> (predicted).&nbsp; The quadratic weighted kappa is calculated as follows. First, an N&nbsp;x&nbsp;N histogram matrix <em>O</em> is constructed, such that <em>O</em> corresponds to the number of images&nbsp;that received a rating <em>i</em> by<em>&nbsp;A</em> and a rating <em>j</em> by<em>&nbsp;B</em>.&nbsp;An <em>N-by-N </em>matrix of weights, <em>w</em>, is calculated based on the difference between raters' scores:</p>
<p></p>
<p>An <em>N-by-N</em> histogram matrix of expected ratings, <em>E</em>, is calculated, assuming that there is no correlation between rating scores.&nbsp; This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that <em>E</em> and <em>O</em> have the same sum.</p>
<p>From these three matrices, the quadratic weighted kappa is calculated as:&nbsp;</p>
<p></p>
<h2></h2>
<p></p>
<pre></pre>"
Microsoft Malware Classification Challenge (BIG 2015),Classify malware into families based on file content and characteristics,https://www.kaggle.com/competitions/malware-classification,https://storage.googleapis.com/kaggle-competitions/kaggle/4117/logos/header.png?t=2018-12-13-20-11-47,"Multiclass Classification,Text,Internet",377,481,7669,"<h3>March 2018 Update: <br>
when using this dataset, please cite http://arxiv.org/abs/1802.10135</h3>


<p>In recent years, the malware industry has become a well organized market&nbsp;involving&nbsp;large amounts of money. Well funded, multi-player syndicates invest heavily in technologies and capabilities built&nbsp;to evade traditional protection, requiring anti-malware vendors to develop counter mechanisms&nbsp;for finding and deactivating them. In the meantime, they inflict&nbsp;real financial and emotional pain to users of computer systems.<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4117/media/malware_classification.png"">One of the major&nbsp;challenges that anti-malware faces today is the vast amounts of data and files which need to be evaluated&nbsp;for potential malicious intent. For example, Microsoft's real-time detection anti-malware products are present on over&nbsp;160M computers worldwide and inspect over&nbsp;700M computers monthly. This generates&nbsp;tens of millions of daily data points&nbsp;to be analyzed&nbsp;as potential malware. One of the main reasons&nbsp;for these&nbsp;high volumes of different files is the fact that,&nbsp;in order to&nbsp;evade detection, malware authors introduce polymorphism to the malicious components. This means that malicious files belonging to the same malware ""family"", with the same forms of malicious behavior, are constantly modified and/or obfuscated using various tactics, such that they look like many different files.</p>
<p>In order to be effective in analyzing and classifying such large amounts of files, we need to be able to&nbsp;group them into groups and identify their respective families. In addition, such grouping criteria&nbsp;may be applied to new files encountered on computers in order to detect them&nbsp;as malicious and of a certain family.</p>
<p>For this challenge, Microsoft is&nbsp;providing the&nbsp;data science community with an unprecedented malware dataset and encouraging open-source progress on&nbsp;effective techniques for grouping variants of malware files into&nbsp;their respective families.</p>
<h2>Acknowledgements</h2>
<p>This competition is hosted by <a href=""http://www.www2015.it/"">WWW 2015</a> /&nbsp;<a href=""http://wwwusers.di.uniroma1.it/~mei/BIG2015/Home.html"">BIG 2015</a> and the following Microsoft groups: <a href=""http://www.microsoft.com/security/portal/mmpc/"">Microsoft Malware Protection Center</a>,&nbsp;<a href=""http://azure.microsoft.com/en-us/services/machine-learning/"">Microsoft Azure Machine Learning</a> and Microsoft Talent Management. </p>
<p>Microsoft contacts: Dr. Royi Ronen (royir@microsoft.com) and Corina Feuerstein (corinaf@microsoft.com) </p>
<p><a href=""http://www.www2015.it/""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4117/media/cl80N_H1.png"" alt=""www logo""></a></p>","<p>Submissions are evaluated using&nbsp;the multi-class logarithmic loss. Each file&nbsp;has been labeled with one&nbsp;true class. For each file, you must submit a set of predicted probabilities (one for every&nbsp;class):</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of files&nbsp;in the test set, M is the number of labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to&nbsp;class \\(j\\).</p>
<p>The submitted probabilities for a given file&nbsp;are not required to sum to one&nbsp;because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes&nbsp;of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h3>Submission Format</h3>
<p>For every file in the test set, submission files should contain&nbsp;10 columns:</p>
<ol>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
<p>The file should contain a header and have the following format:</p>
<pre>Id,Prediction1,Prediction2,...,Prediction9<br>02IOCvYEy8mjiuAQHax3,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1<br>02K5GMYITj7bBoAisEmD,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1<br>02zcUmKV16Lya5xqnPGB,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1<br>03nJaQV6K2ObICUmyWoR,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1<br>04EjIdbPV5e1XroFOpiN,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1<br><br>.....<br><br></pre>"
March Machine Learning Mania 2015,Predict the 2015 NCAA Basketball Tournament,https://www.kaggle.com/competitions/march-machine-learning-mania-2015,,"Basketball,Tabular,Sports",340,404,611,"<p>At Kaggle HQ and in offices across the country, March is a month when bracketology is in bloom. Back by popular demand, our second annual March Machine Learning Mania competition pits you against the millions of sports fans and office-pool bandwagoners&nbsp;who are hoping to win big by correctly predicting the outcome of the men's <a href=""http://www.ncaa.com/march-madness"">NCAA basketball&nbsp;tournament</a>.&nbsp;</p>
<p>While the odds of forecasting&nbsp;a perfect bracket are astronomical, these odds are improved&nbsp;by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media.</p>
<p>How well can machine learning and statistical techniques improve the forecast? Presented by HP Software's industry leading Big Data group and the&nbsp;<a href=""http://www8.hp.com/us/en/software-solutions/big-data-platform-haven/index.html"" target=""_blank"">HP Haven Big Data platform</a>, this competition will test how well predictions based on data stack up against a (jump) shot in the dark.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4066/media/20150130_MarchMad_627x349.jpg""></p>
<p>This competition allows&nbsp;you to get creative with the datasets you use to create your model. We provide&nbsp;data covering three&nbsp;decades of historical games, but you're highly encouraged to pull in data from external sources.&nbsp;</p>
<p>The&nbsp;<a href=""https://www.idolondemand.com/docs/api-overview.html"" target=""_blank"">50+ REST APIs</a> from <a href=""http://www.idolondemand.com/"" target=""_blank"">HP IDOL OnDemand</a>&nbsp;are a great way to get started&nbsp;augmenting&nbsp;the&nbsp;dataset. Developer accounts are free and includes free monthly quota! Begin by&nbsp;extracting trending topics and&nbsp;<a href=""https://www.idolondemand.com/developer/apis/extractentities#overview"" target=""_blank"">identifying entities</a>&nbsp;from the IDOL OnDemand news dataset&nbsp;(accessed via the <a href=""https://www.idolondemand.com/developer/apis/querytextindex#overview"" target=""_blank"">Query Text Index API</a>) or by&nbsp;analyzing public&nbsp;<a href=""https://www.idolondemand.com/developer/apis/analyzesentiment#overview"" target=""_blank"">sentiment</a> about players and teams using data from&nbsp;your social media feed.&nbsp;</p>
<p>In stage one of this two-stage competition, participants will build and test their models against the previous four&nbsp;tournaments. In the second stage, participants will predict the outcome of the 2015 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2015 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket.&nbsp;HP is sponsoring $15,000 in cash prizes for the winners.</p>
<p>Please visit the <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2015/details/faqs"">FAQs</a> for more information.</p>
<h3>Acknowledgements</h3>
<p>March Machine Learning Mania 2015&nbsp;is presented by HP.&nbsp;Please see <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2015/details/about-the-sponsor"" target=""_blank"">About the sponsor</a>&nbsp;to read&nbsp;more.</p>
<p><img alt="""" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4066/media/hplogo.jpg""></p>","<p>Submissions are scored on the log loss, also called the predictive binomial deviance:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2015 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 &nbsp;= 2278 matchups.&nbsp;</p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104&nbsp;played team 1129&nbsp;in the year 2013. You must&nbsp;predict the probability that the team with the lower id beats the team with the higher id.</p>
<p>The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br>2011_1103_1106,0.5<br>2011_1103_1112,0.5<br>2011_1103_1114,0.5<br>...<br>...</pre>"
How Much Did It Rain?,Predict probabilistic distribution of hourly rain given polarimetric radar measurements,https://www.kaggle.com/competitions/how-much-did-it-rain,,Tabular,319,349,2216,"<p>For agriculture, it is extremely important to know how much it rained on a particular field. However, rainfall is variable in space and time and it is impossible to have rain gauges everywhere. Therefore, remote sensing instruments such as radar are used to provide wide spatial coverage. Rainfall estimates drawn from remotely sensed observations will never exactly match the measurements that are&nbsp;carried out using rain gauges, due to the inherent characteristics of both sensors. Currently, radar observations are ""corrected"" using nearby gauges and a single estimate of rainfall is provided to users who need to know how much it rained. This competition&nbsp;will explore how to address this problem in a probabilistic manner. &nbsp;Knowing the full probabilistic spread of rainfall amounts can be very useful to drive hydrological and agronomic models -- much more than a single estimate of rainfall.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4195/media/dual_pol2.jpg"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<h6 style=""text-align: center"">Image courtesy of&nbsp;<a href=""http://www.roc.noaa.gov/WSR88D/Images/dual_pol2.jpg"">NOAA</a></h6>
<p>Unlike a conventional Doppler radar, a polarimetric radar transmits&nbsp;radio wave pulses that have both horizontal and vertical orientations. Because rain drops become flatter as they increase in size and because ice crystals tend to be elongated vertically, whereas liquid droplets tend to be flattened, it is possible to infer the size of rain drops and the type of hydrometeor from the differential reflectivity of the two orientations.</p>
<p>In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total. More details are on&nbsp;the&nbsp;data page.</p>
<p>This competition is sponsored by the <a href=""http://www2.ametsoc.org/stac/index.cfm/committees/committee-on-artificial-intelligence-applications-to-environmental-science/"">Artificial Intelligence Committee</a>&nbsp;of the American Meteorological Society. The <a href=""http://www.climate.com"">Climate Corporation</a>&nbsp;has kindly agreed to&nbsp;sponsor the prizes.</p>","<p>The winning entry will be the one that minimizes the&nbsp;Continuous Ranked Probability Score:</p>
<p>\[&nbsp;C = \frac{1}{70N} \sum_{N} \sum_{n=0}^{69} (P(y \le n) -H(n -z))^2&nbsp;\]</p>
<p>over the testing dataset (of size N) where z is the actually recorded gauge value (in mm) and&nbsp;H(x) is the Heavyside step function i.e. H(x) = 1 for \[x \ge 0\] and zero otherwise.&nbsp;The entry will be discarded if any of the answers has \[P(y \le k) &gt; P(y \le k+1)\] for any k,&nbsp;i.e., the CDF has to be non-decreasing.</p>
<h2>Submission Instructions</h2>
<p>The submission file specifies for each location (with Id), the predicted cumulative probabilities between 0 and 69 mm (both inclusive). There are 70 columns of probabilities for each Id.&nbsp;</p>
<pre style=""white-space: pre-wrap"">Id,Predicted0,Predicted1,Predicted2,Predicted3,...,Predicted69<br>1,0.493069074336,0.725572625942,0.87785520942,0.951305748155,...,1.0<br>2,0.5,0.73105857863,0.880797077978,0.952574126822,...,1.0<br>...<br>etc</pre>"
National Data Science Bowl,"Predict ocean health, one plankton at a time",https://www.kaggle.com/competitions/datasciencebowl,,"Multiclass Classification,Water Bodies,Image",1049,1293,15120,"<p>Plankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.</p>
<p><a href=""https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/Plankton-Diagram3-lg.png"" target=""_blank""><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/Plankton-Diagram3.png"" alt=""""></a></p>
<p>Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions.</p>
<p>Manual analysis of the imagery is infeasible&nbsp;– it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health.</p>
<p>The National Data Science Bowl challenges you to build an algorithm to automate the image identification process. Scientists at the Hatfield Marine Science Center and beyond will use the algorithms you create to study marine food webs, fisheries, ocean conservation, and more. This is your chance to contribute to the&nbsp;health of the world’s oceans, one plankton at a time.</p>
<h2>Acknowledgements</h2>
<p>The National Data Science Bowl&nbsp;is presented&nbsp;by<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/booz_kaggle.png""><br>with data provided by the <a href=""http://hmsc.oregonstate.edu/"">Hatfield Marine Science Center</a>&nbsp;at Oregon State University.</p>","<p>Submissions are evaluated using&nbsp;the multi-class logarithmic loss. Each image has been labeled with one&nbsp;true class. For each image, you must submit a set of predicted probabilities (one for every&nbsp;class). The formula is then,</p>
<p>$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$</p>
<p>where N is the number of images in the test set, M is the number of class labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to&nbsp;class \\(j\\).</p>
<p>The submitted probabilities for a given image are not required to sum to one&nbsp;because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes&nbsp;of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).</p>
<h2>Submission Format</h2>
<p>You must&nbsp;submit a csv file with the image name, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:</p>
<pre>image,acantharia_protist_big_center,...,unknown_unclassified<br>1.jpg,0.00826446,...,0.00826446<br>10.jpg,0.00826446,...,0.00826446<br>...<br>etc.</pre>"
Driver Telematics Analysis,Use telematic data to identify a driver signature,https://www.kaggle.com/competitions/axa-driver-telematics-analysis,,"Multiclass Classification,Tabular",1524,1861,35962,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4031/media/axa_trip.png"" alt="""" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>For automobile insurers, telematics represents a growing&nbsp;and valuable way to quantify driver risk. Instead of pricing&nbsp;decisions on vehicle and driver characteristics, telematics gives&nbsp;the&nbsp;opportunity&nbsp;to measure the quantity and quality of a driver's behavior. This can&nbsp;lead to savings for safe or&nbsp;infrequent drivers, and transition&nbsp;the burden to policies that represent increased&nbsp;liability.</p>
<p>AXA has provided a dataset of over 50,000 anonymized driver trips. The intent of this competition is to develop an algorithmic signature of driving type. Does&nbsp;a&nbsp;driver drive long trips? Short trips? Highway trips? Back roads? Do they accelerate hard from stops? Do they take turns at high speed? The answers to these questions combine to form&nbsp;an aggregate profile that potentially&nbsp;makes each driver unique.</p>
<p>For this competition, Kaggle participants must come up with a ""telematic fingerprint"" capable of distinguishing&nbsp;when a trip was driven by a given driver. The features of this&nbsp;driver fingerprint&nbsp;could&nbsp;help assess&nbsp;risk and form a crucial piece of a&nbsp;larger telematics puzzle.</p>","<p>Submissions are judged on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>. The ROC area is calculated in a&nbsp;global manner&nbsp;(all predictions together). You should therefore aim to submit calibrated probabilities between&nbsp;the drivers.</p>
<h2>Submission File</h2>
<p>You must submit a predicted probability for all possible driver_trip pairs.&nbsp;The resulting submission format looks like like the following, where ""prob"" represents the predicted probability that the trip belongs to the associated driver (1 = the trip belongs to the driver of interest, 0 = the trip does not belong):</p>
<pre>driver_trip,prob<br>1_1,1<br>1_2,1<br>1_3,1<br>...</pre>"
Bag of Words Meets Bags of Popcorn,Use Google's Word2Vec for movie reviews,https://www.kaggle.com/competitions/word2vec-nlp-tutorial,,"Text,Binary Classification,Movies and TV Shows",577,659,4271,"<p>In this tutorial competition, we dig a little ""deeper"" into sentiment analysis. <a href=""https://code.google.com/p/word2vec/"">Google's Word2Vec</a>&nbsp;is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to <a href=""http://google-opensource.blogspot.com/2013/08/learning-meaning-behind-words.html"">understand meaning</a>&nbsp;and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.</p>
<p>Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another <a href=""https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"">Kaggle competition</a>&nbsp;for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.</p>
<p>Deep learning has been in the news a lot over the past few years, even making it to the <a href=""http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html?pagewanted=all&amp;_r=0"">front page of the New York Times</a>. These machine learning&nbsp;techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via&nbsp;breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a <a href=""https://www.kaggle.com/c/MerckActivity"">drug discovery</a>&nbsp;task, and&nbsp;<a href=""https://www.kaggle.com/c/dogs-vs-cats"">cat and dog image recognition.</a></p>
<h2>Tutorial Overview</h2>
<p>This tutorial will&nbsp;help you get started with Word2Vec for natural language processing. It&nbsp;has two goals:&nbsp;</p>
<p><strong>Basic Natural Language Processing</strong>:&nbsp;<strong>Part 1</strong> of this tutorial is intended for beginners and covers basic natural language processing&nbsp;techniques, which are needed for later parts of the tutorial.</p>
<p><strong>Deep Learning for Text&nbsp;Understanding</strong>: In&nbsp;<strong>Parts 2 and 3</strong>, we delve into how to train a model&nbsp;using Word2Vec and how to use the resulting word vectors&nbsp;for sentiment analysis.</p>
<p>Since deep learning is a rapidly evolving field, large amounts of&nbsp;the work has not yet been published, or exists only as&nbsp;academic papers. Part 3 of the&nbsp;tutorial is more exploratory than prescriptive -- we experiment with several ways of using Word2Vec rather than giving you a recipe for using the output.</p>
<p>To achieve these goals, we rely on an&nbsp;IMDB sentiment analysis data set, which has 100,000 multi-paragraph movie reviews, both positive and negative.&nbsp;</p>
<h2>Acknowledgements</h2>
<p>This dataset was collected in association with the following publication:</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). ""Learning Word Vectors for Sentiment Analysis."" <em>The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</em>. (<a href=""http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf"">link</a>)</p>
<p>Please email the author of that paper if you use the data for any research applications. The tutorial was developed by <a href=""http://www.linkedin.com/pub/angela-chapman/5/330/b97"">Angela Chapman</a>&nbsp;during her summer 2014 internship at Kaggle.</p>","<h2>Metric</h2>
<p>Submissions are judged on&nbsp;<a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>.&nbsp;</p>
<h2>Submission Instructions</h2>
<p>You should submit a comma-separated file with 25,000 row&nbsp;plus a header row. There should be 2 columns: ""id"" and ""sentiment"", which contain your binary predictions: 1 for positive reviews, 0 for negative reviews.&nbsp;For an example, see ""sampleSubmission.csv"" on the Data page.&nbsp;</p>
<pre>id,sentiment
123_45,0 
678_90,1
12_34,0
...</pre>"
Poker Rule Induction,Determine the poker hand of five playing cards,https://www.kaggle.com/competitions/poker-rule-induction,,"Multiclass Classification,Card Games,Tabular",207,212,762,"<p>Your friend bailed last minute on poker night? Before giving up on a much-needed evening of bad bluffs and quarter buy ins, light a&nbsp;cigar and get familiar with the rules of the game. Each record in&nbsp;this competition consists of five playing cards and an&nbsp;attribute&nbsp;representing&nbsp;the poker hand.&nbsp;You are asked to predict the best hand you can play based on the cards you've been dealt.&nbsp;</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/4128/media/dogs_playing_poker.jpg"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>The order of cards is important, which means there are 480 possible Royal Flush hands instead of just four. Identify those, and the other 311,875,200&nbsp;possible hands correctly, and you’re in the money!</p>
<p>""<em>Isn't this easy? I know two-of-a-kind when I see it</em>"", you might rightfully wonder.</p>
<p>And you'd be right. &nbsp;<strong>The intent&nbsp;of this challenge&nbsp;is automatic rules induction, i.e. to learn the rules using&nbsp;machine learning, without hand coding heuristics</strong>. Pretend you are in a foreign land, have never played the game before, are&nbsp;given a history of thousands of games, and are&nbsp;asked to come up with the rules.&nbsp;It is potentially difficult to&nbsp;discover rules that can correctly classify poker&nbsp;hands, yet it is trivial for a&nbsp;human to validate the&nbsp;rules objectively. Remember, your algorithm will need to find rules that are general enough to be broadly useful, without being so broad that they end up being occasionally wrong. We suggest reading&nbsp;the <a href=""http://www.wseas.us/e-library/conferences/crete2002/papers/444-494.pdf"">paper</a>&nbsp;by Cattral et al. for more background&nbsp;on the topic.</p>
<p>Playground competitions are an opportunity to build and stretch your machine learning muscles. Pull up a chair to the data science poker table and ante up.</p>
<h2>Acknowledgements</h2>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created&nbsp;by Robert Cattral and Franz Oppacher.&nbsp;We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:</p>
<p><em>Bache, K. &amp; Lichman, M. (2013). <a href=""http://archive.ics.uci.edu/ml"">UCI Machine Learning Repository</a>. Irvine, CA: University of California, School of Information and Computer Science</em></p>","<p>Submissions are evaluated on the categorization accuracy (the percent of hands that you correctly classify).</p>
<h2>Submission File</h2>
<p>Your submission file should predict the hand for each id in the test set.&nbsp;The file should contain a header and have the following format:</p>
<pre>id,hand<br>1,0<br>2,0<br>3,9<br>...<br>etc.</pre>"
Finding Elo,Predict a chess player's FIDE Elo rating from one game,https://www.kaggle.com/competitions/finding-elo,,"Board Games,Tabular",157,170,1873,"<p>Elite chess players are rated, ranked, analyzed, and compared&nbsp;in many&nbsp;ways. Classical&nbsp;methods of ranking chess players have focused on game histories, paying particular attention to the relative strength of the players involved. This includes the popular <a href=""http://en.wikipedia.org/wiki/Elo_rating_system"">FIDE Elo</a>&nbsp;score, which was the focus of one of Kaggle's first ever competitions - <a href=""http://www.kaggle.com/c/chess"">Elo vs. the Rest of the World</a>.</p>
<p>Recent&nbsp;work on&nbsp;chess analysis has focused on&nbsp;intrinsic performance ratings, where one assesses&nbsp;skill based on the quality of decisions&nbsp;rather than the outcomes of games. For an example of this kind of approach, see&nbsp;<a href=""http://www.cse.buffalo.edu/~regan/papers/pdf/Reg12IPRs.pdf"">this draft</a>&nbsp;by&nbsp;<a href=""http://www.cse.buffalo.edu/~regan/"">Kenneth Regan</a>.&nbsp;Two&nbsp;advantages of&nbsp;an intrinsic approach are an&nbsp;increased&nbsp;sample size (there are many more moves than games)&nbsp;and the ability to approach&nbsp;new challenges, such as determining whether a player is cheating by performing moves above their skill level.</p>
<p>This competition challenges Kagglers&nbsp;to determine&nbsp;players' FIDE Elo ratings at the time a game is played, based solely on the moves in one&nbsp;game. Do a player's moves reflect their absolute skill? Does the opponent matter? How closely does one game reflect intrinsic ability? How well&nbsp;can an algorithm do? Does computational horsepower increase accuracy? Let's find out!</p>
<p>You do not need to be a chess expert -- or even know how to play chess -- to attempt this competition. You do need patience and a computer that doesn't mind some&nbsp;heat.&nbsp;The dataset includes 50,000 games between elite, ranked players. As a getting-started computational bonus, Kaggle has run these games through a&nbsp;<a href=""http://en.wikipedia.org/wiki/Chess_engine"">chess engine</a>&nbsp;to score each move.</p>
<p>Good luck finding Elo!</p>","<p>Submissions are evaluated using the <a href=""https://www.kaggle.com/wiki/MeanAbsoluteError"">mean absolute error</a>.</p>
<h3>Submission File</h3>
<p>For each game in the test set, you should predict the Elo rating of both the black and white player.&nbsp;The file should contain a header and have the following format:</p>
<pre>Event,WhiteElo,BlackElo<br>25001,1000,2000<br>25002,2000,1500<br>25003,1746,2200<br>etc...</pre>"
First Steps With Julia,Use Julia to identify characters from Google Street View images,https://www.kaggle.com/competitions/street-view-getting-started-with-julia,,Image,56,64,252,"<p>This competition is designed to help you get&nbsp;started with <a href=""http://julialang.org/"" target=""_blank"">Julia</a>. If you are looking for a good programming language for data science, or if you are already accustomed to one language, we encourage you to also try Julia. Julia&nbsp;is a relatively new language&nbsp;for technical computing that attempts to combine the strengths of other popular programming languages.&nbsp;</p>
<p><img src=""https://camo.githubusercontent.com/e1ae5c7f6fe275a50134d5889a68f0acdd09ada8/687474703a2f2f6a756c69616c616e672e6f72672f696d616765732f6c6f676f5f68697265732e706e67"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>Here we introduce two tutorials to highlight some of Julia's features.&nbsp;The first is focused on the basics of the language.&nbsp;In the second, a complete implementation of the K Nearest Neighbor algorithm is presented, highlighting features such as parallelization and speed.&nbsp;</p>
<p>Both tutorials show that&nbsp;it is easy to write code in Julia, due to its&nbsp;intuitive syntax and design.&nbsp;The tutorials also describe some basics of&nbsp;image processing and some concepts of machine learning such as cross validation.&nbsp;After reviewing&nbsp;them, we hope you will be motivated to write your own machine learning algorithms in Julia.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3947/media/chars74k.jpg"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>This tutorial focuses on&nbsp;the task of&nbsp;identifying characters from Google Street View images. It differs from traditional character recognition because the data set contains different character fonts and the background is not the same for all images.&nbsp;</p>
<h2>Acknowledgements</h2>
<p>The data&nbsp;was taken from the <a href=""http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/"">Chars74K dataset</a>, which&nbsp;consists of images of characters selected from Google Street View images. We ask that you cite the following reference in any publication resulting from your&nbsp;work:</p>
<p>T. E. de Campos, B. R. Babu and M. Varma, Character recognition in natural images, <em>Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP)</em>, Lisbon, Portugal, February 2009.</p>
<p>This tutorial was developed by <a href=""https://www.kaggle.com/users/23759/luis-tandalla"">Luis Tandalla</a>&nbsp;during his summer 2014 internship at Kaggle.</p>","<p>Your model should identify the character in&nbsp;each image in the test set. The possible characters are 'A-Z', 'a-z', and '0-9'.&nbsp;</p>
<p>The predictions will be evaluated using Classification Accuracy.</p>
<p>\[ &nbsp;\textrm{Accuracy} =\frac{ &nbsp;\sum_{i=1}^N \textrm{true}_i = \textrm{prediction}_i &nbsp;}{N} \]</p>
<h2>Submission File</h2>
<p>For every image in the dataset, submission files should contain two columns: ImageId and Class (character predicted)&nbsp;.&nbsp;</p>
<p>The file should contain a header and have the following format:</p>
<pre>ImageId,Class<br>6284,A<br>6285,b<br>6286,0<br>...</pre>"
Random Acts of Pizza,Predicting altruism through free pizza,https://www.kaggle.com/competitions/random-acts-of-pizza,,"Binary Classification,Text,Internet",462,503,3570,"<p><a href=""https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza"">Get started on this competition through Kaggle Scripts</a></p>
<p>In machine learning, it is often said&nbsp;there are <a href=""http://en.wikipedia.org/wiki/No_free_lunch_theorem"">no free lunches</a>.&nbsp;<em>How wrong we were.</em></p>
<p>This competition contains a dataset with 5671 textual requests for pizza from the Reddit community <a href=""http://www.reddit.com/r/Random_Acts_Of_Pizza/"">Random Acts of Pizza</a>&nbsp;together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm&nbsp;capable of predicting&nbsp;which requests will garner a cheesy (but sincere!) act of&nbsp;kindness.</p>
<p>""I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,"" says one hopeful poster. What about making an algorithm?</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3949/media/pizzas.png"" alt=""Pizzas""></p>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This <a href=""http://cs.stanford.edu/~althoff/raop-dataset/"">data</a>&nbsp;was collected and graciously shared by <a href=""http://www.timalthoff.com/"">Althoff</a>&nbsp;et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their <a href=""http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf"">accompanying paper</a>&nbsp;and ask that you cite the following reference in any publications that result from your work:</p>
<p>Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky.&nbsp;<em>How to Ask for a Favor: A Case Study on the Success of Altruistic Requests</em>,&nbsp;Proceedings of ICWSM, 2014.</p>
<p></p>","<p>Submissions are evaluated on <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>&nbsp;between the predicted probability that a request will get pizza and the observed outcomes.</p>
<h2>Submission File</h2>
<p>For each request&nbsp;in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:</p>
<pre>request_id,requester_received_pizza<br>t3_i8iy4,0<br>t3_1mfqi0,0<br>t3_lclka,0<br>...<br>...</pre>"
Bike Sharing Demand,Forecast use of a city bikeshare system,https://www.kaggle.com/competitions/bike-sharing-demand,,"Cycling,Tabular,Time Series Analysis",3242,3559,32809,"<p><a href=""https://www.kaggle.com/c/bike-sharing-demand/forums/t/13228/kaggle-scripts/69563#post69563"">Get started&nbsp;on this competition through Kaggle Scripts</a></p>
<p>Bike sharing systems are a means of renting bicycles where the&nbsp;process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using&nbsp;these systems, people are&nbsp;able rent a bike from a one&nbsp;location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.</p>
<p>The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed&nbsp;is explicitly recorded. Bike sharing systems therefore function as a&nbsp;sensor network, which can be used for studying&nbsp;mobility in a&nbsp;city. In this competition, participants are asked to combine&nbsp;historical usage&nbsp;patterns with weather data in order to forecast bike&nbsp;rental demand in&nbsp;the Capital Bikeshare program in Washington, D.C.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3948/media/bikes.png"" alt=""Bikes""></p>
<h2>Acknowledgements</h2>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was&nbsp;provided by Hadi Fanaee Tork using data from&nbsp;<a href=""http://www.capitalbikeshare.com/system-data"">Capital Bikeshare</a>.&nbsp;We also thank the UCI machine learning repository for <a href=""http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"">hosting the dataset</a>. If you use the problem in publication, please cite:</p>
<p>Fanaee-T, Hadi, and Gama, Joao, <em>Event labeling combining ensemble detectors and background knowledge</em>, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.</p>","<p>Submissions are evaluated one the&nbsp;Root Mean Squared Logarithmic Error (RMSLE).&nbsp;The RMSLE is calculated as</p>
<p>\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]</p>
<p>Where:</p>
<ul>
<li></li>
<li></li>
<li></li>
<li>logarithm</li>
</ul>
<h2>Submission Format</h2>
<p>Your submission file must have a header and should be structured in the following format:</p>
<pre>datetime,count<br>2011-01-20 00:00:00,0<br>2011-01-20 01:00:00,0<br>2011-01-20 02:00:00,0
...<br>...
</pre>"
Forest Cover Type Prediction,Use cartographic variables to classify forest categories,https://www.kaggle.com/competitions/forest-cover-type-prediction,,"Multiclass Classification,Forestry",1692,1860,15404,"<p><a href=""https://www.kaggle.com/c/forest-cover-type-prediction/forums/t/13534/get-started-with-kaggle-scripts/72674#post72674"">Get started on this competition with&nbsp;Kaggle Scripts.</a>&nbsp;No data download or local environment needed!</p>
<p><a href=""http://en.wikipedia.org/wiki/Random_forest"">Random forests?</a>&nbsp;<a href=""http://en.wikipedia.org/wiki/Cover_tree"">Cover trees?</a>&nbsp;Not so fast, computer nerds. We're talking about the real thing.</p>
<p>In this competition you are asked to predict the forest cover type (the predominant&nbsp;kind&nbsp;of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.</p>
<p>This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.</p>
<h2>Acknowledgements</h2>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was&nbsp;provided by Jock A. Blackard and Colorado State University.&nbsp;We also thank the UCI machine learning repository for <a href=""https://archive.ics.uci.edu/ml/datasets/Covertype"">hosting the dataset</a>. If you use the problem in publication, please cite:</p>
<p><em>Bache, K. &amp; Lichman, M. (2013). <a href=""http://archive.ics.uci.edu/ml"">UCI Machine Learning Repository</a>. Irvine, CA: University of California, School of Information and Computer Science</em></p>","<p>Submissions are evaluated on multi-class&nbsp;classification accuracy.</p>
<h2>Submission File</h2>
<p>Your submission file should have&nbsp;the observation Id and a&nbsp;predicted cover type (an integer between 1 and 7, inclusive).&nbsp;The file should contain a header and have the following format:</p>
<pre>Id,Cover_Type<br>15121,1<br>15122,1<br>15123,1<br>...</pre>"
Billion Word Imputation,Find and impute missing words in the billion word corpus,https://www.kaggle.com/competitions/billion-word-imputation,,"Text,Linguistics",87,125,629,"<p><img style=""float: left; margin: 0 15px 15px 0"" src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3927/media/corpus.png"" alt="""">This competition uses the billion-word benchmark corpus provided by&nbsp;<a href=""http://arxiv.org/abs/1312.3005"">Chelba et al.</a>&nbsp;for language modeling. Rather than ask participants to create a classic language model and evaluate sentence probabilities -- a task which is difficult to faithfully score&nbsp;in Kaggle's supervised ML setting -- we have introduced a variation&nbsp;on the&nbsp;language modeling task.</p>
<p>For each sentence in the test set, we have removed exactly one word. Participants must create a model capable of inserting back the&nbsp;correct missing word at the correct location in the sentence. Submissions are scored using an edit distance to allow for partial credit.</p>
<p>We extend our thanks to authors who created this corpus and shared it for the research community to use. Please cite this paper if you use this dataset in your research: <em>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn: One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling, CoRR, 2013.</em></p>
<p>Note: the train/test split used in this competition is different than the published&nbsp;version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website.</p>","<p>Submissions are evaluated on the mean <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"">Levenshtein distance</a>&nbsp;between the sentences you submit and the original sentences in the test set.</p>
<p><em>Note: due to the size and computations necessary to score submissions for this competition, scoring may take 5-10&nbsp;minutes, and possibly longer if there are other submissions in front of yours. Please be patient!</em></p>
<h2>Submission File</h2>
<p>Your submission file should contain the sentence id and a&nbsp;predicted sentence. To prevent parsing issues, you should use double quotes to escape the sentence text and two&nbsp;double quotes ("""") for double quotes within a&nbsp;sentence. Note that test.csv is a valid submission file itself.</p>
<p>The file should contain a header and have the following format:</p>
<pre>id,""sentence""<br>1,""Former Dodgers manager , the team 's undisputed top ambassador , is going strong at 83 and serving up one great story after another .""<br>2,""8 parliamentary elections meant to restore democracy in this nuclear armed nation , a key ally against Islamic .""<br>3,""Sales of drink are growing 37 per cent month-on-month from a small base .""<br>etc...</pre>"
Sentiment Analysis on Movie Reviews,Classify the sentiment of sentences from the Rotten Tomatoes dataset,https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews,,"Text,Multiclass Classification",860,1011,6813,"<p>""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.""</p>
<p>The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases&nbsp;on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png"" alt=""Treebank""></p>
<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of <a href=""http://www.socher.org/"">Socher</a> et al [2].&nbsp;We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper:</p>
<p><a href=""http://nlp.stanford.edu/sentiment/"">http://nlp.stanford.edu/sentiment/</a></p>
<p>There you will find have source code, a live demo, and even an online interface to help train the model.</p>
<p>[1]&nbsp;Pang and L. Lee. 2005. <em>Seeing stars: Exploiting class&nbsp;relationships for sentiment categorization with respect&nbsp;to rating scales</em>. In ACL, pages 115–124.</p>
<p>[2]&nbsp;<em>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</em>, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).</p>
<p></p>","<p>Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:</p>
<p>0 - negative<br>1 - somewhat negative<br>2 - neutral<br>3 - somewhat positive<br>4 - positive</p>
<h2>Submission Format</h2>
<p>For each phrase&nbsp;in the test set, predict a label for the sentiment.&nbsp;Your submission should have a header and look like the following:</p>
<pre>PhraseId,Sentiment<br>156061,2<br>156062,2<br>156063,2<br>...</pre>"
Walmart Recruiting - Store Sales Forecasting,Use historical markdown data to predict store sales,https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting,,Time Series Analysis,688,688,12240,"<p>One challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line.</p>
<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3816/media/markdowns.png"" alt=""Markdowns"" style=""display: block; margin-left: auto; margin-right: auto""></p>
<p>In this recruiting competition, job-seekers are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact.</p>
<p>Want to work&nbsp;in a great environment with some of the world's largest data sets? This is a chance to display your modeling mettle to the Walmart hiring teams.</p>
<p><strong>This competition counts towards rankings &amp; achievements. &nbsp;</strong>If you wish to be considered for an interview at Walmart, check the box ""Allow host to contact me"" when you make your first entry.&nbsp;</p>
<p>You must compete as an individual in recruiting competitions. You may only use the provided data to make your predictions.</p>","<p>This competition is evaluated on the weighted mean absolute error (WMAE):</p>
<p>$$<br>\textrm{WMAE} = \frac{1}{\sum{w_i}} \sum_{i=1}^n w_i | y_i - \hat{y}_i |<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of rows</li>
<li>\\( \hat{y}_i \\) is the predicted sales</li>
<li>\\( y_i \\) is the actual sales</li>
<li>\\( w_i \\) are weights. w = 5 if the week is a holiday week, 1 otherwise</li>
</ul>
<h2>Submission File</h2>
<p>For each row in the test set (store + department + date triplet), you should predict the weekly sales of that department. The Id column is formed by concatenating the Store, Dept, and Date with underscores (e.g. Store_Dept_2012-11-02). &nbsp;The file should have a header and looks like the following:</p>
<pre>Id,Weekly_Sales
1_1_2012-11-02,0<br>1_1_2012-11-09,0<br>1_1_2012-11-16,0
...
</pre>"
March Machine Learning Mania,Tip off college basketball by predicting the 2014 NCAA Tournament,https://www.kaggle.com/competitions/march-machine-learning-mania-2014,,"Sports,Basketball",248,337,434,"<h3><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3812/media/Kaggle_web_banner_FINAL.png"" alt="""" style=""color: rgba(85, 85, 85, 1); font-size: 1em; font-family: &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; line-height: 1.5em""></h3>
<p>Each year, millions of people fill out a bracket to predict the outcome of the popular men’s college basketball tournament that tips off in March. While the odds of creating a perfect bracket are astronomical, these odds are made better by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by Intel, this competition will test how well predictions based on data stack up against a (jump) shot in the dark.</p>
<p>We have assembled the basic elements necessary to get started with tournament prediction. The provided data covers nearly two decades of historical games, but you’re also encouraged to use data from external sources. To help turn all of that information into useful insight, Intel is making its big data technologies more affordable, available, and easier to use for everything from helping develop new scientific discoveries and business models to gaining the upper hand on good-natured predictions of sporting events.<img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3706/media/bball-logo-sm.png"" alt=""Basketball bracket"" style=""color: rgba(51, 51, 51, 1); font-size: 1em; line-height: 1.5em""></p>
<p>In stage one of this two-stage competition, participants will build and test their models against the previous five tournaments. In the second stage, participants will predict the outcome of the 2014 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2014 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket.</p>
<p>To sweeten the pot, Intel will present the team with the most accurate predictions a $15,000 cash prize. Get started today – predictions are due by Wednesday, March 19, 2014.</p>
<p>Please visit the <a href=""//www.kaggle.com/c/march-machine-learning-mania/details/faqs"">FAQs</a> for more information.</p>","<p>Submissions are scored on the log loss, also called the predictive binomial deviance:</p>
<p>$$<br>\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],<br>$$</p>
<p>where</p>
<ul>
<li>n is the number of games played</li>
<li>\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2</li>
<li>\\( y_i \\) is the outcome of each game</li>
<li>\\( log() \\) is the natural (base e) logarithm</li>
</ul>
<p>The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.</p>
<h2>Submission File</h2>
<p>The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2014 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 &nbsp;= 2278 matchups.&nbsp;</p>
<p>Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""N_501_502"" indicates team 501 played team 502 in season N.</p>
<p>The resulting submission format looks like like the following, where ""pred"" represents the predicted probability that the first team will win:</p>
<pre>id,pred<br>N_503_507,0.2<br>N_503_511,0.5<br>N_503_521,0.8<br>...</pre>"
StumbleUpon Evergreen Classification Challenge,Build a classifier to categorize webpages as evergreen or non-evergreen,https://www.kaggle.com/competitions/stumbleupon,,"Text,Tabular,Internet",624,624,7486,"<p><img src=""https://storage.googleapis.com/kaggle-competitions/kaggle/3526/media/stumbleupon_evergreen.jpg"" alt=""Image courtesy of basheertome - http://www.flickr.com/photos/basheertome/9150903985/"" style=""color: rgba(51, 51, 51, 1); font-size: 1em; line-height: 1.5em"">StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as ""ephemeral"" or ""evergreen"". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of ""ephemeral"" or ""evergreen"" would greatly improve a recommendation system like ours.</p>
<p>Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon?&nbsp;As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.</p>","<p>Submissions are judged on&nbsp;<a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">area under the ROC curve</a>.&nbsp;</p>
<p>In Matlab (using the stats toolbox):</p>
<pre>[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
</pre>
<p>In R (using the verification package):</p>
<pre>auc = roc.area(true_labels, predictions)
</pre>
<p>In python (using the metrics module of scikit-learn):</p>
<pre>fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)</pre>
<h2>Submission Format</h2>
<p>Each line of your submission should contain an urlid and a label. Note that you may submit any real-valued number as a prediction, since AUC is only sensitive to the ranking. sampleSubmission.csv shows a representative valid submission. The format looks like this:</p>
<pre>urlid,label<br>5865,0<br>782,0<br>6962,0
etc...
</pre>"
Facial Keypoints Detection,Detect the location of keypoints on face images,https://www.kaggle.com/competitions/facial-keypoints-detection,,Image,175,208,1220,"<p>The objective of this task is to predict keypoint positions on face images. This can be used as a building block in several applications, such as:</p>
<ul>
<li>tracking faces in images and video</li>
<li>analysing facial expressions</li>
<li>detecting dysmorphic facial signs for medical diagnosis</li>
<li>biometrics / face recognition</li>
</ul>
<p>Detecing facial keypoints is a very challenging problem. &nbsp;Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement.</p>
<p>This getting-started competition provides a benchmark data set and an <a href=""https://www.kaggle.com/c/facial-keypoints-detection/details/getting-started-with-r""> R tutorial</a> to get you going on analysing face images.&nbsp;<a href=""https://www.kaggle.com/c/facial-keypoints-detection/details/getting-started-with-r"" style=""font-size: 1.17em; text-align: center; line-height: 1.4em"">Get started with R &gt;&gt;</a></p>
<h2>Acknowledgements</h2>
<p>The data set for this competition was graciously provided by <a href=""http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html""> Dr. Yoshua Bengio</a> of the University of Montreal.&nbsp;<a href=""https://www.kaggle.com/users/5018/james-petterson"" style=""line-height: 1.4em"">James Petterson</a>.</p>","<h2>Root Mean Squared Error (RMSE)</h2>
<p>Submissions are scored on the root mean squared error. RMSE is very common and is a suitable general-purpose error metric.&nbsp;Compared to the Mean Absolute Error, RMSE punishes large errors:</p>
<p>\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]</p>
<p>where y hat is the predicted value and y is the original value.</p>"
Titanic - Machine Learning from Disaster,Start here! Predict survival on the Titanic and get familiar with ML basics,https://www.kaggle.com/competitions/titanic,https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/header.png,"Binary Classification,Tabular,Beginner",14247,14445,60303,"<h3>👋🛳️ Ahoy, welcome to Kaggle! You’re in the right place. </h3>
<p>This is the legendary Titanic ML competition – the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.</p>
<p>The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.
</p>
<p>Read on or watch the video below to explore more details. Once you’re ready to start competing, click on the <a href=""https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic"" target=""_blank"">""Join Competition button</a> to create an account and gain access to the <a href=""https://www.kaggle.com/c/titanic/data"" target=""_blank"">competition data</a>. Then check out <a href=""https://www.kaggle.com/alexisbcook/titanic-tutorial"">Alexis Cook’s Titanic Tutorial</a> that walks you through step by step how to make your first submission!</p>

<a href=""https://www.youtube.com/watch?v=8yZMXCaFshs&amp;feature=youtu.be"" target=""_blank"">
  <img width=""699"" align=""center"" src=""https://storage.googleapis.com/kaggle-media/welcome/video_thumbnail.jpg"">
</a>

<h3>The Challenge</h3>
<p>The sinking of the Titanic is one of the most infamous shipwrecks in history.</p>
<p>On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.</p>
<p>While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.</p>
<p>In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).
</p>
<div class=""note"">
<strong>Recommended Tutorial</strong><br>
We highly recommend <a href=""https://www.kaggle.com/alexisbcook/titanic-tutorial"">Alexis Cook’s Titanic Tutorial</a> that walks you through making your very first submission step by step.
</div>
<h3>Overview of How Kaggle’s Competitions Work</h3>
<p><b>1. Join the Competition</b></p>
Read about the challenge description, accept the Competition Rules and gain access to the competition dataset.<p></p>
<p><b>2. Get to Work</b></p>
Download the data, build models on it locally or on Kaggle Kernels (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.<p></p>
<p><b>3. Make a Submission</b></p>
Upload your prediction as a submission on Kaggle and receive an accuracy score.<p></p>
<p><b>4. Check the Leaderboard</b></p>
See how your model ranks against other Kagglers on our leaderboard. <p></p>
<p><b>5. Improve Your Score</b></p>
Check out the <a href=""https://www.kaggle.com/c/titanic/discussion"" target=""_blank"">discussion forum</a> to find lots of tutorials and insights from other competitors.<p></p>
<div class=""note"">
<strong>Kaggle Lingo Video</strong><br>
You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman’s <a href=""https://www.youtube.com/watch?v=sEJHyuWKd-s"">video on Kaggle Lingo</a> to get up to speed!
</div>
<h3>What Data Will I Use in This Competition?</h3>
<p>In this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.</p>
<p>Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.</p>
<p>The `test.csv` dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.</p>
<p>Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived. </p>
<p>Check out the <a href=""https://www.kaggle.com/c/titanic/data"">“Data” tab</a> to explore the datasets even further. Once you feel you’ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.</p>
<h3>How to Submit your Prediction to Kaggle</h3>
<p>Once you’re ready to make a submission and get on the leaderboard:</p>
<p>1. Click on the “Submit Predictions” button</p>
<img width=""699"" align=""center"" src=""https://storage.googleapis.com/kaggle-media/welcome/screen1.png"">
<p>2. Upload a CSV file in the submission file format. You’re able to submit 10 submissions a day.</p>
<img width=""699"" align=""center"" src=""https://storage.googleapis.com/kaggle-media/welcome/screen2.png"">
<p><b>Submission File Format:</b></p>
<p>You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.</p>
<p>The file should have exactly 2 columns:</p>
<ul>
<li>PassengerId (sorted in any order)</li>
<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>
</ul>
<h3>Got it! I’m ready to get started. Where do I get help if I need it?</h3>
<p>For Competition Help: <a href=""https://www.kaggle.com/c/titanic/discussion"">Titanic Discussion Forum</a>
<br>
Technical Help: <a href=""https://www.kaggle.com/contact"">Kaggle Contact Us Page</a></p>
<p>Kaggle doesn’t have a dedicated support team so you’ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>
<h3>A Last Word on Kaggle Notebooks</h3>
<p>As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.</p>
<p>In every competition, you’ll find many Kernels publically shared with incredible insights. It’s an invaluable resource worth becoming familiar with. Check out this competition’s Kernel’s <a href=""https://www.kaggle.com/c/titanic/kernels"">here</a>.</p>
<h3>🏃‍♀Ready to Compete? <a href=""https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic"">Join the Competition Here!</a>
</h3>","<h3>Goal</h3>
<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>
<h3>Metric</h3>
<p>Your score is the percentage of passengers you correctly predict. This is known as <a href=""https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification"" target=""_blank"">accuracy</a>.</p>
<h3>Submission File Format</h3>
<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>
<ul>
<li>PassengerId (sorted in any order)</li>
<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>
</ul>
<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>
<p>You can download an example submission file (gender_submission.csv) on the <a href=""https://www.kaggle.com/c/titanic/data"">Data page</a>.</p>"
Digit Recognizer,Learn computer vision fundamentals with the famous MNIST data,https://www.kaggle.com/competitions/digit-recognizer,https://storage.googleapis.com/kaggle-competitions/kaggle/3004/logos/header.png?t=2018-11-14-20-12-43,"Tabular,Image,Multiclass Classification",1243,1243,4392,"<h3>Start here if...</h3>
<p>You have some experience with R or Python and machine learning basics, but you’re new to computer vision. This competition is the perfect introduction to techniques like neural networks using a classic dataset including pre-extracted features.</p>


<h3>Competition Description</h3>
<p>MNIST (""Modified National Institute of Standards and Technology"") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.</p>
<p>In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.</p>
<h3>Practice Skills</h3>
<ul>
<li>
<p>Computer vision fundamentals including simple neural networks</p>
</li>
<li>
<p>Classification methods such as SVM and K-nearest neighbors</p>
</li>
</ul>
<h3>Acknowledgements&nbsp;</h3>
<p>More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at <a href=""http://yann.lecun.com/exdb/mnist/index.html"" target=""_blank"">http://yann.lecun.com/exdb/mnist/index.html</a>. The dataset is made available under a <a href=""https://creativecommons.org/licenses/by-sa/3.0/"" target=""_blank"">Creative Commons Attribution-Share Alike 3.0 license</a>.</p>","<h3>Goal</h3>
The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.<br>For every  in the test set, you should predict the correct label. 

<h3>Metric</h3>
<p>This competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).</p>

<h3>Submission File Format</h3>
<p>The file should contain a header and have the following format:</p>
<pre>ImageId,Label<br>1,0<br>2,0<br>3,0<br>etc.</pre>"
